{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Berigny/p-adic-memory/blob/main/DualSubstrateBaselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf83aed3"
      },
      "source": [
        "## Notebook Summary\n",
        "\n",
        "This notebook is designed to evaluate a \"dual-substrate memory\" mechanism (`p_adic_memory`) against a baseline language model without this memory.\n",
        "\n",
        "**Hypothesis:** The dual-substrate memory will improve the language model's ability to recall information from long contexts compared to a standard model.\n",
        "\n",
        "**Method:**\n",
        "1.  **Environment Setup:** Install necessary libraries and clone relevant repositories (LongBench, RULER, p-adic-memory).\n",
        "2.  **Smoke Test:** Perform a minimal test to ensure the dual-substrate memory can be instantiated and used for basic recall.\n",
        "3.  **LongBench-style Evaluation:** Implement a custom harness to evaluate the model with and without the dual-substrate memory on tasks inspired by LongBench, focusing on prompt/response logging.\n",
        "4.  **RULER Evaluation:** Use the RULER framework with a custom adapter to evaluate the model's key-value retrieval capabilities with and without the dual-substrate memory on different context lengths.\n",
        "5.  **Result Export:** Save the evaluation results (JSON and text files) and optionally copy them to Google Drive for persistence.\n",
        "\n",
        "**Assessment of Changes to Resolve Ongoing Issues:**\n",
        "The notebook includes steps to address potential issues like:\n",
        "*   **GPU Availability:** Checking for and mounting Google Drive for persistent storage and displaying GPU information (`!nvidia-smi`).\n",
        "*   **Dependency Conflicts:** Skipping upstream `requirements.txt` and installing compatible versions of libraries like Transformers, Datasets, Accelerate, and BitsAndBytes.\n",
        "*   **Repository Access:** Cloning repositories directly and appending their source paths to the system path.\n",
        "*   **Hugging Face Authentication:** Providing a cell to authenticate with Hugging Face for accessing gated models.\n",
        "*   **LongBench Evaluator:** Acknowledging the lack of a standard LongBench `Evaluator` and providing a custom harness as an alternative.\n",
        "*   **vLLM/flash-attn:** Noting that these are not installed by default on Colab T4 and are optional for A100+ runtimes.\n",
        "*   **Troubleshooting Tips:** Including a dedicated section for common issues like CUDA out-of-memory, tokenizer errors, authentication failures, dataset download issues, and custom module not found errors.\n",
        "\n",
        "The notebook aims to provide a reproducible environment for benchmarking the dual-substrate memory and identifying its impact on language model performance, particularly in long-context scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5cDhH5wn0Jf"
      },
      "source": [
        "# Dual Substrate Colab Test Plan\n",
        "\n",
        "This notebook prepares a Google Colab environment for evaluating the `p_adic_memory` dual-substrate memory against baseline language-model behaviour. Follow the cells in order when running on a T4 GPU runtime.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX7DXs_3n0Jg"
      },
      "source": [
        "## 0. Reality checks\n",
        "\n",
        "Before committing to long runs, make sure the selected model fits in 16¬†GB of VRAM. Start with 4-bit quantised checkpoints such as **TinyLlama/TinyLlama-1.1B-Chat-v1.0** and scale to **mistralai/Mistral-7B-Instruct-v0.2** once everything works.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÅ New runtime first (Runtime ‚Üí Restart)\n",
        "\n",
        "# Remove things that pull conflicting pins (you don't need them for text LLMs)\n",
        "%pip uninstall -y -q torchvision torchaudio opencv-python opencv-contrib-python opencv-python-headless thinc gcsfs fsspec\n",
        "\n",
        "# Fully remove any leftover NumPy wheels and compiled extensions\n",
        "%pip uninstall -y -q numpy numpy-base\n",
        "!rm -rf /usr/local/lib/python3.12/dist-packages/numpy*\n",
        "!rm -rf /usr/local/lib/python3.12/site-packages/numpy*\n"
      ],
      "metadata": {
        "id": "LjbgQkzcuBum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q --upgrade pip\n",
        "%pip install -q \"torch==2.3.1\" --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install -q --no-cache-dir --force-reinstall \"numpy==2.1.3\"\n",
        "%pip install -q --no-cache-dir \"transformers==4.44.2\" \"tokenizers==0.19.1\" \"accelerate==0.33.0\" \\\n",
        "                               \"datasets==2.20.0\" \"evaluate==0.4.2\" sentencepiece ujson\n"
      ],
      "metadata": {
        "id": "oEAak2iLkwxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, numpy.random as npr\n",
        "print(\"numpy:\", np.__version__)\n",
        "print(\"RandomState OK:\", isinstance(npr.RandomState(), npr.RandomState))\n"
      ],
      "metadata": {
        "id": "JM0Ac6pdk8Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/p-adic-memory\n",
        "!git clone -q https://github.com/Berigny/p-adic-memory.git /content/p-adic-memory\n",
        "%pip install -q -e /content/p-adic-memory\n"
      ],
      "metadata": {
        "id": "UDkt6GCgk-Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNFqy1JSn0Jh"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional: mount Google Drive for persistent artifacts and confirm GPU availability\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X8vjBnjn0Ji"
      },
      "source": [
        "## 1. Environment setup\n",
        "\n",
        "Install dependencies in a fixed order so NumPy stays compatible with Colab's preinstalled OpenCV 4.12. Pin PyTorch/cu121, bitsandbytes, and Triton explicitly and then layer the Hugging Face tooling. If you really need the full lock file, override its NumPy 1.26 pin with a constraint (see the note below).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Consolidate package installations to avoid compatibility issues\n",
        "%pip uninstall -y torchvision torchaudio opencv-python opencv-contrib-python opencv-python-headless thinc gcsfs fsspec\n",
        "\n",
        "# Install dependencies in a fixed order to ensure compatibility\n",
        "%pip install -q --upgrade pip\n",
        "\n",
        "# 1) Torch (CUDA 12.1) ‚Äî ONLY torch, not torchvision/torchaudio\n",
        "%pip install -q \"torch==2.3.1\" --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# 2) HF + utils (no bitsandbytes)\n",
        "%pip install -q \"transformers==4.44.2\" \"tokenizers==0.19.1\" \"accelerate==0.33.0\" \\\n",
        "               \"datasets==2.20.0\" \"evaluate==0.4.2\" sentencepiece ujson\n",
        "\n",
        "# 3) NumPy that won‚Äôt fight OpenCV (if it sneaks back) and is fine with Torch\n",
        "# Installing after torch and HF libraries helps prevent compatibility issues\n",
        "%pip install -q \"numpy==1.26.4\"\n",
        "\n",
        "# 4) Install p-adic-memory after core dependencies\n",
        "!rm -rf /content/p-adic-memory\n",
        "!git clone -q https://github.com/Berigny/p-adic-memory.git /content/p-adic-memory\n",
        "\n",
        "%cd /content/p-adic-memory\n",
        "!pip install -q -e .\n",
        "\n",
        "# Add p-adic-memory src to sys.path\n",
        "import sys\n",
        "src_path = \"/content/p-adic-memory/src\"\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "# Return to content directory\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "0yHok_uyWAOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, transformers, tokenizers, numpy as np\n",
        "print(\"torch\", torch.__version__)\n",
        "print(\"transformers\", transformers.__version__)\n",
        "print(\"tokenizers\", tokenizers.__version__)\n",
        "print(\"numpy\", np.__version__)\n"
      ],
      "metadata": {
        "id": "HyPrIeq6XF9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ensureTorch"
      },
      "outputs": [],
      "source": [
        "import os, time, re, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "# Alternative:\n",
        "# MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# ---- shared decoding (deterministic) ----\n",
        "GEN_KW = dict(\n",
        "    do_sample=False,\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    repetition_penalty=1.15,\n",
        "    no_repeat_ngram_size=3,\n",
        "    max_new_tokens=64,\n",
        "    pad_token_id=tok.eos_token_id,\n",
        "    eos_token_id=tok.eos_token_id,\n",
        ")\n",
        "\n",
        "# ---- chat frame + prompt slicing + cleanup ----\n",
        "SYS = (\"Follow instructions exactly. Never repeat the prompt. \"\n",
        "       \"Never invent facts. If uncertain, output 'UNKNOWN'.\")\n",
        "FEWSHOT = \"Only output: TIME=9:00; PRIME=2.\\nTIME=9:00; PRIME=2\\n\"\n",
        "\n",
        "def chatify(user_text: str) -> str:\n",
        "    msgs = [\n",
        "        {\"role\": \"system\", \"content\": SYS},\n",
        "        {\"role\": \"user\", \"content\": \"Only output: TIME=9:00; PRIME=2.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"TIME=9:00; PRIME=2\"},\n",
        "        {\"role\": \"user\", \"content\": user_text},\n",
        "    ]\n",
        "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "ANGLE = re.compile(r\"<[^>]{0,200}>\")\n",
        "\n",
        "def clean_out(s: str) -> str:\n",
        "    return ANGLE.sub(\"\", (s or \"\")).strip()\n",
        "\n",
        "def decode_new_only(inputs, out_ids) -> str:\n",
        "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "    gen_only = out_ids[0][prompt_len:]\n",
        "    return tok.decode(gen_only, skip_special_tokens=True).strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use your installed package; if not installed, stub the memory so A/B still runs.\n",
        "try:\n",
        "    from p_adic_memory import DualSubstrate\n",
        "    MEM = DualSubstrate(dim=128, cycle_minutes=15)\n",
        "except Exception:\n",
        "    MEM = None\n",
        "\n",
        "POLICY = (\"<memory-policy hidden='true'>Use memory facts if present. \"\n",
        "          \"Never print memory tags. If conflict with the prompt, prefer memory.</memory-policy>\")\n",
        "\n",
        "def build_mem_blob(prompt: str) -> str:\n",
        "    if MEM is None:\n",
        "        return \"<mem exact=0 p=0.000>\"\n",
        "    toks = prompt.split()\n",
        "    for i, t in enumerate(toks):\n",
        "        MEM.observe(t, {\"pos\": i % 11, \"role\": \"ctx\"})\n",
        "    recent = toks[-64:]\n",
        "    rec = []\n",
        "    for t in recent:\n",
        "        q = MEM.query(t)  # expects {'exact': bool, 'p': float, ...}\n",
        "        rec.append(f\"<mem exact={int(q.get('exact', False))} p={q.get('p',0.0):.3f}>\")\n",
        "    return \" \".join(rec[:64])\n",
        "\n",
        "def hf_generate(user_text: str) -> str:\n",
        "    chat_str = chatify(user_text)\n",
        "    inputs = tok(chat_str, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(**inputs, **GEN_KW)\n",
        "    return clean_out(decode_new_only(inputs, out))\n",
        "\n",
        "def hf_generate_dual(user_text: str) -> str:\n",
        "    mem_blob = build_mem_blob(user_text)\n",
        "    aug_user = f\"{POLICY}\\n<memory hidden='true'>{mem_blob}</memory>\\n\\n{user_text}\"\n",
        "    chat_str = chatify(aug_user)\n",
        "    inputs = tok(chat_str, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(**inputs, **GEN_KW)\n",
        "    return clean_out(decode_new_only(inputs, out))\n"
      ],
      "metadata": {
        "id": "KTKIsOS4XVA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random, re, time  # ‚Üê add time\n",
        "\n",
        "FMT = re.compile(r\"^TIME=\\d{1,2}:\\d{2}; PRIME=\\d+$\")\n",
        "\n",
        "def make_kv_doc(num_noise_pairs=4000, seed=42):\n",
        "    random.seed(seed)\n",
        "    gt_time, gt_prime = \"9:00\", 2\n",
        "    noise = \" \".join(f\"Z{i}:{(i*7)%97};\" for i in range(num_noise_pairs))\n",
        "    payload = f\"{noise} TIME:{gt_time}; PRIME:{gt_prime}; {noise}\"\n",
        "    instr = \"Only output in this exact format: TIME=<time>; PRIME=<n>.\"\n",
        "    return f\"{payload}\\n\\n{instr}\"\n",
        "\n",
        "def run_ruler(gen_fn, noise_sizes=(1000, 4000, 8000, 16000)):\n",
        "    rows = []\n",
        "    for L in noise_sizes:\n",
        "        prompt = make_kv_doc(L)\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            resp = gen_fn(prompt)\n",
        "        except Exception as e:\n",
        "            resp = f\"ERROR: {type(e).__name__}: {e}\"\n",
        "        lat = round(time.time() - t0, 3)\n",
        "        ok = isinstance(resp, str) and FMT.fullmatch(resp or \"\") and (\"TIME=9:00\" in resp) and (\"PRIME=2\" in resp)\n",
        "        rows.append({\"noise_pairs\": L, \"response\": resp, \"ok\": bool(ok), \"latency_s\": lat})\n",
        "    return rows\n",
        "\n",
        "ruler_baseline = run_ruler(hf_generate)\n",
        "ruler_dual     = run_ruler(hf_generate_dual)\n",
        "\n",
        "with open(\"/content/ruler_baseline.json\",\"w\") as f: json.dump(ruler_baseline, f, indent=2)\n",
        "with open(\"/content/ruler_dual_substrate.json\",\"w\") as f: json.dump(ruler_dual, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", \"/content/ruler_baseline.json\", \"/content/ruler_dual_substrate.json\")\n",
        "\n",
        "def summary(rows):\n",
        "    return [{\"noise_pairs\": r[\"noise_pairs\"], \"acc\": int(r[\"ok\"]), \"latency_s\": r[\"latency_s\"]} for r in rows]\n",
        "\n",
        "print(\"Baseline:\", summary(ruler_baseline))\n",
        "print(\"Dual    :\", summary(ruler_dual))\n"
      ],
      "metadata": {
        "id": "GVTJxCLtaCMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_5kV9R9n0Jj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- clone official repos (source only; no requirements.txt installs) ---\n",
        "!rm -rf /content/LongBench /content/RULER\n",
        "!git clone -q https://github.com/THUDM/LongBench.git /content/LongBench\n",
        "!git clone -q https://github.com/NVIDIA/RULER.git /content/RULER\n",
        "\n",
        "import sys\n",
        "if \"/content/LongBench\" not in sys.path:\n",
        "    sys.path.append(\"/content/LongBench\")\n",
        "if \"/content/RULER\" not in sys.path:\n",
        "    sys.path.append(\"/content/RULER\")\n",
        "\n",
        "# --- your package from GitHub (editable for quick iteration) ---\n",
        "!rm -rf /content/p-adic-memory\n",
        "!git clone -q https://github.com/Berigny/p-adic-memory.git /content/p-adic-memory\n",
        "\n",
        "%cd /content/p-adic-memory\n",
        "!pip install -q -e .\n",
        "\n",
        "src_path = \"/content/p-adic-memory/src\"\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "%cd /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dxC0CO6InPG"
      },
      "outputs": [],
      "source": [
        "# --- quick sanity checks ---\n",
        "import os, importlib.util\n",
        "print(\"LongBench pred.py:\", os.path.exists(\"/content/LongBench/pred.py\"))\n",
        "print(\"RULER top-level:\", os.listdir(\"/content/RULER\")[:10])\n",
        "print(\"p_adic_memory importable?\", importlib.util.find_spec(\"p_adic_memory\") is not None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k76d5ZCSQF2M"
      },
      "outputs": [],
      "source": [
        "# LongBench is script-first; confirm entry scripts exist and explain why Evaluator imports fail\n",
        "import os\n",
        "LB_INNER = '/content/LongBench/LongBench'\n",
        "print('Has LongBench inner dir?', os.path.isdir(LB_INNER))\n",
        "if os.path.isdir(LB_INNER):\n",
        "    print('Contents:', sorted(f for f in os.listdir(LB_INNER) if f.endswith('.py'))[:6])\n",
        "    if not os.path.exists(os.path.join(LB_INNER, 'eval.py')):\n",
        "        print('Note: no eval.py script found ‚Äî use the custom harness below.')\n",
        "else:\n",
        "    print('Clone LongBench with: !git clone https://github.com/THUDM/LongBench.git /content/LongBench')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chn_3W6Tn0Jj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Colab T4 runtimes lack wheels for vLLM/flash-attn pinned by LongBench; install only on A100+\n",
        "# !pip install -q vllm vllm-flash-attn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGx2LqcPn0Jj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Authenticate with Hugging Face if you intend to use gated checkpoints\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "token = getpass(\"Paste your Hugging Face token (press enter to skip): \")\n",
        "if token:\n",
        "    os.environ[\"HF_TOKEN\"] = token\n",
        "    from huggingface_hub import login\n",
        "    login(token=token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa5xYOoan0Jk"
      },
      "source": [
        "## 2. Minimal smoke test with the shared harness\n",
        "\n",
        "The following cell uses the shared harness to load the model and run text generation. It loads prompts from `tests/test_cases.json` to ensure consistency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQerV2j9n0Jk"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "try:\n",
        "    from p_adic_memory import DualSubstrate\n",
        "    MEM = DualSubstrate(dim=128, cycle_minutes=15)\n",
        "except Exception as e:\n",
        "    print(\"DualSubstrate not available, using stub:\", e)\n",
        "    MEM = None\n",
        "\n",
        "POLICY = (\"<memory-policy hidden='true'>Use memory facts if present. \"\n",
        "          \"Never print memory tags. If conflict with the prompt, prefer memory.</memory-policy>\")\n",
        "\n",
        "def build_mem_blob(prompt: str) -> str:\n",
        "    if MEM is None:\n",
        "        return \"<mem exact=0 p=0.000>\"\n",
        "    toks = prompt.split()\n",
        "    for i, t in enumerate(toks):\n",
        "        MEM.observe(t, {\"pos\": i % 11, \"role\": \"ctx\"})\n",
        "    recent = toks[-64:]\n",
        "    recs = []\n",
        "    for t in recent:\n",
        "        q = MEM.query(t)  # {'exact': bool, 'p': float, ...}\n",
        "        recs.append(f\"<mem exact={int(q.get('exact', False))} p={q.get('p',0.0):.3f}>\")\n",
        "    return \" \".join(recs[:64])\n",
        "\n",
        "def hf_generate_dual(user_text: str) -> str:\n",
        "    mem_blob = build_mem_blob(user_text)\n",
        "    aug_user = f\"{POLICY}\\n<memory hidden='true'>{mem_blob}</memory>\\n\\n{user_text}\"\n",
        "    s = chatify(aug_user)\n",
        "    inputs = tok(s, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(**inputs, **GEN_KW)\n",
        "    return clean_out(decode_new_only(inputs, out))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSWa2YiBn0Jl"
      },
      "source": [
        "## tests\n",
        "\n",
        "LongBench and RULER\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQQMe_y8n0Jm"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json, random, re, time  # ‚Üê add time\n",
        "\n",
        "FMT = re.compile(r\"^TIME=\\d{1,2}:\\d{2}; PRIME=\\d+$\")\n",
        "\n",
        "def make_kv_doc(num_noise_pairs=4000, seed=42):\n",
        "    random.seed(seed)\n",
        "    gt_time, gt_prime = \"9:00\", 2\n",
        "    noise = \" \".join(f\"Z{i}:{(i*7)%97};\" for i in range(num_noise_pairs))\n",
        "    payload = f\"{noise} TIME:{gt_time}; PRIME:{gt_prime}; {noise}\"\n",
        "    instr = \"Only output in this exact format: TIME=<time>; PRIME=<n>.\"\n",
        "    return f\"{payload}\\n\\n{instr}\"\n",
        "\n",
        "def run_ruler(gen_fn, noise_sizes=(1000, 4000, 8000, 16000)):\n",
        "    rows = []\n",
        "    for L in noise_sizes:\n",
        "        prompt = make_kv_doc(L)\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            resp = gen_fn(prompt)\n",
        "        except Exception as e:\n",
        "            resp = f\"ERROR: {type(e).__name__}: {e}\"\n",
        "        lat = round(time.time() - t0, 3)\n",
        "        ok = isinstance(resp, str) and FMT.fullmatch(resp or \"\") and (\"TIME=9:00\" in resp) and (\"PRIME=2\" in resp)\n",
        "        rows.append({\"noise_pairs\": L, \"response\": resp, \"ok\": bool(ok), \"latency_s\": lat})\n",
        "    return rows\n",
        "\n",
        "ruler_baseline = run_ruler(hf_generate)\n",
        "ruler_dual     = run_ruler(hf_generate_dual)\n",
        "\n",
        "with open(\"/content/ruler_baseline.json\",\"w\") as f: json.dump(ruler_baseline, f, indent=2)\n",
        "with open(\"/content/ruler_dual_substrate.json\",\"w\") as f: json.dump(ruler_dual, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", \"/content/ruler_baseline.json\", \"/content/ruler_dual_substrate.json\")\n",
        "\n",
        "def summary(rows):\n",
        "    return [{\"noise_pairs\": r[\"noise_pairs\"], \"acc\": int(r[\"ok\"]), \"latency_s\": r[\"latency_s\"]} for r in rows]\n",
        "\n",
        "print(\"Baseline:\", summary(ruler_baseline))\n",
        "print(\"Dual    :\", summary(ruler_dual))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmpEknogn0Jm"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "queries = [\n",
        "  {\"id\": \"summary_1\",\n",
        "   \"prompt\": \"In one sentence, summarise the following log:\\nAlice met Bob at 9:00. They discussed primes 2, 3, 5, 7 and M√∂bius transforms.\"},\n",
        "  {\"id\": \"recall_1\",\n",
        "   \"prompt\": \"Recall the meeting time and the smallest prime they discussed. Only output in this exact format: TIME=<time>; PRIME=<n>.\"}\n",
        "]\n",
        "\n",
        "def tag_ok(resp, qid):\n",
        "    if \"recall\" in qid:\n",
        "        return bool(FMT.match(resp or \"\"))\n",
        "    return None\n",
        "\n",
        "lb_baseline, lb_dual = [], []\n",
        "for q in queries:\n",
        "    t0 = time.time(); r = hf_generate(q[\"prompt\"])\n",
        "    lb_baseline.append({\"id\": q[\"id\"], \"response\": r, \"ok\": tag_ok(r, q[\"id\"]), \"latency_s\": round(time.time()-t0, 3)})\n",
        "\n",
        "for q in queries:\n",
        "    t0 = time.time(); r = hf_generate_dual(q[\"prompt\"])\n",
        "    lb_dual.append({\"id\": q[\"id\"], \"response\": r, \"ok\": tag_ok(r, q[\"id\"]), \"latency_s\": round(time.time()-t0, 3)})\n",
        "\n",
        "with open(\"/content/longbench_baseline.json\",\"w\") as f: json.dump(lb_baseline, f, indent=2)\n",
        "with open(\"/content/longbench_dual_substrate.json\",\"w\") as f: json.dump(lb_dual, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", \"/content/longbench_baseline.json\", \"/content/longbench_dual_substrate.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dDQXaLPHnj7"
      },
      "outputs": [],
      "source": [
        "# Prompt slicing sanity check\n",
        "GEN_KW_DEBUG = dict(baseline_defaults)\n",
        "GEN_KW_DEBUG['max_new_tokens'] = 8\n",
        "\n",
        "text = chatify(baseline_tok, \"Only output: TIME=9:00; PRIME=2.\")\n",
        "ids = baseline_tok(text, return_tensors=\"pt\").to(baseline_model.device)\n",
        "with torch.inference_mode():\n",
        "    out = baseline_model.generate(**ids, **GEN_KW_DEBUG)\n",
        "\n",
        "full = baseline_tok.decode(out[0], skip_special_tokens=True)\n",
        "new = decode_new_only(baseline_tok, ids, out)\n",
        "\n",
        "print(\"FULL====\\n\", full[:300])\n",
        "print(\"\\nNEW====\\n\", new)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2KdWBPrn0Jm"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "for name in [\"longbench_dual_substrate.json\", \"longbench_baseline.json\"]:\n",
        "    path = Path(\"/content\") / name\n",
        "    if not path.exists():\n",
        "        print(f\"Missing {name}; run the harness cell above first.\")\n",
        "        continue\n",
        "    with path.open() as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"\\n{name} (records={len(data)}):\")\n",
        "    for item in data:\n",
        "        snippet = item[\"prompt\"][:48].replace(\"\\n\", \" \")\n",
        "        print(\"- prompt[:48]={!r} | latency={}\".format(snippet, item.get(\"latency_s\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddxESaHPn0Jm"
      },
      "source": [
        "## 4. RULER evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXFDO-B-n0Jn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > /content/ruler_adapter.py <<'PY'\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if \"/content\" not in sys.path:\n",
        "    sys.path.append(\"/content\")\n",
        "\n",
        "from dual_substrate_adapter import DualSubstrateGenerator\n",
        "\n",
        "_model = None\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    global _model\n",
        "    if _model is None:\n",
        "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "        _model = DualSubstrateGenerator(name, hf_token=os.environ.get(\"HF_TOKEN\"))\n",
        "    return _model\n",
        "\n",
        "\n",
        "def generate(prompt: str) -> str:\n",
        "    model = load_model()\n",
        "    return model.generate(prompt, max_new_tokens=256)\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56t_anUYn0Jn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, subprocess, sys\n",
        "\n",
        "os.environ[\"PYTHONPATH\"] = f\"/content:{os.environ.get('PYTHONPATH', '')}\"\n",
        "os.environ[\"RULER_MODEL\"] = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    '-m',\n",
        "    'ruler.evaluate',\n",
        "    '--model',\n",
        "    'custom',\n",
        "    '--custom_module',\n",
        "    'ruler_adapter',\n",
        "    '--tasks',\n",
        "    'kv_retrieval',\n",
        "    '--context_lengths',\n",
        "    '4k,8k',\n",
        "    '--num_samples',\n",
        "    '50',\n",
        "]\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print(completed.stdout)\n",
        "print(completed.stderr)\n",
        "\n",
        "with open('/content/ruler_dual_substrate.txt', 'w') as f:\n",
        "    f.write(completed.stdout)\n",
        "\n",
        "print('Saved:', '/content/ruler_dual_substrate.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK0uV5yYn0Jn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional vanilla RULER baseline using transformers only\n",
        "%%bash\n",
        "cat > /content/ruler_vanilla_adapter.py <<'PY'\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "if \"/content\" not in sys.path:\n",
        "    sys.path.append(\"/content\")\n",
        "\n",
        "from dual_substrate_adapter import (\n",
        "    ALLOWED_GEN_KW,\n",
        "    GEN_KW_BASE,\n",
        "    chatify,\n",
        "    clean_output,\n",
        "    decode_new_only,\n",
        "    enforce_recall_format,\n",
        ")\n",
        "\n",
        "_model = None\n",
        "_tok = None\n",
        "_defaults = None\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    global _model, _tok, _defaults\n",
        "    if _model is None or _tok is None or _defaults is None:\n",
        "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "        qconf = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "        _tok = AutoTokenizer.from_pretrained(name, token=os.environ.get(\"HF_TOKEN\"))\n",
        "        _model = AutoModelForCausalLM.from_pretrained(\n",
        "            name,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            quantization_config=qconf,\n",
        "        )\n",
        "        _defaults = dict(GEN_KW_BASE)\n",
        "        _defaults[\"pad_token_id\"] = _tok.eos_token_id\n",
        "        _defaults[\"eos_token_id\"] = _tok.eos_token_id\n",
        "    return _tok, _model, _defaults\n",
        "\n",
        "\n",
        "def build_generation_kwargs(base_settings, max_new_tokens, **overrides):\n",
        "    settings = dict(base_settings)\n",
        "    settings[\"max_new_tokens\"] = max_new_tokens\n",
        "    for key, value in overrides.items():\n",
        "        if key in ALLOWED_GEN_KW and value is not None:\n",
        "            settings[key] = value\n",
        "    return settings\n",
        "\n",
        "\n",
        "def generate(prompt: str) -> str:\n",
        "    tok, model, defaults = load_model()\n",
        "    chat_prompt = chatify(tok, prompt)\n",
        "    inputs = tok(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    settings = build_generation_kwargs(defaults, 256)\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(**inputs, **settings)\n",
        "    text = decode_new_only(tok, inputs, output)\n",
        "    text = clean_output(text)\n",
        "    return enforce_recall_format(prompt, text)\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCHxB-Ron0Jn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import subprocess, sys\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    '-m',\n",
        "    'ruler.evaluate',\n",
        "    '--model',\n",
        "    'custom',\n",
        "    '--custom_module',\n",
        "    'ruler_vanilla_adapter',\n",
        "    '--tasks',\n",
        "    'kv_retrieval',\n",
        "    '--context_lengths',\n",
        "    '4k,8k',\n",
        "    '--num_samples',\n",
        "    '50',\n",
        "]\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print(completed.stdout)\n",
        "print(completed.stderr)\n",
        "\n",
        "with open('/content/ruler_baseline.txt', 'w') as f:\n",
        "    f.write(completed.stdout)\n",
        "\n",
        "print('Saved:', '/content/ruler_baseline.txt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7-WvjIPn0Jn"
      },
      "source": [
        "## 5. Export and persist results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0Dqr0agn0Jo"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!ls -lh /content/*longbench*.json /content/*ruler* 2>/dev/null || true\n",
        "!cp /content/longbench_*.json /content/ruler_* /content/drive/MyDrive/ 2>/dev/null || true\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6CrjjJxsGEw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kor6GcPnn0Jo"
      },
      "source": [
        "## 6. Scaling plan\n",
        "\n",
        "1. Swap `MODEL_NAME` to **mistralai/Mistral-7B-Instruct-v0.2** with 4-bit quantisation.\n",
        "2. Increase LongBench `sample_size` (e.g., 25 ‚Üí 100) and add tasks such as `LongBookSummEng` and additional QA tracks.\n",
        "3. Extend RULER coverage to multi-hop and longer contexts once the pipeline is reliable.\n",
        "4. Introduce vLLM for batching after verifying correctness with Transformers.\n",
        "5. Maintain A/B JSON outputs (`baseline` vs `dual_substrate`) and track latency, VRAM, and accuracy deltas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qxrWXzqn0Jo"
      },
      "source": [
        "## 7. Troubleshooting tips\n",
        "\n",
        "* **CUDA out-of-memory**: lower `max_new_tokens`, revert to the TinyLlama checkpoint, or ensure 4-bit loading is active.\n",
        "* **Tokenizer errors**: set `pad_token_id` to `tok.eos_token_id`.\n",
        "* **Authentication failures**: provide a Hugging Face token and request model access if required.\n",
        "* **Dataset download issues**: run the dataset setup cells once with a stable internet connection.\n",
        "* **Custom module not found**: confirm that `/content` is on `PYTHONPATH` before invoking RULER.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JH4C4fXn0Jo"
      },
      "source": [
        "## 8. Publishing checklist\n",
        "\n",
        "* Commit `dual_substrate_adapter.py`, `ruler_adapter.py`, and this notebook to a dedicated branch (e.g., `colab-benchmark/`).\n",
        "* Archive JSON artefacts (`longbench_*.json`, `ruler_*.txt`) for baseline comparisons.\n",
        "* Summarise the metrics in a short report covering recall, drift, latency, and energy usage.\n"
      ]
    }
  ]
}