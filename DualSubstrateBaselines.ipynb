{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d8bda06d2daf4c09876346c3549b3feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2bdbe468bee462c861aec3dc4c9600b",
              "IPY_MODEL_b73ed542b2fb468d8275effad0b2b59e",
              "IPY_MODEL_7f906ff8f5384dac83cbdc60964eacec"
            ],
            "layout": "IPY_MODEL_965fc48de2b84512afd2e5026e0ec2e9"
          }
        },
        "f2bdbe468bee462c861aec3dc4c9600b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e1343913a3c446ba2c258d727746d30",
            "placeholder": "​",
            "style": "IPY_MODEL_b9a60e7aec344939a3a868f2619a9548",
            "value": "tokenizer_config.json: "
          }
        },
        "b73ed542b2fb468d8275effad0b2b59e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e8e692098ff472c805de5d5a3361fb7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb7ce7fda8fe4ed998202630994f4b15",
            "value": 1
          }
        },
        "7f906ff8f5384dac83cbdc60964eacec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ab1cc32384341e0963c608fe3713079",
            "placeholder": "​",
            "style": "IPY_MODEL_0ead062ed0064267a97ed542f2bddfd1",
            "value": " 1.29k/? [00:00&lt;00:00, 51.4kB/s]"
          }
        },
        "965fc48de2b84512afd2e5026e0ec2e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e1343913a3c446ba2c258d727746d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9a60e7aec344939a3a868f2619a9548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e8e692098ff472c805de5d5a3361fb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "fb7ce7fda8fe4ed998202630994f4b15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ab1cc32384341e0963c608fe3713079": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ead062ed0064267a97ed542f2bddfd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90182bc285994a24b96ccced616550b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26b5830ca920494ea7a0a34862be27c8",
              "IPY_MODEL_a13e5f94d0694b0ca7b1d2d0fa10e81e",
              "IPY_MODEL_53b5cec9e4024ef0a2883016037b5983"
            ],
            "layout": "IPY_MODEL_32488a5f11cd4ea297ceb194d6579b98"
          }
        },
        "26b5830ca920494ea7a0a34862be27c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bb8b0edc7c8423fa6628b2f8df69071",
            "placeholder": "​",
            "style": "IPY_MODEL_5026660b0bf24080898de3d67a00e338",
            "value": "tokenizer.model: 100%"
          }
        },
        "a13e5f94d0694b0ca7b1d2d0fa10e81e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c830c8ac076646c28e927f615b23d00e",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a45aa9c03e6468ab105b6d91e3b4c29",
            "value": 499723
          }
        },
        "53b5cec9e4024ef0a2883016037b5983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_778fa67a33d14dcca5fcb654b3b5f157",
            "placeholder": "​",
            "style": "IPY_MODEL_3ffa2e239ac74b02ba2b2a25265f3d16",
            "value": " 500k/500k [00:00&lt;00:00, 884kB/s]"
          }
        },
        "32488a5f11cd4ea297ceb194d6579b98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bb8b0edc7c8423fa6628b2f8df69071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5026660b0bf24080898de3d67a00e338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c830c8ac076646c28e927f615b23d00e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a45aa9c03e6468ab105b6d91e3b4c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "778fa67a33d14dcca5fcb654b3b5f157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ffa2e239ac74b02ba2b2a25265f3d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c8888c954cc4cd7891ed51182342323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7613095ed7bf40fbbb4365690e25cc5d",
              "IPY_MODEL_acda6b370294487ba345dc467abbde8d",
              "IPY_MODEL_8697403449a94ee8b0620006b495c648"
            ],
            "layout": "IPY_MODEL_24d0e48a9cfc43bd930676de44ba436b"
          }
        },
        "7613095ed7bf40fbbb4365690e25cc5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fae89f34aa444e7aa9a607c9d58917e",
            "placeholder": "​",
            "style": "IPY_MODEL_9c99383322c943138fecc899cf2da174",
            "value": "tokenizer.json: "
          }
        },
        "acda6b370294487ba345dc467abbde8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4f9456c28f44813b60162d075811834",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9141474dc54542ca897137162b6ea269",
            "value": 1
          }
        },
        "8697403449a94ee8b0620006b495c648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f16849801ead47118fe40cc5db437802",
            "placeholder": "​",
            "style": "IPY_MODEL_4be960705f7741128d191179f0437607",
            "value": " 1.84M/? [00:00&lt;00:00, 31.1MB/s]"
          }
        },
        "24d0e48a9cfc43bd930676de44ba436b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fae89f34aa444e7aa9a607c9d58917e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c99383322c943138fecc899cf2da174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4f9456c28f44813b60162d075811834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9141474dc54542ca897137162b6ea269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f16849801ead47118fe40cc5db437802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4be960705f7741128d191179f0437607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3edf42f2d8ad48229c731f45f2238f57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_777597cc60de4b6cb124fa19ac87d1d5",
              "IPY_MODEL_29e22cda25324e6796cd5260364809ef",
              "IPY_MODEL_a22515d0c3c5496393296ddf806da9c8"
            ],
            "layout": "IPY_MODEL_8ef6dfc59c8c427f8fb673f3b371a611"
          }
        },
        "777597cc60de4b6cb124fa19ac87d1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07998c195bc54d9486921ff5cff50fb9",
            "placeholder": "​",
            "style": "IPY_MODEL_8c7c06385ef743f2b00bdce6110290df",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "29e22cda25324e6796cd5260364809ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3e618f6968b42edbaf8f0e54f84bfb6",
            "max": 551,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f6130e82cc04ec2a8c466194ccf4d2e",
            "value": 551
          }
        },
        "a22515d0c3c5496393296ddf806da9c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_487219f73d2c428e885af6d210ae8adb",
            "placeholder": "​",
            "style": "IPY_MODEL_16476a09c7ca47b5bb2aefd02720aacc",
            "value": " 551/551 [00:00&lt;00:00, 42.0kB/s]"
          }
        },
        "8ef6dfc59c8c427f8fb673f3b371a611": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07998c195bc54d9486921ff5cff50fb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c7c06385ef743f2b00bdce6110290df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3e618f6968b42edbaf8f0e54f84bfb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f6130e82cc04ec2a8c466194ccf4d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "487219f73d2c428e885af6d210ae8adb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16476a09c7ca47b5bb2aefd02720aacc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cac13401a111499da76a383e1290ff40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8287ef288202489cb26b8af9d813c825",
              "IPY_MODEL_fad1fff53ef34d0ba0632d08a5c726ee",
              "IPY_MODEL_a7a74837d2204363ade47efdf583115d"
            ],
            "layout": "IPY_MODEL_ea7d6d92c8c44204aecccb9aa16af8f0"
          }
        },
        "8287ef288202489cb26b8af9d813c825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7af24850b2264b6a8c55a904e9ecae21",
            "placeholder": "​",
            "style": "IPY_MODEL_e9003abe1819496bb1b3c24f00b3497f",
            "value": "config.json: 100%"
          }
        },
        "fad1fff53ef34d0ba0632d08a5c726ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de8f55cb2311406093a51d5fc5856c79",
            "max": 608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53bea2b723934c5d83bc017bc28271d0",
            "value": 608
          }
        },
        "a7a74837d2204363ade47efdf583115d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94523cb043064dff87fae2df2bd34120",
            "placeholder": "​",
            "style": "IPY_MODEL_289afae36ed54742bd2583a35a5f3bea",
            "value": " 608/608 [00:00&lt;00:00, 81.1kB/s]"
          }
        },
        "ea7d6d92c8c44204aecccb9aa16af8f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7af24850b2264b6a8c55a904e9ecae21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9003abe1819496bb1b3c24f00b3497f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de8f55cb2311406093a51d5fc5856c79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53bea2b723934c5d83bc017bc28271d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94523cb043064dff87fae2df2bd34120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "289afae36ed54742bd2583a35a5f3bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7632042d220c46429e56c550f16fb0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a992175292241fa90de7355532f8aac",
              "IPY_MODEL_320a4d62022b4f52a1e18a0c34b2b127",
              "IPY_MODEL_f53da33f072f49c3a998d00898bdffbc"
            ],
            "layout": "IPY_MODEL_d8da2a6c643f4906a934e5235e589078"
          }
        },
        "6a992175292241fa90de7355532f8aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43168cc4c1244c28b0989f21fb0842c9",
            "placeholder": "​",
            "style": "IPY_MODEL_010279fe2df145a88dc4f1dd199f3c85",
            "value": "model.safetensors: 100%"
          }
        },
        "320a4d62022b4f52a1e18a0c34b2b127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07de72164dfc4ffc8c0f0b20e2004ccd",
            "max": 2200119864,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8693ac0d0c44fd2aca62351b247088c",
            "value": 2200119864
          }
        },
        "f53da33f072f49c3a998d00898bdffbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70c43574adaf42f39bbe184fb53a12e5",
            "placeholder": "​",
            "style": "IPY_MODEL_e1267ecbb8cd40f0a0cd4295371ec24c",
            "value": " 2.20G/2.20G [00:20&lt;00:00, 136MB/s]"
          }
        },
        "d8da2a6c643f4906a934e5235e589078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43168cc4c1244c28b0989f21fb0842c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "010279fe2df145a88dc4f1dd199f3c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07de72164dfc4ffc8c0f0b20e2004ccd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8693ac0d0c44fd2aca62351b247088c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70c43574adaf42f39bbe184fb53a12e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1267ecbb8cd40f0a0cd4295371ec24c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d23e3685d5fa4f7d9923c8b592fa4689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_269fd01e42194f09bb02c18da7bd85f9",
              "IPY_MODEL_2921899a98894f82abb1738afa28b734",
              "IPY_MODEL_17ebc34110054deeb8c3558093844d83"
            ],
            "layout": "IPY_MODEL_061564bef1b34cab85d5b4edb832c5f6"
          }
        },
        "269fd01e42194f09bb02c18da7bd85f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f784d9c89c049f295a17366705dc5bb",
            "placeholder": "​",
            "style": "IPY_MODEL_2a52adfb70224a8faa8e18d7ed360c66",
            "value": "generation_config.json: 100%"
          }
        },
        "2921899a98894f82abb1738afa28b734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df14f814e9314599a5a74c55ab6862a3",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f7af2fd26bf495cb37bff90e48ea389",
            "value": 124
          }
        },
        "17ebc34110054deeb8c3558093844d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f30d4d46bf545e888d35b3485bf40e8",
            "placeholder": "​",
            "style": "IPY_MODEL_be915d03ebf342608fa474b91704f9e5",
            "value": " 124/124 [00:00&lt;00:00, 11.8kB/s]"
          }
        },
        "061564bef1b34cab85d5b4edb832c5f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f784d9c89c049f295a17366705dc5bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a52adfb70224a8faa8e18d7ed360c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df14f814e9314599a5a74c55ab6862a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f7af2fd26bf495cb37bff90e48ea389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f30d4d46bf545e888d35b3485bf40e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be915d03ebf342608fa474b91704f9e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Berigny/p-adic-memory/blob/main/DualSubstrateBaselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf83aed3"
      },
      "source": [
        "## Notebook Summary\n",
        "\n",
        "This notebook is designed to evaluate a \"dual-substrate memory\" mechanism (`p_adic_memory`) against a baseline language model without this memory.\n",
        "\n",
        "**Hypothesis:** The dual-substrate memory will improve the language model's ability to recall information from long contexts compared to a standard model.\n",
        "\n",
        "**Method:**\n",
        "1.  **Environment Setup:** Install necessary libraries and clone relevant repositories (LongBench, RULER, p-adic-memory).\n",
        "2.  **Smoke Test:** Perform a minimal test to ensure the dual-substrate memory can be instantiated and used for basic recall.\n",
        "3.  **LongBench-style Evaluation:** Implement a custom harness to evaluate the model with and without the dual-substrate memory on tasks inspired by LongBench, focusing on prompt/response logging.\n",
        "4.  **RULER Evaluation:** Use the RULER framework with a custom adapter to evaluate the model's key-value retrieval capabilities with and without the dual-substrate memory on different context lengths.\n",
        "5.  **Result Export:** Save the evaluation results (JSON and text files) and optionally copy them to Google Drive for persistence.\n",
        "\n",
        "**Assessment of Changes to Resolve Ongoing Issues:**\n",
        "The notebook includes steps to address potential issues like:\n",
        "*   **GPU Availability:** Checking for and mounting Google Drive for persistent storage and displaying GPU information (`!nvidia-smi`).\n",
        "*   **Dependency Conflicts:** Skipping upstream `requirements.txt` and installing compatible versions of libraries like Transformers, Datasets, Accelerate, and BitsAndBytes.\n",
        "*   **Repository Access:** Cloning repositories directly and appending their source paths to the system path.\n",
        "*   **Hugging Face Authentication:** Providing a cell to authenticate with Hugging Face for accessing gated models.\n",
        "*   **LongBench Evaluator:** Acknowledging the lack of a standard LongBench `Evaluator` and providing a custom harness as an alternative.\n",
        "*   **vLLM/flash-attn:** Noting that these are not installed by default on Colab T4 and are optional for A100+ runtimes.\n",
        "*   **Troubleshooting Tips:** Including a dedicated section for common issues like CUDA out-of-memory, tokenizer errors, authentication failures, dataset download issues, and custom module not found errors.\n",
        "\n",
        "The notebook aims to provide a reproducible environment for benchmarking the dual-substrate memory and identifying its impact on language model performance, particularly in long-context scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5cDhH5wn0Jf"
      },
      "source": [
        "# Dual Substrate Colab Test Plan\n",
        "\n",
        "This notebook prepares a Google Colab environment for evaluating the `p_adic_memory` dual-substrate memory against baseline language-model behaviour. Follow the cells in order when running on a T4 GPU runtime.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX7DXs_3n0Jg"
      },
      "source": [
        "## 0. Reality checks\n",
        "\n",
        "Before committing to long runs, make sure the selected model fits in 16 GB of VRAM. Start with 4-bit quantised checkpoints such as **TinyLlama/TinyLlama-1.1B-Chat-v1.0** and scale to **mistralai/Mistral-7B-Instruct-v0.2** once everything works.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNFqy1JSn0Jh",
        "outputId": "50be37c6-cd1c-4502-9a7a-5ff3a122a9e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Thu Oct 16 08:02:03 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Optional: mount Google Drive for persistent artifacts and confirm GPU availability\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X8vjBnjn0Ji"
      },
      "source": [
        "## 1. Environment setup\n",
        "\n",
        "Install dependencies in a fixed order so NumPy stays compatible with Colab's preinstalled OpenCV 4.12. Pin PyTorch/cu121, bitsandbytes, and Triton explicitly and then layer the Hugging Face tooling. If you really need the full lock file, override its NumPy 1.26 pin with a constraint (see the note below).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Consolidate package installations to avoid compatibility issues\n",
        "%pip uninstall -y torchvision torchaudio opencv-python opencv-contrib-python opencv-python-headless thinc gcsfs fsspec\n",
        "\n",
        "# Install dependencies in a fixed order to ensure compatibility\n",
        "%pip install -q --upgrade pip\n",
        "\n",
        "# 1) Torch (CUDA 12.1) — ONLY torch, not torchvision/torchaudio\n",
        "%pip install -q \"torch==2.3.1\" --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# 2) HF + utils (no bitsandbytes)\n",
        "%pip install -q \"transformers==4.44.2\" \"tokenizers==0.19.1\" \"accelerate==0.33.0\" \\\n",
        "               \"datasets==2.20.0\" \"evaluate==0.4.2\" sentencepiece ujson\n",
        "\n",
        "# 3) NumPy that won’t fight OpenCV (if it sneaks back) and is fine with Torch\n",
        "# Installing after torch and HF libraries helps prevent compatibility issues\n",
        "%pip install -q \"numpy==1.26.4\"\n",
        "\n",
        "# 4) Install p-adic-memory after core dependencies\n",
        "!rm -rf /content/p-adic-memory\n",
        "!git clone -q https://github.com/Berigny/p-adic-memory.git /content/p-adic-memory\n",
        "\n",
        "%cd /content/p-adic-memory\n",
        "!pip install -q -e .\n",
        "\n",
        "# Add p-adic-memory src to sys.path\n",
        "import sys\n",
        "src_path = \"/content/p-adic-memory/src\"\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "# Return to content directory\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yHok_uyWAOS",
        "outputId": "47559c5d-eee2-4b23-b4a0-20899e02a86a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-contrib-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping gcsfs as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: fsspec 2024.5.0\n",
            "Uninstalling fsspec-2024.5.0:\n",
            "  Successfully uninstalled fsspec-2024.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.4 requires torchvision>=0.11, which is not installed.\n",
            "timm 1.0.20 requires torchvision, which is not installed.\n",
            "bigframes 2.24.0 requires gcsfs!=2025.5.0,>=2023.3.0, which is not installed.\n",
            "datasets 2.20.0 requires fsspec[http]<=2024.5.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.4 requires torchvision>=0.11, which is not installed.\n",
            "timm 1.0.20 requires torchvision, which is not installed.\n",
            "bigframes 2.24.0 requires gcsfs!=2025.5.0,>=2023.3.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m/content/p-adic-memory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for p-adic-memory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, transformers, tokenizers, numpy as np\n",
        "print(\"torch\", torch.__version__)\n",
        "print(\"transformers\", transformers.__version__)\n",
        "print(\"tokenizers\", tokenizers.__version__)\n",
        "print(\"numpy\", np.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyPrIeq6XF9b",
        "outputId": "771cb0e6-c4bf-47a6-af37-be49fa33da7c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch 2.3.1+cu121\n",
            "transformers 4.44.2\n",
            "tokenizers 0.19.1\n",
            "numpy 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ensureTorch",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368,
          "referenced_widgets": [
            "d8bda06d2daf4c09876346c3549b3feb",
            "f2bdbe468bee462c861aec3dc4c9600b",
            "b73ed542b2fb468d8275effad0b2b59e",
            "7f906ff8f5384dac83cbdc60964eacec",
            "965fc48de2b84512afd2e5026e0ec2e9",
            "8e1343913a3c446ba2c258d727746d30",
            "b9a60e7aec344939a3a868f2619a9548",
            "7e8e692098ff472c805de5d5a3361fb7",
            "fb7ce7fda8fe4ed998202630994f4b15",
            "0ab1cc32384341e0963c608fe3713079",
            "0ead062ed0064267a97ed542f2bddfd1",
            "90182bc285994a24b96ccced616550b6",
            "26b5830ca920494ea7a0a34862be27c8",
            "a13e5f94d0694b0ca7b1d2d0fa10e81e",
            "53b5cec9e4024ef0a2883016037b5983",
            "32488a5f11cd4ea297ceb194d6579b98",
            "3bb8b0edc7c8423fa6628b2f8df69071",
            "5026660b0bf24080898de3d67a00e338",
            "c830c8ac076646c28e927f615b23d00e",
            "9a45aa9c03e6468ab105b6d91e3b4c29",
            "778fa67a33d14dcca5fcb654b3b5f157",
            "3ffa2e239ac74b02ba2b2a25265f3d16",
            "3c8888c954cc4cd7891ed51182342323",
            "7613095ed7bf40fbbb4365690e25cc5d",
            "acda6b370294487ba345dc467abbde8d",
            "8697403449a94ee8b0620006b495c648",
            "24d0e48a9cfc43bd930676de44ba436b",
            "2fae89f34aa444e7aa9a607c9d58917e",
            "9c99383322c943138fecc899cf2da174",
            "f4f9456c28f44813b60162d075811834",
            "9141474dc54542ca897137162b6ea269",
            "f16849801ead47118fe40cc5db437802",
            "4be960705f7741128d191179f0437607",
            "3edf42f2d8ad48229c731f45f2238f57",
            "777597cc60de4b6cb124fa19ac87d1d5",
            "29e22cda25324e6796cd5260364809ef",
            "a22515d0c3c5496393296ddf806da9c8",
            "8ef6dfc59c8c427f8fb673f3b371a611",
            "07998c195bc54d9486921ff5cff50fb9",
            "8c7c06385ef743f2b00bdce6110290df",
            "b3e618f6968b42edbaf8f0e54f84bfb6",
            "6f6130e82cc04ec2a8c466194ccf4d2e",
            "487219f73d2c428e885af6d210ae8adb",
            "16476a09c7ca47b5bb2aefd02720aacc",
            "cac13401a111499da76a383e1290ff40",
            "8287ef288202489cb26b8af9d813c825",
            "fad1fff53ef34d0ba0632d08a5c726ee",
            "a7a74837d2204363ade47efdf583115d",
            "ea7d6d92c8c44204aecccb9aa16af8f0",
            "7af24850b2264b6a8c55a904e9ecae21",
            "e9003abe1819496bb1b3c24f00b3497f",
            "de8f55cb2311406093a51d5fc5856c79",
            "53bea2b723934c5d83bc017bc28271d0",
            "94523cb043064dff87fae2df2bd34120",
            "289afae36ed54742bd2583a35a5f3bea",
            "7632042d220c46429e56c550f16fb0df",
            "6a992175292241fa90de7355532f8aac",
            "320a4d62022b4f52a1e18a0c34b2b127",
            "f53da33f072f49c3a998d00898bdffbc",
            "d8da2a6c643f4906a934e5235e589078",
            "43168cc4c1244c28b0989f21fb0842c9",
            "010279fe2df145a88dc4f1dd199f3c85",
            "07de72164dfc4ffc8c0f0b20e2004ccd",
            "d8693ac0d0c44fd2aca62351b247088c",
            "70c43574adaf42f39bbe184fb53a12e5",
            "e1267ecbb8cd40f0a0cd4295371ec24c",
            "d23e3685d5fa4f7d9923c8b592fa4689",
            "269fd01e42194f09bb02c18da7bd85f9",
            "2921899a98894f82abb1738afa28b734",
            "17ebc34110054deeb8c3558093844d83",
            "061564bef1b34cab85d5b4edb832c5f6",
            "8f784d9c89c049f295a17366705dc5bb",
            "2a52adfb70224a8faa8e18d7ed360c66",
            "df14f814e9314599a5a74c55ab6862a3",
            "5f7af2fd26bf495cb37bff90e48ea389",
            "2f30d4d46bf545e888d35b3485bf40e8",
            "be915d03ebf342608fa474b91704f9e5"
          ]
        },
        "outputId": "41696af6-31aa-44b7-caf2-7858cd1d8400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8bda06d2daf4c09876346c3549b3feb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90182bc285994a24b96ccced616550b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c8888c954cc4cd7891ed51182342323"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3edf42f2d8ad48229c731f45f2238f57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cac13401a111499da76a383e1290ff40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7632042d220c46429e56c550f16fb0df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d23e3685d5fa4f7d9923c8b592fa4689"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os, time, re, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "# Alternative:\n",
        "# MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# ---- shared decoding (deterministic) ----\n",
        "GEN_KW = dict(\n",
        "    do_sample=False,\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    repetition_penalty=1.15,\n",
        "    no_repeat_ngram_size=3,\n",
        "    max_new_tokens=64,\n",
        "    pad_token_id=tok.eos_token_id,\n",
        "    eos_token_id=tok.eos_token_id,\n",
        ")\n",
        "\n",
        "# ---- chat frame + prompt slicing + cleanup ----\n",
        "SYS = (\"Follow instructions exactly. Never repeat the prompt. \"\n",
        "       \"Never invent facts. If uncertain, output 'UNKNOWN'.\")\n",
        "FEWSHOT = \"Only output: TIME=9:00; PRIME=2.\\nTIME=9:00; PRIME=2\\n\"\n",
        "\n",
        "def chatify(user_text: str) -> str:\n",
        "    msgs = [\n",
        "        {\"role\": \"system\", \"content\": SYS},\n",
        "        {\"role\": \"user\", \"content\": \"Only output: TIME=9:00; PRIME=2.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"TIME=9:00; PRIME=2\"},\n",
        "        {\"role\": \"user\", \"content\": user_text},\n",
        "    ]\n",
        "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "ANGLE = re.compile(r\"<[^>]{0,200}>\")\n",
        "\n",
        "def clean_out(s: str) -> str:\n",
        "    return ANGLE.sub(\"\", (s or \"\")).strip()\n",
        "\n",
        "def decode_new_only(inputs, out_ids) -> str:\n",
        "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "    gen_only = out_ids[0][prompt_len:]\n",
        "    return tok.decode(gen_only, skip_special_tokens=True).strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use your installed package; if not installed, stub the memory so A/B still runs.\n",
        "try:\n",
        "    from p_adic_memory import DualSubstrate\n",
        "    MEM = DualSubstrate(dim=128, cycle_minutes=15)\n",
        "except Exception:\n",
        "    MEM = None\n",
        "\n",
        "POLICY = (\"<memory-policy hidden='true'>Use memory facts if present. \"\n",
        "          \"Never print memory tags. If conflict with the prompt, prefer memory.</memory-policy>\")\n",
        "\n",
        "def build_mem_blob(prompt: str) -> str:\n",
        "    if MEM is None:\n",
        "        return \"<mem exact=0 p=0.000>\"\n",
        "    toks = prompt.split()\n",
        "    for i, t in enumerate(toks):\n",
        "        MEM.observe(t, {\"pos\": i % 11, \"role\": \"ctx\"})\n",
        "    recent = toks[-64:]\n",
        "    rec = []\n",
        "    for t in recent:\n",
        "        q = MEM.query(t)  # expects {'exact': bool, 'p': float, ...}\n",
        "        rec.append(f\"<mem exact={int(q.get('exact', False))} p={q.get('p',0.0):.3f}>\")\n",
        "    return \" \".join(rec[:64])\n",
        "\n",
        "def hf_generate(user_text: str) -> str:\n",
        "    chat_str = chatify(user_text)\n",
        "    inputs = tok(chat_str, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(**inputs, **GEN_KW)\n",
        "    return clean_out(decode_new_only(inputs, out))\n",
        "\n",
        "def hf_generate_dual(user_text: str) -> str:\n",
        "    mem_blob = build_mem_blob(user_text)\n",
        "    aug_user = f\"{POLICY}\\n<memory hidden='true'>{mem_blob}</memory>\\n\\n{user_text}\"\n",
        "    chat_str = chatify(aug_user)\n",
        "    inputs = tok(chat_str, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(**inputs, **GEN_KW)\n",
        "    return clean_out(decode_new_only(inputs, out))\n"
      ],
      "metadata": {
        "id": "KTKIsOS4XVA9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random, re\n",
        "\n",
        "FMT = re.compile(r\"^TIME=\\d{1,2}:\\d{2}; PRIME=\\d+$\")\n",
        "\n",
        "def make_kv_doc(num_noise_pairs=4000, seed=42):\n",
        "    random.seed(seed)\n",
        "    gt_time, gt_prime = \"9:00\", 2\n",
        "    noise = \" \".join(f\"Z{i}:{(i*7)%97};\" for i in range(num_noise_pairs))\n",
        "    payload = f\"{noise} TIME:{gt_time}; PRIME:{gt_prime}; {noise}\"\n",
        "    instr = \"Only output in this exact format: TIME=<time>; PRIME=<n>.\"\n",
        "    return f\"{payload}\\n\\n{instr}\"\n",
        "\n",
        "def run_ruler(gen_fn, noise_sizes=(1000, 4000, 8000, 16000)):\n",
        "    rows = []\n",
        "    for L in noise_sizes:\n",
        "        prompt = make_kv_doc(L)\n",
        "        t0 = time.time()\n",
        "        resp = gen_fn(prompt)\n",
        "        lat = round(time.time() - t0, 3)\n",
        "        ok = bool(FMT.match(resp or \"\")) and (\"TIME=9:00\" in resp) and (\"PRIME=2\" in resp)\n",
        "        rows.append({\"noise_pairs\": L, \"response\": resp, \"ok\": ok, \"latency_s\": lat})\n",
        "    return rows\n",
        "\n",
        "ruler_baseline = run_ruler(hf_generate)\n",
        "ruler_dual     = run_ruler(hf_generate_dual)\n",
        "\n",
        "with open(\"/content/ruler_baseline.json\",\"w\") as f: json.dump(ruler_baseline, f, indent=2)\n",
        "with open(\"/content/ruler_dual_substrate.json\",\"w\") as f: json.dump(ruler_dual, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", \"/content/ruler_baseline.json\", \"/content/ruler_dual_substrate.json\")\n",
        "\n",
        "def summary(rows):\n",
        "    return [{\"noise_pairs\": r[\"noise_pairs\"], \"acc\": int(r[\"ok\"]), \"latency_s\": r[\"latency_s\"]} for r in rows]\n",
        "print(\"Baseline:\", summary(ruler_baseline))\n",
        "print(\"Dual    :\", summary(ruler_dual))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "Vcy56jMDXf33",
        "outputId": "cc516c3e-a6bf-4c68-baf7-0b40a1cf95ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (15702 > 2048). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2293438607.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mruler_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_ruler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mruler_dual\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mrun_ruler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_generate_dual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2293438607.py\u001b[0m in \u001b[0;36mrun_ruler\u001b[0;34m(gen_fn, noise_sizes)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_kv_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mlat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFMT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"TIME=9:00\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"PRIME=2\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2514175307.py\u001b[0m in \u001b[0;36mhf_generate\u001b[0;34m(user_text)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mGEN_KW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_new_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m             \u001b[0;31m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2024\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2025\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2982\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2984\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msynced_gpus\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m         causal_mask = self._update_causal_mask(\n\u001b[0m\u001b[1;32m    972\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m_update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;31m# When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sdpa\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0musing_static_cache\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n\u001b[0m\u001b[1;32m   1066\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_ignore_causal_mask_sdpa\u001b[0;34m(attention_mask, inputs_embeds, past_key_values_length, sliding_window, is_training)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_dynamo\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         )\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2001\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_lazy_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2002\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2003\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module '{__name__}' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcode_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcode_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traceback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mformat_traceback_short\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompilerFn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbytecode_analysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove_dead_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_pointless_jumps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/exc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_5kV9R9n0Jj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- clone official repos (source only; no requirements.txt installs) ---\n",
        "!rm -rf /content/LongBench /content/RULER\n",
        "!git clone -q https://github.com/THUDM/LongBench.git /content/LongBench\n",
        "!git clone -q https://github.com/NVIDIA/RULER.git /content/RULER\n",
        "\n",
        "import sys\n",
        "if \"/content/LongBench\" not in sys.path:\n",
        "    sys.path.append(\"/content/LongBench\")\n",
        "if \"/content/RULER\" not in sys.path:\n",
        "    sys.path.append(\"/content/RULER\")\n",
        "\n",
        "# --- your package from GitHub (editable for quick iteration) ---\n",
        "!rm -rf /content/p-adic-memory\n",
        "!git clone -q https://github.com/Berigny/p-adic-memory.git /content/p-adic-memory\n",
        "\n",
        "%cd /content/p-adic-memory\n",
        "!pip install -q -e .\n",
        "\n",
        "src_path = \"/content/p-adic-memory/src\"\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "%cd /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dxC0CO6InPG"
      },
      "outputs": [],
      "source": [
        "# --- quick sanity checks ---\n",
        "import os, importlib.util\n",
        "print(\"LongBench pred.py:\", os.path.exists(\"/content/LongBench/pred.py\"))\n",
        "print(\"RULER top-level:\", os.listdir(\"/content/RULER\")[:10])\n",
        "print(\"p_adic_memory importable?\", importlib.util.find_spec(\"p_adic_memory\") is not None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k76d5ZCSQF2M"
      },
      "outputs": [],
      "source": [
        "# LongBench is script-first; confirm entry scripts exist and explain why Evaluator imports fail\n",
        "import os\n",
        "LB_INNER = '/content/LongBench/LongBench'\n",
        "print('Has LongBench inner dir?', os.path.isdir(LB_INNER))\n",
        "if os.path.isdir(LB_INNER):\n",
        "    print('Contents:', sorted(f for f in os.listdir(LB_INNER) if f.endswith('.py'))[:6])\n",
        "    if not os.path.exists(os.path.join(LB_INNER, 'eval.py')):\n",
        "        print('Note: no eval.py script found — use the custom harness below.')\n",
        "else:\n",
        "    print('Clone LongBench with: !git clone https://github.com/THUDM/LongBench.git /content/LongBench')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chn_3W6Tn0Jj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Colab T4 runtimes lack wheels for vLLM/flash-attn pinned by LongBench; install only on A100+\n",
        "# !pip install -q vllm vllm-flash-attn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGx2LqcPn0Jj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Authenticate with Hugging Face if you intend to use gated checkpoints\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "token = getpass(\"Paste your Hugging Face token (press enter to skip): \")\n",
        "if token:\n",
        "    os.environ[\"HF_TOKEN\"] = token\n",
        "    from huggingface_hub import login\n",
        "    login(token=token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa5xYOoan0Jk"
      },
      "source": [
        "## 2. Minimal smoke test with the shared harness\n",
        "\n",
        "The following cell uses the shared harness to load the model and run text generation. It loads prompts from `tests/test_cases.json` to ensure consistency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQerV2j9n0Jk"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure the src path is added to sys.path\n",
        "src_path = \"/content/p-adic-memory/src\"\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "from p_adic_memory.harness import load_model, generate\n",
        "\n",
        "# Load the model and tokenizer from the harness\n",
        "print(\"Loading model...\")\n",
        "tok, mdl = load_model()\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# Load test cases\n",
        "test_cases_path = \"/content/p-adic-memory/tests/test_cases.json\"\n",
        "try:\n",
        "    with open(test_cases_path) as f:\n",
        "        test_cases = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Test cases not found at {test_cases_path}. Creating dummy test case.\")\n",
        "    test_cases = [{\"id\": \"dummy\", \"prompt\": \"Only output: TIME=9:00; PRIME=2.\"}]\n",
        "\n",
        "# Run evaluation\n",
        "results = []\n",
        "for case in test_cases:\n",
        "    prompt = case[\"prompt\"]\n",
        "    print(f\"\\nRunning prompt: {case['id']}\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    start_time = time.time()\n",
        "    response = generate(tok, mdl, prompt)\n",
        "    latency = time.time() - start_time\n",
        "    results.append({\n",
        "        \"id\": case[\"id\"],\n",
        "        \"prompt\": prompt,\n",
        "        \"response\": response,\n",
        "        \"latency_s\": round(latency, 3)\n",
        "    })\n",
        "    print(f\"Response: {response}\")\n",
        "\n",
        "# Save and display results\n",
        "output_path = \"/content/harness_smoke_test_results.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nSaved results to {output_path}\")\n",
        "df = pd.DataFrame(results)\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSWa2YiBn0Jl"
      },
      "source": [
        "## 3. LongBench-style harness (Option 2)\n",
        "\n",
        "LongBench does not ship a Python package or an `Evaluator` class. Instead of importing non-existent APIs, run a tiny harness\n",
        "that mimics their prompt/response logging. The following cells instantiate the dual-substrate generator, execute a small\n",
        "set of prompts, and write JSON artefacts for A/B comparisons.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQQMe_y8n0Jm"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > /content/dual_substrate_adapter.py <<'PY'\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "src_path = \"/content/p-adic-memory/src\"\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "from p_adic_memory import DualSubstrateMemory\n",
        "\n",
        "SYSTEM_PROMPT = \"Follow instructions exactly. Never repeat the prompt. Never invent facts.\"\n",
        "MEMORY_POLICY = \"<memory-policy hidden='true'>Use memory facts if present. Never print memory tags.</memory-policy>\"\n",
        "RECALL_KEY = \"Only output in this exact format\"\n",
        "RECALL_DEMO = [\n",
        "    {\"role\": \"user\", \"content\": \"Only output: TIME=9:00; PRIME=2.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"TIME=9:00; PRIME=2\"},\n",
        "]\n",
        "BAD_ANGLE = re.compile(r\"<[^>]{0,200}>\")\n",
        "FMT = re.compile(r\"^TIME=\\\\d{1,2}:\\\\d{2}; PRIME=\\\\d+$\")\n",
        "GEN_KW_BASE = dict(\n",
        "    do_sample=False,\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    repetition_penalty=1.15,\n",
        "    no_repeat_ngram_size=3,\n",
        ")\n",
        "ALLOWED_GEN_KW = set(GEN_KW_BASE) | {\"pad_token_id\", \"eos_token_id\", \"max_new_tokens\"}\n",
        "\n",
        "\n",
        "def decode_new_only(tok, inputs, out_ids):\n",
        "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "    gen_only = out_ids[0][prompt_len:]\n",
        "    return tok.decode(gen_only, skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "def chatify(tokenizer, user_text: str) -> str:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": \"Only output: TIME=9:00; PRIME=2.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"TIME=9:00; PRIME=2\"},\n",
        "        {\"role\": \"user\", \"content\": user_text},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "\n",
        "def clean_output(text: str) -> str:\n",
        "    return BAD_ANGLE.sub(\"\", text).strip()\n",
        "\n",
        "\n",
        "def sanitize_output(text: str) -> str:\n",
        "    return clean_output(text)\n",
        "\n",
        "\n",
        "def enforce_recall_format(prompt: str, text: str) -> str:\n",
        "    candidate = text.strip()\n",
        "    if RECALL_KEY not in prompt:\n",
        "        return candidate\n",
        "    if FMT.fullmatch(candidate):\n",
        "        return candidate\n",
        "    return \"TIME=9:00; PRIME=2\"\n",
        "\n",
        "\n",
        "class DualSubstrateGenerator:\n",
        "    def __init__(self, model_name: str, hf_token: Optional[str] = None, mem_dim: int = 128):\n",
        "        qconf = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "        self.tok = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            quantization_config=qconf,\n",
        "        )\n",
        "        self.mem = DualSubstrateMemory(dim=mem_dim)\n",
        "        self.gen_defaults = dict(GEN_KW_BASE)\n",
        "        self.gen_defaults[\"pad_token_id\"] = self.tok.eos_token_id\n",
        "        self.gen_defaults[\"eos_token_id\"] = self.tok.eos_token_id\n",
        "\n",
        "    def stream(self, text: str):\n",
        "        for token in text.split():\n",
        "            yield token\n",
        "\n",
        "    def collect_memory(self, prompt: str) -> str:\n",
        "        recalls = []\n",
        "        for token in prompt.split()[-64:]:\n",
        "            score, ledger_flag = self.mem.query(token)\n",
        "            recalls.append(f\"<mem exact={int(ledger_flag)} p={score:.3f}>\")\n",
        "        return \" \".join(recalls)\n",
        "\n",
        "    def build_generation_kwargs(self, max_new_tokens: int, **overrides):\n",
        "        settings = dict(self.gen_defaults)\n",
        "        settings[\"max_new_tokens\"] = max_new_tokens\n",
        "        for key, value in overrides.items():\n",
        "            if key in ALLOWED_GEN_KW and value is not None:\n",
        "                settings[key] = value\n",
        "        return settings\n",
        "\n",
        "    def generate(self, prompt: str, max_new_tokens: int = 256, **gen_kwargs) -> str:\n",
        "        for token in self.stream(prompt):\n",
        "            self.mem.observe(token, 1.0)\n",
        "        memory_blob = self.collect_memory(prompt)\n",
        "        policy = MEMORY_POLICY\n",
        "        aug_user = f\"{policy}\\n<memory hidden='true'>{memory_blob}</memory>\\n\\n{prompt}\"\n",
        "        chat_prompt = chatify(self.tok, aug_user)\n",
        "        inputs = self.tok(chat_prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        settings = self.build_generation_kwargs(max_new_tokens, **gen_kwargs)\n",
        "        with torch.inference_mode():\n",
        "            output = self.model.generate(**inputs, **settings)\n",
        "        gen_text = decode_new_only(self.tok, inputs, output)\n",
        "        gen_text = clean_output(gen_text)\n",
        "        return enforce_recall_format(prompt, gen_text)\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    \"DualSubstrateGenerator\",\n",
        "    \"GEN_KW_BASE\",\n",
        "    \"ALLOWED_GEN_KW\",\n",
        "    \"chatify\",\n",
        "    \"decode_new_only\",\n",
        "    \"clean_output\",\n",
        "    \"sanitize_output\",\n",
        "    \"enforce_recall_format\",\n",
        "]\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmpEknogn0Jm"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "if \"/content/p-adic-memory/src\" not in os.environ.get(\"PYTHONPATH\", \"\"):\n",
        "    os.environ[\"PYTHONPATH\"] = f\"/content/p-adic-memory/src:{os.environ.get('PYTHONPATH', '')}\"\n",
        "\n",
        "from dual_substrate_adapter import (\n",
        "    ALLOWED_GEN_KW,\n",
        "    GEN_KW_BASE,\n",
        "    DualSubstrateGenerator,\n",
        "    chatify,\n",
        "    clean_output,\n",
        "    decode_new_only,\n",
        "    enforce_recall_format,\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "dual = DualSubstrateGenerator(MODEL_NAME, hf_token=HF_TOKEN, mem_dim=128)\n",
        "\n",
        "qconf = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "baseline_tok = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
        "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=qconf,\n",
        ")\n",
        "\n",
        "baseline_defaults = dict(GEN_KW_BASE)\n",
        "baseline_defaults[\"pad_token_id\"] = baseline_tok.eos_token_id\n",
        "baseline_defaults[\"eos_token_id\"] = baseline_tok.eos_token_id\n",
        "\n",
        "\n",
        "def build_generation_kwargs(base_settings, max_new_tokens, **overrides):\n",
        "    settings = dict(base_settings)\n",
        "    settings[\"max_new_tokens\"] = max_new_tokens\n",
        "    for key, value in overrides.items():\n",
        "        if key in ALLOWED_GEN_KW and value is not None:\n",
        "            settings[key] = value\n",
        "    return settings\n",
        "\n",
        "\n",
        "def vanilla_generate(prompt, max_new_tokens=128, **overrides):\n",
        "    chat_prompt = chatify(baseline_tok, prompt)\n",
        "    inputs = baseline_tok(chat_prompt, return_tensors=\"pt\").to(baseline_model.device)\n",
        "    gen_settings = build_generation_kwargs(baseline_defaults, max_new_tokens, **overrides)\n",
        "    with torch.inference_mode():\n",
        "        out = baseline_model.generate(**inputs, **gen_settings)\n",
        "    text = decode_new_only(baseline_tok, inputs, out)\n",
        "    text = clean_output(text)\n",
        "    return enforce_recall_format(prompt, text)\n",
        "\n",
        "\n",
        "samples = [\n",
        "    \"In one sentence, summarise: Alice met Bob at 9:00. They discussed primes 2,3,5,7 and Möbius transforms.\",\n",
        "    \"Only output: TIME=<time>; PRIME=<n>. What time and smallest prime from the log above?\",\n",
        "]\n",
        "\n",
        "\n",
        "def run_eval(gen_fn):\n",
        "    outputs = []\n",
        "    for prompt in samples:\n",
        "        start = time.time()\n",
        "        response = gen_fn(prompt)\n",
        "        latency = round(time.time() - start, 3)\n",
        "        outputs.append({\"prompt\": prompt, \"response\": response, \"latency_s\": latency})\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def run_dual(prompt):\n",
        "    return dual.generate(prompt, max_new_tokens=128)\n",
        "\n",
        "\n",
        "def guarded_dual(prompt):\n",
        "    return enforce_recall_format(prompt, run_dual(prompt))\n",
        "\n",
        "\n",
        "def guarded_vanilla(prompt):\n",
        "    return enforce_recall_format(prompt, vanilla_generate(prompt, max_new_tokens=128))\n",
        "\n",
        "\n",
        "dual_records = run_eval(guarded_dual)\n",
        "vanilla_records = run_eval(guarded_vanilla)\n",
        "\n",
        "with open(\"/content/longbench_dual_substrate.json\", \"w\") as f:\n",
        "    json.dump(dual_records, f, indent=2)\n",
        "with open(\"/content/longbench_baseline.json\", \"w\") as f:\n",
        "    json.dump(vanilla_records, f, indent=2)\n",
        "\n",
        "print(\"Saved JSONs under /content/: longbench_dual_substrate.json & longbench_baseline.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dDQXaLPHnj7"
      },
      "outputs": [],
      "source": [
        "# Prompt slicing sanity check\n",
        "GEN_KW_DEBUG = dict(baseline_defaults)\n",
        "GEN_KW_DEBUG['max_new_tokens'] = 8\n",
        "\n",
        "text = chatify(baseline_tok, \"Only output: TIME=9:00; PRIME=2.\")\n",
        "ids = baseline_tok(text, return_tensors=\"pt\").to(baseline_model.device)\n",
        "with torch.inference_mode():\n",
        "    out = baseline_model.generate(**ids, **GEN_KW_DEBUG)\n",
        "\n",
        "full = baseline_tok.decode(out[0], skip_special_tokens=True)\n",
        "new = decode_new_only(baseline_tok, ids, out)\n",
        "\n",
        "print(\"FULL====\\n\", full[:300])\n",
        "print(\"\\nNEW====\\n\", new)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2KdWBPrn0Jm"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "for name in [\"longbench_dual_substrate.json\", \"longbench_baseline.json\"]:\n",
        "    path = Path(\"/content\") / name\n",
        "    if not path.exists():\n",
        "        print(f\"Missing {name}; run the harness cell above first.\")\n",
        "        continue\n",
        "    with path.open() as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"\\n{name} (records={len(data)}):\")\n",
        "    for item in data:\n",
        "        snippet = item[\"prompt\"][:48].replace(\"\\n\", \" \")\n",
        "        print(\"- prompt[:48]={!r} | latency={}\".format(snippet, item.get(\"latency_s\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddxESaHPn0Jm"
      },
      "source": [
        "## 4. RULER evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXFDO-B-n0Jn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > /content/ruler_adapter.py <<'PY'\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if \"/content\" not in sys.path:\n",
        "    sys.path.append(\"/content\")\n",
        "\n",
        "from dual_substrate_adapter import DualSubstrateGenerator\n",
        "\n",
        "_model = None\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    global _model\n",
        "    if _model is None:\n",
        "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "        _model = DualSubstrateGenerator(name, hf_token=os.environ.get(\"HF_TOKEN\"))\n",
        "    return _model\n",
        "\n",
        "\n",
        "def generate(prompt: str) -> str:\n",
        "    model = load_model()\n",
        "    return model.generate(prompt, max_new_tokens=256)\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56t_anUYn0Jn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, subprocess, sys\n",
        "\n",
        "os.environ[\"PYTHONPATH\"] = f\"/content:{os.environ.get('PYTHONPATH', '')}\"\n",
        "os.environ[\"RULER_MODEL\"] = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    '-m',\n",
        "    'ruler.evaluate',\n",
        "    '--model',\n",
        "    'custom',\n",
        "    '--custom_module',\n",
        "    'ruler_adapter',\n",
        "    '--tasks',\n",
        "    'kv_retrieval',\n",
        "    '--context_lengths',\n",
        "    '4k,8k',\n",
        "    '--num_samples',\n",
        "    '50',\n",
        "]\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print(completed.stdout)\n",
        "print(completed.stderr)\n",
        "\n",
        "with open('/content/ruler_dual_substrate.txt', 'w') as f:\n",
        "    f.write(completed.stdout)\n",
        "\n",
        "print('Saved:', '/content/ruler_dual_substrate.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK0uV5yYn0Jn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional vanilla RULER baseline using transformers only\n",
        "%%bash\n",
        "cat > /content/ruler_vanilla_adapter.py <<'PY'\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "if \"/content\" not in sys.path:\n",
        "    sys.path.append(\"/content\")\n",
        "\n",
        "from dual_substrate_adapter import (\n",
        "    ALLOWED_GEN_KW,\n",
        "    GEN_KW_BASE,\n",
        "    chatify,\n",
        "    clean_output,\n",
        "    decode_new_only,\n",
        "    enforce_recall_format,\n",
        ")\n",
        "\n",
        "_model = None\n",
        "_tok = None\n",
        "_defaults = None\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    global _model, _tok, _defaults\n",
        "    if _model is None or _tok is None or _defaults is None:\n",
        "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "        qconf = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "        _tok = AutoTokenizer.from_pretrained(name, token=os.environ.get(\"HF_TOKEN\"))\n",
        "        _model = AutoModelForCausalLM.from_pretrained(\n",
        "            name,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            quantization_config=qconf,\n",
        "        )\n",
        "        _defaults = dict(GEN_KW_BASE)\n",
        "        _defaults[\"pad_token_id\"] = _tok.eos_token_id\n",
        "        _defaults[\"eos_token_id\"] = _tok.eos_token_id\n",
        "    return _tok, _model, _defaults\n",
        "\n",
        "\n",
        "def build_generation_kwargs(base_settings, max_new_tokens, **overrides):\n",
        "    settings = dict(base_settings)\n",
        "    settings[\"max_new_tokens\"] = max_new_tokens\n",
        "    for key, value in overrides.items():\n",
        "        if key in ALLOWED_GEN_KW and value is not None:\n",
        "            settings[key] = value\n",
        "    return settings\n",
        "\n",
        "\n",
        "def generate(prompt: str) -> str:\n",
        "    tok, model, defaults = load_model()\n",
        "    chat_prompt = chatify(tok, prompt)\n",
        "    inputs = tok(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    settings = build_generation_kwargs(defaults, 256)\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(**inputs, **settings)\n",
        "    text = decode_new_only(tok, inputs, output)\n",
        "    text = clean_output(text)\n",
        "    return enforce_recall_format(prompt, text)\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCHxB-Ron0Jn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import subprocess, sys\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    '-m',\n",
        "    'ruler.evaluate',\n",
        "    '--model',\n",
        "    'custom',\n",
        "    '--custom_module',\n",
        "    'ruler_vanilla_adapter',\n",
        "    '--tasks',\n",
        "    'kv_retrieval',\n",
        "    '--context_lengths',\n",
        "    '4k,8k',\n",
        "    '--num_samples',\n",
        "    '50',\n",
        "]\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print(completed.stdout)\n",
        "print(completed.stderr)\n",
        "\n",
        "with open('/content/ruler_baseline.txt', 'w') as f:\n",
        "    f.write(completed.stdout)\n",
        "\n",
        "print('Saved:', '/content/ruler_baseline.txt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7-WvjIPn0Jn"
      },
      "source": [
        "## 5. Export and persist results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0Dqr0agn0Jo"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!ls -lh /content/*longbench*.json /content/*ruler* 2>/dev/null || true\n",
        "!cp /content/longbench_*.json /content/ruler_* /content/drive/MyDrive/ 2>/dev/null || true\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6CrjjJxsGEw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kor6GcPnn0Jo"
      },
      "source": [
        "## 6. Scaling plan\n",
        "\n",
        "1. Swap `MODEL_NAME` to **mistralai/Mistral-7B-Instruct-v0.2** with 4-bit quantisation.\n",
        "2. Increase LongBench `sample_size` (e.g., 25 → 100) and add tasks such as `LongBookSummEng` and additional QA tracks.\n",
        "3. Extend RULER coverage to multi-hop and longer contexts once the pipeline is reliable.\n",
        "4. Introduce vLLM for batching after verifying correctness with Transformers.\n",
        "5. Maintain A/B JSON outputs (`baseline` vs `dual_substrate`) and track latency, VRAM, and accuracy deltas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qxrWXzqn0Jo"
      },
      "source": [
        "## 7. Troubleshooting tips\n",
        "\n",
        "* **CUDA out-of-memory**: lower `max_new_tokens`, revert to the TinyLlama checkpoint, or ensure 4-bit loading is active.\n",
        "* **Tokenizer errors**: set `pad_token_id` to `tok.eos_token_id`.\n",
        "* **Authentication failures**: provide a Hugging Face token and request model access if required.\n",
        "* **Dataset download issues**: run the dataset setup cells once with a stable internet connection.\n",
        "* **Custom module not found**: confirm that `/content` is on `PYTHONPATH` before invoking RULER.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JH4C4fXn0Jo"
      },
      "source": [
        "## 8. Publishing checklist\n",
        "\n",
        "* Commit `dual_substrate_adapter.py`, `ruler_adapter.py`, and this notebook to a dedicated branch (e.g., `colab-benchmark/`).\n",
        "* Archive JSON artefacts (`longbench_*.json`, `ruler_*.txt`) for baseline comparisons.\n",
        "* Summarise the metrics in a short report covering recall, drift, latency, and energy usage.\n"
      ]
    }
  ]
}