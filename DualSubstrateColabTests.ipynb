{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Berigny/p-adic-memory/blob/main/DualSubstrateColabTests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5cDhH5wn0Jf"
      },
      "source": [
        "# Dual Substrate Colab Test Plan\n",
        "\n",
        "This notebook prepares a Google Colab environment for evaluating the `p_adic_memory` dual-substrate memory against baseline language-model behaviour. Follow the cells in order when running on a T4 GPU runtime.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX7DXs_3n0Jg"
      },
      "source": [
        "## 0. Reality checks\n",
        "\n",
        "Before committing to long runs, make sure the selected model fits in 16\u00a0GB of VRAM. Start with 4-bit quantised checkpoints such as **TinyLlama/TinyLlama-1.1B-Chat-v1.0** and scale to **mistralai/Mistral-7B-Instruct-v0.2** once everything works.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNFqy1JSn0Jh",
        "outputId": "ee7d1e32-6741-459c-923f-11965fb537b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Mon Oct 13 13:05:41 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Optional: mount Google Drive for persistent artifacts and confirm GPU availability\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X8vjBnjn0Ji"
      },
      "source": [
        "## 1. Environment setup\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- base deps (GPU-friendly) ---\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q \"transformers>=4.44.0\" \"datasets>=2.20.0\" \"evaluate>=0.4.2\" accelerate bitsandbytes sentencepiece\n",
        "\n"
      ],
      "metadata": {
        "id": "Sh49T71MrdJE",
        "outputId": "70c43358-92a5-42a6-d536-44b51b15dde6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_5kV9R9n0Jj",
        "outputId": "1e005217-c8c3-4469-bd2a-c6722b74257b"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- clone official repos (not pip) ---\n",
        "!rm -rf LongBench RULER\n",
        "!git clone -q https://github.com/THUDM/LongBench.git\n",
        "!git clone -q https://github.com/NVIDIA/RULER.git\n",
        "%cd LongBench\n",
        "!pip install -q -r requirements.txt || true\n",
        "%cd /content/RULER\n",
        "!pip install -q -r requirements.txt || true\n",
        "%cd /content\n",
        "\n",
        "import sys\n",
        "sys.path += [\"/content/LongBench\", \"/content/RULER\"]\n",
        "\n",
        "# --- your package from GitHub (editable for quick iteration) ---\n",
        "!rm -rf p-adic-memory\n",
        "!git clone -q https://github.com/Berigny/p-adic-memory.git\n",
        "%cd p-adic-memory\n",
        "!pip install -q -e .\n",
        "%cd /content\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- quick sanity checks ---\n",
        "import os\n",
        "print(\"LongBench pred.py:\", os.path.exists(\"/content/LongBench/pred.py\"))\n",
        "print(\"RULER scripts dir:\", os.listdir(\"/content/RULER\")[:10])\n",
        "import p_adic_memory as pam\n",
        "print(\"p_adic_memory version:\", getattr(pam, \"__version__\", \"dev\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chn_3W6Tn0Jj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional: install vLLM if batching/throughput becomes a requirement later\n",
        "# !pip install -q vllm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGx2LqcPn0Jj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Authenticate with Hugging Face if you intend to use gated checkpoints\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "token = getpass(\"Paste your Hugging Face token (press enter to skip): \")\n",
        "if token:\n",
        "    os.environ[\"HF_TOKEN\"] = token\n",
        "    from huggingface_hub import login\n",
        "    login(token=token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa5xYOoan0Jk"
      },
      "source": [
        "## 2. Minimal dual-substrate smoke test\n",
        "\n",
        "The following cell instantiates a quantised model, attaches the dual-substrate memory, and produces a JSON log comparing prompts and responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQerV2j9n0Jk"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json, time, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from p_adic_memory import DualSubstrateMemory\n",
        "import os\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "# To scale later (requires token + 4-bit):\n",
        "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=os.environ.get(\"HF_TOKEN\"))\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "mem = DualSubstrateMemory(dim=128, cycle_minutes=15)\n",
        "\n",
        "\n",
        "def stream_tokens(text: str):\n",
        "    for i, t in enumerate(text.split()):\n",
        "        yield t, {\"pos\": i % 7, \"role\": \"ctx\"}\n",
        "\n",
        "\n",
        "def augment_with_memory(prompt: str, tokens_now):\n",
        "    recalls = []\n",
        "    for t in tokens_now:\n",
        "        q = mem.query(t)\n",
        "        recalls.append(f\"<mem exact={int(q.get('exact', False))} p={q.get('p', 0.0):.3f}>\")\n",
        "    tag = \" \".join(recalls[:64])\n",
        "    return f\"{prompt}\\n\\n<memory>{tag}</memory>\"\n",
        "\n",
        "\n",
        "def dual_substrate_generate(prompt: str, max_new_tokens=256, temperature=0.2):\n",
        "    for token, label in stream_tokens(prompt):\n",
        "        mem.observe(token, label)\n",
        "    current_tokens = prompt.split()[-64:]\n",
        "    augmented = augment_with_memory(prompt, current_tokens)\n",
        "    inputs = tok(augmented, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tok.eos_token_id\n",
        "        )\n",
        "    return tok.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "queries = [\n",
        "    \"Summarise the following log: Alice met Bob at 9:00. They discussed primes 2, 3, 5, 7 and M\u00f6bius transforms.\",\n",
        "    \"Recall the meeting time and the smallest prime they discussed.\",\n",
        "]\n",
        "\n",
        "results = []\n",
        "for q in queries:\n",
        "    start = time.time()\n",
        "    response = dual_substrate_generate(q, max_new_tokens=64)\n",
        "    latency = time.time() - start\n",
        "    results.append({\"prompt\": q, \"response\": response, \"latency_s\": round(latency, 3)})\n",
        "\n",
        "with open(\"/content/dual_substrate_smoke.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", \"/content/dual_substrate_smoke.json\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqIpzAZzn0Jl"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Baseline comparison without memory augmentation\n",
        "baseline = []\n",
        "for q in queries:\n",
        "    start = time.time()\n",
        "    inputs = tok(q, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            do_sample=True,\n",
        "            temperature=0.2,\n",
        "            max_new_tokens=64,\n",
        "            pad_token_id=tok.eos_token_id\n",
        "        )\n",
        "    response = tok.decode(output[0], skip_special_tokens=True)\n",
        "    latency = time.time() - start\n",
        "    baseline.append({\"prompt\": q, \"response\": response, \"latency_s\": round(latency, 3)})\n",
        "\n",
        "with open(\"/content/baseline_smoke.json\", \"w\") as f:\n",
        "    json.dump(baseline, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", \"/content/baseline_smoke.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO3Np7w1n0Jl"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optionally copy smoke-test outputs to Drive for persistence\n",
        "!cp /content/*smoke.json /content/drive/MyDrive/ 2>/dev/null || true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSWa2YiBn0Jl"
      },
      "source": [
        "## 3. LongBench integration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQQMe_y8n0Jm"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > /content/dual_substrate_adapter.py <<'PY'\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from p_adic_memory import DualSubstrateMemory\n",
        "\n",
        "\n",
        "class DualSubstrateGenerator:\n",
        "    def __init__(self, model_name: str, hf_token: str | None = None, mem_dim: int = 128, cycle_minutes: int = 15):\n",
        "        qconf = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "        self.tok = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            quantization_config=qconf,\n",
        "        )\n",
        "        self.mem = DualSubstrateMemory(dim=mem_dim, cycle_minutes=cycle_minutes)\n",
        "\n",
        "    def stream(self, text: str):\n",
        "        for i, token in enumerate(text.split()):\n",
        "            yield token, {\"pos\": i % 11}\n",
        "\n",
        "    def augment(self, prompt: str, tokens_now):\n",
        "        recalls = []\n",
        "        for token in tokens_now:\n",
        "            q = self.mem.query(token)\n",
        "            recalls.append(f\"<mem exact={int(q.get(\\\"exact\\\", False))} p={q.get(\\\"p\\\", 0.0):.3f}>\")\n",
        "        tag = \" \\\".join(recalls[:64])\n",
        "        return f\"{prompt}\\n\\n<memory>{tag}</memory>\"\n",
        "\n",
        "    def generate(self, prompt: str, max_new_tokens: int = 256, temperature: float = 0.2):\n",
        "        for token, label in self.stream(prompt):\n",
        "            self.mem.observe(token, label)\n",
        "        current_tokens = prompt.split()[-64:]\n",
        "        augmented = self.augment(prompt, current_tokens)\n",
        "        inputs = self.tok(augmented, return_tensors=\"pt\").to(self.model.device)\n",
        "        with torch.inference_mode():\n",
        "            output = self.model.generate(\n",
        "                **inputs,\n",
        "                do_sample=True,\n",
        "                temperature=temperature,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                pad_token_id=self.tok.eos_token_id,\n",
        "            )\n",
        "        return self.tok.decode(output[0], skip_special_tokens=True)\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmpEknogn0Jm"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, json\n",
        "from longbench import Evaluator\n",
        "from dual_substrate_adapter import DualSubstrateGenerator\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "generator = DualSubstrateGenerator(model_name, hf_token=os.environ.get(\"HF_TOKEN\"))\n",
        "\n",
        "def custom_generate(text: str) -> str:\n",
        "    return generator.generate(text, max_new_tokens=256, temperature=0.2)\n",
        "\n",
        "evaluator = Evaluator(model=custom_generate, dataset=\"longbench\")\n",
        "results = evaluator.evaluate(tasks=[\"MultiDocQA\"], sample_size=25)\n",
        "\n",
        "with open(\"/content/longbench_dual_substrate.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", \"/content/longbench_dual_substrate.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2KdWBPrn0Jm"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vanilla baseline on the same LongBench slice\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "baseline_tok = AutoTokenizer.from_pretrained(model_name, token=os.environ.get(\"HF_TOKEN\"))\n",
        "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "def vanilla_generate(text: str) -> str:\n",
        "    inputs = baseline_tok(text, return_tensors=\"pt\").to(baseline_model.device)\n",
        "    with torch.inference_mode():\n",
        "        output = baseline_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            pad_token_id=baseline_tok.eos_token_id,\n",
        "        )\n",
        "    return baseline_tok.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "evaluator_baseline = Evaluator(model=vanilla_generate, dataset=\"longbench\")\n",
        "results_baseline = evaluator_baseline.evaluate(tasks=[\"MultiDocQA\"], sample_size=25)\n",
        "\n",
        "with open(\"/content/longbench_baseline.json\", \"w\") as f:\n",
        "    json.dump(results_baseline, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", \"/content/longbench_baseline.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddxESaHPn0Jm"
      },
      "source": [
        "## 4. RULER evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXFDO-B-n0Jn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > /content/ruler_adapter.py <<'PY'\n",
        "import os\n",
        "from dual_substrate_adapter import DualSubstrateGenerator\n",
        "\n",
        "_model = None\n",
        "\n",
        "def load_model():\n",
        "    global _model\n",
        "    if _model is None:\n",
        "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "        _model = DualSubstrateGenerator(name, hf_token=os.environ.get(\"HF_TOKEN\"))\n",
        "    return _model\n",
        "\n",
        "def generate(prompt: str) -> str:\n",
        "    model = load_model()\n",
        "    return model.generate(prompt, max_new_tokens=256, temperature=0.2)\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56t_anUYn0Jn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, subprocess, sys\n",
        "\n",
        "os.environ[\"PYTHONPATH\"] = f\"/content:{os.environ.get('PYTHONPATH', '')}\"\n",
        "os.environ[\"RULER_MODEL\"] = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    '-m',\n",
        "    'ruler.evaluate',\n",
        "    '--model',\n",
        "    'custom',\n",
        "    '--custom_module',\n",
        "    'ruler_adapter',\n",
        "    '--tasks',\n",
        "    'kv_retrieval',\n",
        "    '--context_lengths',\n",
        "    '4k,8k',\n",
        "    '--num_samples',\n",
        "    '50',\n",
        "]\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print(completed.stdout)\n",
        "print(completed.stderr)\n",
        "\n",
        "with open('/content/ruler_dual_substrate.txt', 'w') as f:\n",
        "    f.write(completed.stdout)\n",
        "\n",
        "print('Saved:', '/content/ruler_dual_substrate.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK0uV5yYn0Jn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional vanilla RULER baseline using transformers only\n",
        "%%bash\n",
        "cat > /content/ruler_vanilla_adapter.py <<'PY'\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "_model = None\n",
        "_tok = None\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    global _model, _tok\n",
        "    if _model is None or _tok is None:\n",
        "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "        qconf = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "        _tok = AutoTokenizer.from_pretrained(name, token=os.environ.get(\"HF_TOKEN\"))\n",
        "        _model = AutoModelForCausalLM.from_pretrained(\n",
        "            name,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            quantization_config=qconf,\n",
        "        )\n",
        "    return _tok, _model\n",
        "\n",
        "\n",
        "def generate(prompt: str) -> str:\n",
        "    tok, model = load_model()\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            pad_token_id=tok.eos_token_id,\n",
        "        )\n",
        "    return tok.decode(output[0], skip_special_tokens=True)\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCHxB-Ron0Jn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import subprocess, sys\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    '-m',\n",
        "    'ruler.evaluate',\n",
        "    '--model',\n",
        "    'custom',\n",
        "    '--custom_module',\n",
        "    'ruler_vanilla_adapter',\n",
        "    '--tasks',\n",
        "    'kv_retrieval',\n",
        "    '--context_lengths',\n",
        "    '4k,8k',\n",
        "    '--num_samples',\n",
        "    '50',\n",
        "]\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print(completed.stdout)\n",
        "print(completed.stderr)\n",
        "\n",
        "with open('/content/ruler_baseline.txt', 'w') as f:\n",
        "    f.write(completed.stdout)\n",
        "\n",
        "print('Saved:', '/content/ruler_baseline.txt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7-WvjIPn0Jn"
      },
      "source": [
        "## 5. Export and persist results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0Dqr0agn0Jo"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!ls -lh /content/*longbench*.json /content/*ruler* 2>/dev/null || true\n",
        "!cp /content/longbench_*.json /content/ruler_* /content/drive/MyDrive/ 2>/dev/null || true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kor6GcPnn0Jo"
      },
      "source": [
        "## 6. Scaling plan\n",
        "\n",
        "1. Swap `MODEL_NAME` to **mistralai/Mistral-7B-Instruct-v0.2** with 4-bit quantisation.\n",
        "2. Increase LongBench `sample_size` (e.g., 25 \u2192 100) and add tasks such as `LongBookSummEng` and additional QA tracks.\n",
        "3. Extend RULER coverage to multi-hop and longer contexts once the pipeline is reliable.\n",
        "4. Introduce vLLM for batching after verifying correctness with Transformers.\n",
        "5. Maintain A/B JSON outputs (`baseline` vs `dual_substrate`) and track latency, VRAM, and accuracy deltas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qxrWXzqn0Jo"
      },
      "source": [
        "## 7. Troubleshooting tips\n",
        "\n",
        "* **CUDA out-of-memory**: lower `max_new_tokens`, revert to the TinyLlama checkpoint, or ensure 4-bit loading is active.\n",
        "* **Tokenizer errors**: set `pad_token_id` to `tok.eos_token_id`.\n",
        "* **Authentication failures**: provide a Hugging Face token and request model access if required.\n",
        "* **Dataset download issues**: run the dataset setup cells once with a stable internet connection.\n",
        "* **Custom module not found**: confirm that `/content` is on `PYTHONPATH` before invoking RULER.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JH4C4fXn0Jo"
      },
      "source": [
        "## 8. Publishing checklist\n",
        "\n",
        "* Commit `dual_substrate_adapter.py`, `ruler_adapter.py`, and this notebook to a dedicated branch (e.g., `colab-benchmark/`).\n",
        "* Archive JSON artefacts (`longbench_*.json`, `ruler_*.txt`) for baseline comparisons.\n",
        "* Summarise the metrics in a short report covering recall, drift, latency, and energy usage.\n"
      ]
    }
  ]
}
