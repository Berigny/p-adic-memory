{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Berigny/p-adic-memory/blob/main/DualSubstrateColabTests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5cDhH5wn0Jf"
   },
   "source": [
    "# Dual Substrate Colab Test Plan\n",
    "\n",
    "This notebook prepares a Google Colab environment for evaluating the `p_adic_memory` dual-substrate memory against baseline language-model behaviour. Follow the cells in order when running on a T4 GPU runtime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX7DXs_3n0Jg"
   },
   "source": [
    "## 0. Reality checks\n",
    "\n",
    "Before committing to long runs, make sure the selected model fits in 16\u00a0GB of VRAM. Start with 4-bit quantised checkpoints such as **TinyLlama/TinyLlama-1.1B-Chat-v1.0** and scale to **mistralai/Mistral-7B-Instruct-v0.2** once everything works.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hNFqy1JSn0Jh",
    "outputId": "0bae0147-627f-48b0-c618-a6df2749e06a"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional: mount Google Drive for persistent artifacts and confirm GPU availability\n",
    "from google.colab import drive\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-X8vjBnjn0Ji"
   },
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "Skip the upstream \\`requirements.txt\\` files on Colab. They pin incompatible CUDA builds (vLLM/flash-attn)\n",
    "and NumPy releases that do not support Python 3.11. Instead, install a modern Transformers stack and\n",
    "pull the benchmark repos in source mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- base deps tuned for Colab T4 (no vLLM / flash-attn) ---\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q \"numpy>=1.26\" \"transformers>=4.44\" \"datasets>=2.20\"\n",
    "!pip install -q \"evaluate>=0.4.2\" \"accelerate>=0.33\" \"bitsandbytes>=0.43\" \\\n",
    "               sentencepiece ujson nltk rouge-score tyro tabulate\n",
    "\n"
   ],
   "metadata": {
    "id": "Sh49T71MrdJE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_5kV9R9n0Jj",
    "outputId": "f3d3a5d0-48c2-42d8-e2b2-9a35f51f11e6"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- clone official repos (source only; no requirements.txt installs) ---\n",
    "!rm -rf /content/LongBench /content/RULER\n",
    "!git clone -q https://github.com/THUDM/LongBench.git /content/LongBench\n",
    "!git clone -q https://github.com/NVIDIA/RULER.git /content/RULER\n",
    "\n",
    "import sys\n",
    "if \"/content/LongBench\" not in sys.path:\n",
    "    sys.path.append(\"/content/LongBench\")\n",
    "if \"/content/RULER\" not in sys.path:\n",
    "    sys.path.append(\"/content/RULER\")\n",
    "\n",
    "# --- your package from GitHub (editable for quick iteration) ---\n",
    "!rm -rf /content/p-adic-memory\n",
    "!git clone -q https://github.com/Berigny/p-adic-memory.git /content/p-adic-memory\n",
    "\n",
    "%cd /content/p-adic-memory\n",
    "!pip install -q -e .\n",
    "# !python -m pip show -f p-adic-memory | sed -n '1,160p'\n",
    "\n",
    "src_path = \"/content/p-adic-memory/src\"\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "%cd /content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dxC0CO6InPG",
    "outputId": "f95d342e-4ccc-4b36-9094-d20be07bc2a7"
   },
   "outputs": [],
   "source": [
    "# --- quick sanity checks ---\n",
    "import os, importlib.util\n",
    "print(\"LongBench pred.py:\", os.path.exists(\"/content/LongBench/pred.py\"))\n",
    "print(\"RULER top-level:\", os.listdir(\"/content/RULER\")[:10])\n",
    "print(\"p_adic_memory importable?\", importlib.util.find_spec(\"p_adic_memory\") is not None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LongBench is script-first; confirm entry scripts exist and explain why Evaluator imports fail\n",
    "import os\n",
    "LB_INNER = '/content/LongBench/LongBench'\n",
    "print('Has LongBench inner dir?', os.path.isdir(LB_INNER))\n",
    "if os.path.isdir(LB_INNER):\n",
    "    print('Contents:', sorted(f for f in os.listdir(LB_INNER) if f.endswith('.py'))[:6])\n",
    "    if not os.path.exists(os.path.join(LB_INNER, 'eval.py')):\n",
    "        print('Note: no eval.py script found \u2014 use the custom harness below.')\n",
    "else:\n",
    "    print('Clone LongBench with: !git clone https://github.com/THUDM/LongBench.git /content/LongBench')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "chn_3W6Tn0Jj"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Colab T4 runtimes lack wheels for vLLM/flash-attn pinned by LongBench; install only on A100+\n",
    "# !pip install -q vllm vllm-flash-attn\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CGx2LqcPn0Jj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0d5e8809-49e0-44de-9541-36d3d91f8289"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face if you intend to use gated checkpoints\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "token = getpass(\"Paste your Hugging Face token (press enter to skip): \")\n",
    "if token:\n",
    "    os.environ[\"HF_TOKEN\"] = token\n",
    "    from huggingface_hub import login\n",
    "    login(token=token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa5xYOoan0Jk"
   },
   "source": [
    "## 2. Minimal dual-substrate smoke test\n",
    "\n",
    "The following cell instantiates a quantised model, attaches the dual-substrate memory, and produces a JSON log comparing prompts and responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nQerV2j9n0Jk",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "outputId": "4fb2e053-3a16-4f97-dc6f-c7c44fc4f72f"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json, time, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure the src/ directory is available on sys.path when running from notebooks\n",
    "src_path = \"/content/p-adic-memory/src\"\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from p_adic_memory import DualSubstrate\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# To scale later (requires token + 4-bit):\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=os.environ.get(\"HF_TOKEN\"))\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "def as_chat(user_text: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Answer succinctly and do not repeat the question.\"},\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.15,\n",
    "    pad_token_id=tok.eos_token_id,\n",
    "    eos_token_id=tok.eos_token_id,\n",
    ")\n",
    "\n",
    "mem = DualSubstrate(dim=128, cycle=15 * 60)\n",
    "\n",
    "def stream_tokens(text: str):\n",
    "    for token in text.split():\n",
    "        yield token\n",
    "\n",
    "def augment_with_memory(prompt: str, tokens_now):\n",
    "    recalls = []\n",
    "    for token in tokens_now:\n",
    "        score, ledger_flag = mem.query(token)\n",
    "        recalls.append(f\"<mem exact={int(ledger_flag)} p={score:.3f}>\")\n",
    "    policy = \"<memory-policy>Use memory facts if present. If memory and the prompt disagree, prefer memory. Output only what is requested; do not repeat the question.</memory-policy>\"\n",
    "    tag = \" \".join(recalls[:64])\n",
    "    return f\"{policy}\n",
    "<memory>{tag}</memory>\n",
    "\n",
    "{prompt}\"\n",
    "\n",
    "def generate_chat_response(user_text: str, max_new_tokens: int = 64) -> str:\n",
    "    chat_prompt = as_chat(user_text)\n",
    "    inputs = tok(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        generated = model.generate(**inputs, max_new_tokens=max_new_tokens, **gen_kwargs)\n",
    "    response_ids = generated[:, inputs.input_ids.shape[-1]:]\n",
    "    return tok.decode(response_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "def dual_substrate_generate(prompt: str, max_new_tokens: int = 64) -> str:\n",
    "    for idx, token in enumerate(stream_tokens(prompt)):\n",
    "        mem.observe(token, 1.0)\n",
    "    tokens_now = prompt.split()[-64:]\n",
    "    augmented = augment_with_memory(prompt, tokens_now)\n",
    "    return generate_chat_response(augmented, max_new_tokens=max_new_tokens)\n",
    "\n",
    "queries = [\n",
    "    \"In one sentence, summarise the following log:\n",
    "Alice met Bob at 9:00. They discussed primes 2, 3, 5, 7 and M\u00f6bius transforms.\",\n",
    "    \"Recall the meeting time and the smallest prime they discussed. Only output in this exact format: TIME=<time>; PRIME=<n>.\",\n",
    "]\n",
    "\n",
    "dual_results = []\n",
    "for q in queries:\n",
    "    start = time.time()\n",
    "    response = dual_substrate_generate(q, max_new_tokens=64)\n",
    "    latency = time.time() - start\n",
    "    dual_results.append({\"prompt\": q, \"response\": response, \"latency_s\": round(latency, 3)})\n",
    "\n",
    "with open(\"/content/dual_substrate_smoke.json\", \"w\") as f:\n",
    "    json.dump(dual_results, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", \"/content/dual_substrate_smoke.json\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cabaa043"
   },
   "source": [
    "import importlib.util, p_adic_memory as pam\n",
    "\n",
    "print(\"p_adic_memory importable?\", importlib.util.find_spec(\"p_adic_memory\") is not None)\n",
    "print(\"version:\", getattr(pam, \"__version__\", \"dev\"))\n",
    "print(\"has DualSubstrate:\", hasattr(pam, \"DualSubstrate\"))\n",
    "print(\"has DualSubstrateMemory:\", hasattr(pam, \"DualSubstrateMemory\"))\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mqIpzAZzn0Jl"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Baseline comparison without memory augmentation\n",
    "baseline = []\n",
    "for q in queries:\n",
    "    start = time.time()\n",
    "    response = generate_chat_response(q, max_new_tokens=64)\n",
    "    latency = time.time() - start\n",
    "    baseline.append({\"prompt\": q, \"response\": response, \"latency_s\": round(latency, 3)})\n",
    "\n",
    "with open(\"/content/baseline_smoke.json\", \"w\") as f:\n",
    "    json.dump(baseline, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", \"/content/baseline_smoke.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick accuracy/latency scorer for the recall task\n",
    "import json, re\n",
    "\n",
    "def load(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def parse_out(text: str):\n",
    "    match = re.search(r\"TIME\\s*=\\s*([0-9]{1,2}:[0-9]{2})\\s*;\\s*PRIME\\s*=\\s*([0-9]+)\", text)\n",
    "    if not match:\n",
    "        return None, None\n",
    "    return match.group(1), int(match.group(2))\n",
    "\n",
    "def score(records):\n",
    "    gt_time, gt_prime = \"9:00\", 2\n",
    "    data = []\n",
    "    for r in records:\n",
    "        if \"Recall the meeting time\" not in r[\"prompt\"]:\n",
    "            continue\n",
    "        time_out, prime_out = parse_out(r[\"response\"])\n",
    "        ok_time = time_out == gt_time\n",
    "        ok_prime = prime_out == gt_prime\n",
    "        data.append({\"latency_s\": r.get(\"latency_s\"), \"ok_time\": ok_time, \"ok_prime\": ok_prime})\n",
    "    if not data:\n",
    "        return {\"n\": 0}\n",
    "    acc_time = sum(d[\"ok_time\"] for d in data) / len(data)\n",
    "    acc_prime = sum(d[\"ok_prime\"] for d in data) / len(data)\n",
    "    latencies = [d[\"latency_s\"] for d in data if d[\"latency_s\"] is not None]\n",
    "    median_latency = sorted(latencies)[len(latencies) // 2] if latencies else None\n",
    "    return {\"n\": len(data), \"acc_time\": acc_time, \"acc_prime\": acc_prime, \"median_latency\": median_latency}\n",
    "\n",
    "baseline = load(\"/content/baseline_smoke.json\")\n",
    "dual = load(\"/content/dual_substrate_smoke.json\")\n",
    "print(\"Baseline:\", score(baseline))\n",
    "print(\"Dual-substrate:\", score(dual))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sO3Np7w1n0Jl"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optionally copy smoke-test outputs to Drive for persistence\n",
    "!cp /content/*smoke.json /content/drive/MyDrive/ 2>/dev/null || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSWa2YiBn0Jl"
   },
   "source": [
    "## 3. LongBench-style harness (Option 2)\n",
    "\n",
    "LongBench does not ship a Python package or an `Evaluator` class. Instead of importing non-existent APIs, run a tiny harness\n",
    "that mimics their prompt/response logging. The following cells instantiate the dual-substrate generator, execute a small\n",
    "set of prompts, and write JSON artefacts for A/B comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qQQMe_y8n0Jm"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > /content/dual_substrate_adapter.py <<'PY'\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from p_adic_memory import DualSubstrateMemory\n",
    "\n",
    "\n",
    "class DualSubstrateGenerator:\n",
    "    def __init__(self, model_name: str, hf_token: str | None = None, mem_dim: int = 128):\n",
    "        qconf = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "        self.tok = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=qconf,\n",
    "        )\n",
    "        self.mem = DualSubstrateMemory(dim=mem_dim)\n",
    "\n",
    "    def stream(self, text: str):\n",
    "        for token in text.split():\n",
    "            yield token\n",
    "\n",
    "    def augment(self, prompt: str, tokens_now):\n",
    "        recalls = []\n",
    "        for token in tokens_now:\n",
    "            score, ledger_flag = self.mem.query(token)\n",
    "            recalls.append(f\"<mem exact={int(ledger_flag)} p={score:.3f}>\")\n",
    "        tag = \" \".join(recalls[:64])\n",
    "        return f\"{prompt}\\n\\n<memory>{tag}</memory>\"\n",
    "\n",
    "    def generate(self, prompt: str, max_new_tokens: int = 256, temperature: float = 0.2):\n",
    "        for token in self.stream(prompt):\n",
    "            self.mem.observe(token, 1.0)\n",
    "        current_tokens = prompt.split()[-64:]\n",
    "        augmented = self.augment(prompt, current_tokens)\n",
    "        inputs = self.tok(augmented, return_tensors=\"pt\").to(self.model.device)\n",
    "        with torch.inference_mode():\n",
    "            output = self.model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=self.tok.eos_token_id,\n",
    "            )\n",
    "        return self.tok.decode(output[0], skip_special_tokens=True)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kmpEknogn0Jm"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json, os, time, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from dual_substrate_adapter import DualSubstrateGenerator\n",
    "\n",
    "MODEL_NAME = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "\n",
    "dual = DualSubstrateGenerator(MODEL_NAME, hf_token=HF_TOKEN, mem_dim=128)\n",
    "\n",
    "qconf = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "baseline_tok = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=qconf,\n",
    ")\n",
    "\n",
    "def vanilla_generate(prompt: str, max_new_tokens: int = 128):\n",
    "    inputs = baseline_tok(prompt, return_tensors=\"pt\").to(baseline_model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = baseline_model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=baseline_tok.eos_token_id,\n",
    "            eos_token_id=baseline_tok.eos_token_id,\n",
    "        )\n",
    "    return baseline_tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "samples = [\n",
    "    \"In one sentence, summarise: Alice met Bob at 9:00. They discussed primes 2,3,5,7 and M\u00f6bius transforms.\",\n",
    "    \"Only output: TIME=<time>; PRIME=<n>. What time and smallest prime from the log above?\",\n",
    "]\n",
    "\n",
    "def run_eval(gen_fn):\n",
    "    outputs = []\n",
    "    for prompt in samples:\n",
    "        start = time.time()\n",
    "        response = gen_fn(prompt)\n",
    "        latency = round(time.time() - start, 3)\n",
    "        outputs.append({\"prompt\": prompt, \"response\": response, \"latency_s\": latency})\n",
    "    return outputs\n",
    "\n",
    "dual_records = run_eval(lambda p: dual.generate(p, max_new_tokens=128, temperature=0.0))\n",
    "vanilla_records = run_eval(lambda p: vanilla_generate(p, max_new_tokens=128))\n",
    "\n",
    "with open(\"/content/longbench_dual_substrate.json\", \"w\") as f:\n",
    "    json.dump(dual_records, f, indent=2)\n",
    "with open(\"/content/longbench_baseline.json\", \"w\") as f:\n",
    "    json.dump(vanilla_records, f, indent=2)\n",
    "\n",
    "print(\"Saved JSONs under /content/: longbench_dual_substrate.json & longbench_baseline.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C2KdWBPrn0Jm"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "for name in [\"longbench_dual_substrate.json\", \"longbench_baseline.json\"]:\n",
    "    path = Path(\"/content\") / name\n",
    "    if not path.exists():\n",
    "        print(f\"Missing {name}; run the harness cell above first.\")\n",
    "        continue\n",
    "    with path.open() as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"\n{name} (records={len(data)}):\")\n",
    "    for item in data:\n",
    "        snippet = item[\"prompt\"][:48].replace(\"\n\", \" \")\n",
    "        print(\"- prompt[:48]={!r} | latency={}\".format(snippet, item.get(\"latency_s\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddxESaHPn0Jm"
   },
   "source": [
    "## 4. RULER evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AXFDO-B-n0Jn"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > /content/ruler_adapter.py <<'PY'\n",
    "import os\n",
    "from dual_substrate_adapter import DualSubstrateGenerator\n",
    "\n",
    "_model = None\n",
    "\n",
    "def load_model():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        _model = DualSubstrateGenerator(name, hf_token=os.environ.get(\"HF_TOKEN\"))\n",
    "    return _model\n",
    "\n",
    "def generate(prompt: str) -> str:\n",
    "    model = load_model()\n",
    "    return model.generate(prompt, max_new_tokens=256, temperature=0.2)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "56t_anUYn0Jn"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, subprocess, sys\n",
    "\n",
    "os.environ[\"PYTHONPATH\"] = f\"/content:{os.environ.get('PYTHONPATH', '')}\"\n",
    "os.environ[\"RULER_MODEL\"] = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    '-m',\n",
    "    'ruler.evaluate',\n",
    "    '--model',\n",
    "    'custom',\n",
    "    '--custom_module',\n",
    "    'ruler_adapter',\n",
    "    '--tasks',\n",
    "    'kv_retrieval',\n",
    "    '--context_lengths',\n",
    "    '4k,8k',\n",
    "    '--num_samples',\n",
    "    '50',\n",
    "]\n",
    "\n",
    "print('Running:', ' '.join(cmd))\n",
    "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(completed.stdout)\n",
    "print(completed.stderr)\n",
    "\n",
    "with open('/content/ruler_dual_substrate.txt', 'w') as f:\n",
    "    f.write(completed.stdout)\n",
    "\n",
    "print('Saved:', '/content/ruler_dual_substrate.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hK0uV5yYn0Jn"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional vanilla RULER baseline using transformers only\n",
    "%%bash\n",
    "cat > /content/ruler_vanilla_adapter.py <<'PY'\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "_model = None\n",
    "_tok = None\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    global _model, _tok\n",
    "    if _model is None or _tok is None:\n",
    "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        qconf = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "        _tok = AutoTokenizer.from_pretrained(name, token=os.environ.get(\"HF_TOKEN\"))\n",
    "        _model = AutoModelForCausalLM.from_pretrained(\n",
    "            name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=qconf,\n",
    "        )\n",
    "    return _tok, _model\n",
    "\n",
    "\n",
    "def generate(prompt: str) -> str:\n",
    "    tok, model = load_model()\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    return tok.decode(output[0], skip_special_tokens=True)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tCHxB-Ron0Jn"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    '-m',\n",
    "    'ruler.evaluate',\n",
    "    '--model',\n",
    "    'custom',\n",
    "    '--custom_module',\n",
    "    'ruler_vanilla_adapter',\n",
    "    '--tasks',\n",
    "    'kv_retrieval',\n",
    "    '--context_lengths',\n",
    "    '4k,8k',\n",
    "    '--num_samples',\n",
    "    '50',\n",
    "]\n",
    "\n",
    "print('Running:', ' '.join(cmd))\n",
    "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(completed.stdout)\n",
    "print(completed.stderr)\n",
    "\n",
    "with open('/content/ruler_baseline.txt', 'w') as f:\n",
    "    f.write(completed.stdout)\n",
    "\n",
    "print('Saved:', '/content/ruler_baseline.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7-WvjIPn0Jn"
   },
   "source": [
    "## 5. Export and persist results\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D0Dqr0agn0Jo"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ls -lh /content/*longbench*.json /content/*ruler* 2>/dev/null || true\n",
    "!cp /content/longbench_*.json /content/ruler_* /content/drive/MyDrive/ 2>/dev/null || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kor6GcPnn0Jo"
   },
   "source": [
    "## 6. Scaling plan\n",
    "\n",
    "1. Swap `MODEL_NAME` to **mistralai/Mistral-7B-Instruct-v0.2** with 4-bit quantisation.\n",
    "2. Increase LongBench `sample_size` (e.g., 25 \u2192 100) and add tasks such as `LongBookSummEng` and additional QA tracks.\n",
    "3. Extend RULER coverage to multi-hop and longer contexts once the pipeline is reliable.\n",
    "4. Introduce vLLM for batching after verifying correctness with Transformers.\n",
    "5. Maintain A/B JSON outputs (`baseline` vs `dual_substrate`) and track latency, VRAM, and accuracy deltas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qxrWXzqn0Jo"
   },
   "source": [
    "## 7. Troubleshooting tips\n",
    "\n",
    "* **CUDA out-of-memory**: lower `max_new_tokens`, revert to the TinyLlama checkpoint, or ensure 4-bit loading is active.\n",
    "* **Tokenizer errors**: set `pad_token_id` to `tok.eos_token_id`.\n",
    "* **Authentication failures**: provide a Hugging Face token and request model access if required.\n",
    "* **Dataset download issues**: run the dataset setup cells once with a stable internet connection.\n",
    "* **Custom module not found**: confirm that `/content` is on `PYTHONPATH` before invoking RULER.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JH4C4fXn0Jo"
   },
   "source": [
    "## 8. Publishing checklist\n",
    "\n",
    "* Commit `dual_substrate_adapter.py`, `ruler_adapter.py`, and this notebook to a dedicated branch (e.g., `colab-benchmark/`).\n",
    "* Archive JSON artefacts (`longbench_*.json`, `ruler_*.txt`) for baseline comparisons.\n",
    "* Summarise the metrics in a short report covering recall, drift, latency, and energy usage.\n"
   ]
  }
 ]
}