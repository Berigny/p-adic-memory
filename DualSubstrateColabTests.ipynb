{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Berigny/p-adic-memory/blob/main/DualSubstrateColabTests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5cDhH5wn0Jf"
      },
      "source": [
        "# Dual Substrate Colab Test Plan\n",
        "\n",
        "This notebook prepares a Google Colab environment for evaluating the `p_adic_memory` dual-substrate memory against baseline language-model behaviour. Follow the cells in order when running on a T4 GPU runtime.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX7DXs_3n0Jg"
      },
      "source": [
        "## 0. Reality checks\n",
        "\n",
        "Before committing to long runs, make sure the selected model fits in 16 GB of VRAM. Start with 4-bit quantised checkpoints such as **TinyLlama/TinyLlama-1.1B-Chat-v1.0** and scale to **mistralai/Mistral-7B-Instruct-v0.2** once everything works.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNFqy1JSn0Jh",
        "outputId": "fc0474a2-c050-4114-b3aa-5cbfb2bfde0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Tue Oct 14 13:54:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Optional: mount Google Drive for persistent artifacts and confirm GPU availability\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X8vjBnjn0Ji"
      },
      "source": [
        "## 1. Environment setup\n",
        "\n",
        "Skip the upstream \\`requirements.txt\\` files on Colab. They pin incompatible CUDA builds (vLLM/flash-attn)\n",
        "and NumPy releases that do not support Python 3.11. Instead, install a modern Transformers stack and\n",
        "pull the benchmark repos in source mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- base deps tuned for Colab T4 (no vLLM / flash-attn) ---\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q \"numpy>=1.26\" \"transformers>=4.44\" \"datasets>=2.20\"\n",
        "!pip install -q \"evaluate>=0.4.2\" \"accelerate>=0.33\" \"bitsandbytes>=0.43\" \\\n",
        "               sentencepiece ujson nltk rouge-score tyro tabulate\n",
        "\n"
      ],
      "metadata": {
        "id": "Sh49T71MrdJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b85d613-f876-4c32-af6c-535cb645d224"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'rouge-score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge-score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_5kV9R9n0Jj",
        "outputId": "9269d11f-1c18-4e05-f88e-d009d801bdec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/p-adic-memory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for p-adic-memory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# --- clone official repos (source only; no requirements.txt installs) ---\n",
        "!rm -rf /content/LongBench /content/RULER\n",
        "!git clone -q https://github.com/THUDM/LongBench.git /content/LongBench\n",
        "!git clone -q https://github.com/NVIDIA/RULER.git /content/RULER\n",
        "\n",
        "import sys\n",
        "if \"/content/LongBench\" not in sys.path:\n",
        "    sys.path.append(\"/content/LongBench\")\n",
        "if \"/content/RULER\" not in sys.path:\n",
        "    sys.path.append(\"/content/RULER\")\n",
        "\n",
        "# --- your package from GitHub (editable for quick iteration) ---\n",
        "!rm -rf /content/p-adic-memory\n",
        "!git clone -q https://github.com/Berigny/p-adic-memory.git /content/p-adic-memory\n",
        "\n",
        "%cd /content/p-adic-memory\n",
        "!pip install -q -e .\n",
        "# !python -m pip show -f p-adic-memory | sed -n '1,160p'\n",
        "\n",
        "src_path = \"/content/p-adic-memory/src\"\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "%cd /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dxC0CO6InPG",
        "outputId": "966120ad-cf0b-41c5-db53-20926626b2f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LongBench pred.py: True\n",
            "RULER top-level: ['.git', 'docker', '.gitignore', '.gitattributes', 'README.md', 'LICENSE', 'scripts']\n",
            "p_adic_memory importable? True\n"
          ]
        }
      ],
      "source": [
        "# --- quick sanity checks ---\n",
        "import os, importlib.util\n",
        "print(\"LongBench pred.py:\", os.path.exists(\"/content/LongBench/pred.py\"))\n",
        "print(\"RULER top-level:\", os.listdir(\"/content/RULER\")[:10])\n",
        "print(\"p_adic_memory importable?\", importlib.util.find_spec(\"p_adic_memory\") is not None)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k76d5ZCSQF2M",
        "outputId": "d5448a33-176d-4511-af0a-ee7041a55b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Has LongBench inner dir? True\n",
            "Contents: ['eval.py', 'llama_flash_attn_monkey_patch.py', 'metrics.py', 'pred.py']\n"
          ]
        }
      ],
      "source": [
        "# LongBench is script-first; confirm entry scripts exist and explain why Evaluator imports fail\n",
        "import os\n",
        "LB_INNER = '/content/LongBench/LongBench'\n",
        "print('Has LongBench inner dir?', os.path.isdir(LB_INNER))\n",
        "if os.path.isdir(LB_INNER):\n",
        "    print('Contents:', sorted(f for f in os.listdir(LB_INNER) if f.endswith('.py'))[:6])\n",
        "    if not os.path.exists(os.path.join(LB_INNER, 'eval.py')):\n",
        "        print('Note: no eval.py script found — use the custom harness below.')\n",
        "else:\n",
        "    print('Clone LongBench with: !git clone https://github.com/THUDM/LongBench.git /content/LongBench')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chn_3W6Tn0Jj"
      },
      "execution_count": 6,
      "outputs": [],
      "source": [
        "# Colab T4 runtimes lack wheels for vLLM/flash-attn pinned by LongBench; install only on A100+\n",
        "# !pip install -q vllm vllm-flash-attn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGx2LqcPn0Jj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01f269b0-81d8-4113-a4dc-efadc1fbbe06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your Hugging Face token (press enter to skip): ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "# Authenticate with Hugging Face if you intend to use gated checkpoints\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "token = getpass(\"Paste your Hugging Face token (press enter to skip): \")\n",
        "if token:\n",
        "    os.environ[\"HF_TOKEN\"] = token\n",
        "    from huggingface_hub import login\n",
        "    login(token=token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa5xYOoan0Jk"
      },
      "source": [
        "## 2. Minimal dual-substrate smoke test\n",
        "\n",
        "The following cell instantiates a quantised model, attaches the dual-substrate memory, and produces a JSON log comparing prompts and responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQerV2j9n0Jk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404,
          "referenced_widgets": [
            "a3b733432e8d4e5c98a3fa382d5ae14d",
            "91b3f7e7edd0427b9f45a26d79164ce8",
            "29ec88be915e486eb2bfe1c154291ed9",
            "c1ee735b23d0427eaead851fcb766e09",
            "40d2c0dc344a4bb19c18758fbacae811",
            "ab230512428c447a90f5681318a68447",
            "cbeb139094c24d1791df991dacbd4444",
            "f7f03f876f014ce1929740ea13885d07",
            "1dc13fa70245422f961be63312517f0c",
            "e1689b5b8191434a897fc45625f75f9a",
            "ca2b480bb33342e08957bcae0c340f2c",
            "a919e809f0ac46d585e0bcbbdd18144d",
            "d64b8d3cfadc43819b884218ea632989",
            "2ea3cf0246e244fdb93665d49c0c126d",
            "753c1f17ef534076abd0b06ba9d4e0f9",
            "a4390966b3e34698aaffd2de0e64af53",
            "59a3c84dbc7c4baa940e2566c983a5d1",
            "50eac8bd3f1048818926ca970ff01c21",
            "59dc9ea1f3b246c3b82983e979ee1de9",
            "fc9b479d522b4e69bfea1d42b2d8126f",
            "cb5385bc2d2c411490c0a9e6058f942b",
            "d5d45fc29e15489c898c0c5e6cbcf3e1",
            "3e8bc423d47248438e388ffffab485ef",
            "70bdf1684cae4a5283b337814521f71b",
            "1ac0f8943ca248d1800c17f760ba7f36",
            "bff1ee3aa47846bf91a3ba59d5df238a",
            "14d1e4edf0f544829a60799c26121f96",
            "41896ec8708a4888b03ad842d07ff369",
            "c6f2c1dd332f46dab916e838bfa7ca12",
            "970fe2c9500844edbaca55a2bda72b98",
            "59226fb5d6b04b3bb207e1b6079cf282",
            "e8e05d840b304a16909e4cef76098ea5",
            "391e78e3126e4ba3a3eb557193c5d71f",
            "2c8322ac6a5942d997f9c58022461036",
            "bf8b7eeb85b244809621d0c707a93a19",
            "546bcdb9db094d9991c8e72740f2a04b",
            "221264bc778b4cafbf4a6717dfe5e7ff",
            "99d6ae0616b240ffaec4fa9f55ce261f",
            "e61f2838a81746af81392147592167c3",
            "6a2f558195494d8d837fb4bd7fd86591",
            "4d611e319e5b44ccac65532d270a6f60",
            "97dbf47fac134b6191b5113ffb4ccb18",
            "3d7a381dfaa14cc1afe5cfe9e7e30cf8",
            "7d63cae239214187895f3ae76beaf9e9",
            "d654a66e0123487082d47fc28903dd14",
            "616f2172fa334f42833597970ca8c9bd",
            "01f4f99106eb464a957c0cd789f6fdb8",
            "d9d0db620cf24d4fa7f7355897f57444",
            "30361faec6d94453a620de4c4d9a3554",
            "815d9b14b3b64928aa1e89581c125985",
            "4d238059e0ff46cc9dc696769bf1296f",
            "bd81fdabfe3647e7937d26a64df78fac",
            "4d41a9a6b92641cb8868b11d4dde3a1a",
            "07ed9fb630544906bef2a8d51a0545eb",
            "9ccc8a87ad364e1f951debcc566524e0",
            "71206f39c8254811a2f6e5664608cce5",
            "eb43f3a35ae147f1b168da72ffc4e0b8",
            "3b9e1a186ef24d2ea0a502fcda0709c7",
            "0060eb3f6f654499b8144a5200f5195e",
            "95ad6935a6cc4f1f88fef8e138c91d7c",
            "953c563d57144be286fc708071d5285a",
            "0cdf938e6f514d1eaa14e4e2f356a082",
            "3772e615de3a4314a2f6497d61423b2d",
            "84300af64fee491f8a75224780463d3e",
            "49f2013120174299a46aa5d7b4cef490",
            "0f12306291a04fca906b851681df692e",
            "321c0de81e8c47f6927156d790125149",
            "d66d8fe56a6c4f72978243fb8ec0d6ac",
            "508e933e2f2045c7a914c404289d1464",
            "fe4bf4aeebf74383b22e997bba9ad0ed",
            "0bc66f55cbbe4163bfa69ab642bb2e18",
            "59fea08354614d5bbe115913d6a388d3",
            "d8d31b3ce85d47628d78d5ebc89852b5",
            "1640c77c6a454b9fbf221f2257ba5299",
            "f0849104eff2479f867ed330b2ba80c7",
            "2fcb3e3b2fc8425a9bbac18930370b63",
            "4f79046075bf46f0a82d63e98249c1e8"
          ]
        },
        "outputId": "6c59ab9b-19b5-4772-cb85-903d7ad696a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3b733432e8d4e5c98a3fa382d5ae14d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a919e809f0ac46d585e0bcbbdd18144d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e8bc423d47248438e388ffffab485ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c8322ac6a5942d997f9c58022461036"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d654a66e0123487082d47fc28903dd14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71206f39c8254811a2f6e5664608cce5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "321c0de81e8c47f6927156d790125149"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/dual_substrate_smoke.json\n"
          ]
        }
      ],
      "source": [
        "import json, time, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Ensure the src/ directory is available on sys.path when running from notebooks\n",
        "src_path = \"/content/p-adic-memory/src\"\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "from p_adic_memory.dual_substrate import DualSubstrate, DualSubstrateMemory\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "# To scale later (requires token + 4-bit):\n",
        "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=os.environ.get(\"HF_TOKEN\"))\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "def as_chat(user_text: str) -> str:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Answer succinctly and do not repeat the question.\"},\n",
        "        {\"role\": \"user\", \"content\": user_text},\n",
        "    ]\n",
        "    return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "gen_kwargs = dict(\n",
        "    do_sample=False,\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    repetition_penalty=1.15,\n",
        "    pad_token_id=tok.eos_token_id,\n",
        "    eos_token_id=tok.eos_token_id,\n",
        ")\n",
        "\n",
        "mem = DualSubstrate(dim=128, cycle=15 * 60)\n",
        "\n",
        "def stream_tokens(text: str):\n",
        "    for token in text.split():\n",
        "        yield token\n",
        "\n",
        "def augment_with_memory(prompt: str, tokens_now):\n",
        "    recalls = []\n",
        "    for token in tokens_now:\n",
        "        score, ledger_flag = mem.query(token)\n",
        "        recalls.append(f\"<mem exact={int(ledger_flag)} p={score:.3f}>\")\n",
        "    policy = \"<memory-policy>Use memory facts if present. If memory and the prompt disagree, prefer memory. Output only what is requested; do not repeat the question.</memory-policy>\"\n",
        "    tag = \" \".join(recalls[:64])\n",
        "    return f\"{policy}\\n<memory>{tag}</memory>\\n\\n{prompt}\"\n",
        "\n",
        "def generate_chat_response(user_text: str, max_new_tokens: int = 64) -> str:\n",
        "    chat_prompt = as_chat(user_text)\n",
        "    inputs = tok(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        generated = model.generate(**inputs, max_new_tokens=max_new_tokens, **gen_kwargs)\n",
        "    response_ids = generated[:, inputs.input_ids.shape[-1]:]\n",
        "    return tok.decode(response_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "def dual_substrate_generate(prompt: str, max_new_tokens: int = 64) -> str:\n",
        "    for idx, token in enumerate(stream_tokens(prompt)):\n",
        "        mem.observe(token, 1.0)\n",
        "    tokens_now = prompt.split()[-64:]\n",
        "    augmented = augment_with_memory(prompt, tokens_now)\n",
        "    return generate_chat_response(augmented, max_new_tokens=max_new_tokens)\n",
        "\n",
        "queries = [\n",
        "    \"\"\"In one sentence, summarise the following log:\n",
        "Alice met Bob at 9:00. They discussed primes 2, 3, 5, 7 and Möbius transforms.\"\"\",\n",
        "    \"Recall the meeting time and the smallest prime they discussed. Only output in this exact format: TIME=<time>; PRIME=<n>.\",\n",
        "]\n",
        "\n",
        "dual_results = []\n",
        "for q in queries:\n",
        "    start = time.time()\n",
        "    response = dual_substrate_generate(q, max_new_tokens=64)\n",
        "    latency = time.time() - start\n",
        "    dual_results.append({\"prompt\": q, \"response\": response, \"latency_s\": round(latency, 3)})\n",
        "\n",
        "with open(\"/content/dual_substrate_smoke.json\", \"w\") as f:\n",
        "    json.dump(dual_results, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", \"/content/dual_substrate_smoke.json\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cabaa043",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ded34a-b95d-4c70-da81-0e23440c0da8"
      },
      "source": [
        "import importlib.util, p_adic_memory as pam\n",
        "\n",
        "print(\"p_adic_memory importable?\", importlib.util.find_spec(\"p_adic_memory\") is not None)\n",
        "print(\"version:\", getattr(pam, \"__version__\", \"dev\"))\n",
        "print(\"has DualSubstrate:\", hasattr(pam, \"DualSubstrate\"))\n",
        "print(\"has DualSubstrateMemory:\", hasattr(pam, \"DualSubstrateMemory\"))\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p_adic_memory importable? True\n",
            "version: 0.1.0\n",
            "has DualSubstrate: True\n",
            "has DualSubstrateMemory: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqIpzAZzn0Jl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc48239-5dd0-46ce-c260-69511ce4a9c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/baseline_smoke.json\n"
          ]
        }
      ],
      "source": [
        "# Baseline comparison without memory augmentation\n",
        "baseline = []\n",
        "for q in queries:\n",
        "    start = time.time()\n",
        "    response = generate_chat_response(q, max_new_tokens=64)\n",
        "    latency = time.time() - start\n",
        "    baseline.append({\"prompt\": q, \"response\": response, \"latency_s\": round(latency, 3)})\n",
        "\n",
        "with open(\"/content/baseline_smoke.json\", \"w\") as f:\n",
        "    json.dump(baseline, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", \"/content/baseline_smoke.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8SFApB7QF2O",
        "outputId": "d56f47a0-710c-439d-ea84-d255b515bda2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline: {'n': 1, 'acc_time': 0.0, 'acc_prime': 0.0, 'median_latency': 2.676}\n",
            "Dual-substrate: {'n': 1, 'acc_time': 0.0, 'acc_prime': 0.0, 'median_latency': 2.211}\n"
          ]
        }
      ],
      "source": [
        "# Quick accuracy/latency scorer for the recall task\n",
        "import json, re\n",
        "\n",
        "def load(path):\n",
        "    with open(path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def parse_out(text: str):\n",
        "    match = re.search(r\"TIME\\s*=\\s*([0-9]{1,2}:[0-9]{2})\\s*;\\s*PRIME\\s*=\\s*([0-9]+)\", text)\n",
        "    if not match:\n",
        "        return None, None\n",
        "    return match.group(1), int(match.group(2))\n",
        "\n",
        "def score(records):\n",
        "    gt_time, gt_prime = \"9:00\", 2\n",
        "    data = []\n",
        "    for r in records:\n",
        "        if \"Recall the meeting time\" not in r[\"prompt\"]:\n",
        "            continue\n",
        "        time_out, prime_out = parse_out(r[\"response\"])\n",
        "        ok_time = time_out == gt_time\n",
        "        ok_prime = prime_out == gt_prime\n",
        "        data.append({\"latency_s\": r.get(\"latency_s\"), \"ok_time\": ok_time, \"ok_prime\": ok_prime})\n",
        "    if not data:\n",
        "        return {\"n\": 0}\n",
        "    acc_time = sum(d[\"ok_time\"] for d in data) / len(data)\n",
        "    acc_prime = sum(d[\"ok_prime\"] for d in data) / len(data)\n",
        "    latencies = [d[\"latency_s\"] for d in data if d[\"latency_s\"] is not None]\n",
        "    median_latency = sorted(latencies)[len(latencies) // 2] if latencies else None\n",
        "    return {\"n\": len(data), \"acc_time\": acc_time, \"acc_prime\": acc_prime, \"median_latency\": median_latency}\n",
        "\n",
        "baseline = load(\"/content/baseline_smoke.json\")\n",
        "dual = load(\"/content/dual_substrate_smoke.json\")\n",
        "print(\"Baseline:\", score(baseline))\n",
        "print(\"Dual-substrate:\", score(dual))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO3Np7w1n0Jl"
      },
      "execution_count": 12,
      "outputs": [],
      "source": [
        "# Optionally copy smoke-test outputs to Drive for persistence\n",
        "!cp /content/*smoke.json /content/drive/MyDrive/ 2>/dev/null || true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSWa2YiBn0Jl"
      },
      "source": [
        "## 3. LongBench-style harness (Option 2)\n",
        "\n",
        "LongBench does not ship a Python package or an `Evaluator` class. Instead of importing non-existent APIs, run a tiny harness\n",
        "that mimics their prompt/response logging. The following cells instantiate the dual-substrate generator, execute a small\n",
        "set of prompts, and write JSON artefacts for A/B comparisons.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQQMe_y8n0Jm"
      },
      "execution_count": 13,
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > /content/dual_substrate_adapter.py <<'PY'\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add the package's source directory to sys.path\n",
        "src_path = \"/content/p-adic-memory/src\"\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from p_adic_memory import DualSubstrateMemory\n",
        "\n",
        "\n",
        "class DualSubstrateGenerator:\n",
        "    def __init__(self, model_name: str, hf_token: str | None = None, mem_dim: int = 128):\n",
        "        qconf = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "        self.tok = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            quantization_config=qconf,\n",
        "        )\n",
        "        self.mem = DualSubstrateMemory(dim=mem_dim)\n",
        "\n",
        "    def stream(self, text: str):\n",
        "        for token in text.split():\n",
        "            yield token\n",
        "\n",
        "    def augment(self, prompt: str, tokens_now):\n",
        "        recalls = []\n",
        "        for token in tokens_now:\n",
        "            score, ledger_flag = self.mem.query(token)\n",
        "            recalls.append(f\"<mem exact={int(ledger_flag)} p={score:.3f}>\")\n",
        "        tag = \" \".join(recalls[:64])\n",
        "        return f\"{prompt}\\n\\n<memory>{tag}</memory>\"\n",
        "\n",
        "    def generate(self, prompt: str, max_new_tokens: int = 256, temperature: float = 0.2): # Removed do_sample from parameters\n",
        "        for token in self.stream(prompt):\n",
        "            self.mem.observe(token, 1.0)\n",
        "        current_tokens = prompt.split()[-64:]\n",
        "        augmented = self.augment(prompt, current_tokens)\n",
        "        inputs = self.tok(augmented, return_tensors=\"pt\").to(self.model.device)\n",
        "        with torch.inference_mode():\n",
        "            output = self.model.generate(\n",
        "                **inputs,\n",
        "                temperature=temperature,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                pad_token_id=self.tok.eos_token_id,\n",
        "            )\n",
        "        return self.tok.decode(output[0], skip_special_tokens=True)\n",
        "PY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmpEknogn0Jm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b537f0b-8528-46c5-bafa-34ec32785bd9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved JSONs under /content/: longbench_dual_substrate.json & longbench_baseline.json\n"
          ]
        }
      ],
      "source": [
        "import json, os, time, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Ensure the path to the installed package is in PYTHONPATH\n",
        "if \"/content/p-adic-memory/src\" not in os.environ.get(\"PYTHONPATH\", \"\"):\n",
        "    os.environ[\"PYTHONPATH\"] = f\"/content/p-adic-memory/src:{os.environ.get('PYTHONPATH', '')}\"\n",
        "\n",
        "from dual_substrate_adapter import DualSubstrateGenerator\n",
        "\n",
        "MODEL_NAME = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
        "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
        "\n",
        "dual = DualSubstrateGenerator(MODEL_NAME, hf_token=HF_TOKEN, mem_dim=128)\n",
        "\n",
        "qconf = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "baseline_tok = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
        "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=qconf,\n",
        ")\n",
        "\n",
        "def vanilla_generate(prompt: str, max_new_tokens: int = 128):\n",
        "    inputs = baseline_tok(prompt, return_tensors=\"pt\").to(baseline_model.device)\n",
        "    with torch.inference_mode():\n",
        "        out = baseline_model.generate(\n",
        "            **inputs,\n",
        "            do_sample=False,\n",
        "            temperature=0.0,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=baseline_tok.eos_token_id,\n",
        "            eos_token_id=baseline_tok.eos_token_id,\n",
        "        )\n",
        "    return baseline_tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "samples = [\n",
        "    \"In one sentence, summarise: Alice met Bob at 9:00. They discussed primes 2,3,5,7 and Möbius transforms.\",\n",
        "    \"Only output: TIME=<time>; PRIME=<n>. What time and smallest prime from the log above?\",\n",
        "]\n",
        "\n",
        "def run_eval(gen_fn):\n",
        "    outputs = []\n",
        "    for prompt in samples:\n",
        "        start = time.time()\n",
        "        response = gen_fn(prompt)\n",
        "        latency = round(time.time() - start, 3)\n",
        "        outputs.append({\"prompt\": prompt, \"response\": response, \"latency_s\": latency})\n",
        "    return outputs\n",
        "\n",
        "dual_records = run_eval(lambda p: dual.generate(p, max_new_tokens=128, temperature=0.0))\n",
        "vanilla_records = run_eval(lambda p: vanilla_generate(p, max_new_tokens=128))\n",
        "\n",
        "with open(\"/content/longbench_dual_substrate.json\", \"w\") as f:\n",
        "    json.dump(dual_records, f, indent=2)\n",
        "with open(\"/content/longbench_baseline.json\", \"w\") as f:\n",
        "    json.dump(vanilla_records, f, indent=2)\n",
        "\n",
        "print(\"Saved JSONs under /content/: longbench_dual_substrate.json & longbench_baseline.json\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2KdWBPrn0Jm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f61db10d-41e3-431d-d439-cec456c9910b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "longbench_dual_substrate.json (records=2):\n",
            "- prompt[:48]='In one sentence, summarise: Alice met Bob at 9:0' | latency=0.289\n",
            "- prompt[:48]='Only output: TIME=<time>; PRIME=<n>. What time a' | latency=7.071\n",
            "\n",
            "longbench_baseline.json (records=2):\n",
            "- prompt[:48]='In one sentence, summarise: Alice met Bob at 9:0' | latency=5.778\n",
            "- prompt[:48]='Only output: TIME=<time>; PRIME=<n>. What time a' | latency=0.114\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "for name in [\"longbench_dual_substrate.json\", \"longbench_baseline.json\"]:\n",
        "    path = Path(\"/content\") / name\n",
        "    if not path.exists():\n",
        "        print(f\"Missing {name}; run the harness cell above first.\")\n",
        "        continue\n",
        "    with path.open() as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"\\n{name} (records={len(data)}):\")\n",
        "    for item in data:\n",
        "        snippet = item[\"prompt\"][:48].replace(\"\\n\", \" \")\n",
        "        print(\"- prompt[:48]={!r} | latency={}\".format(snippet, item.get(\"latency_s\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddxESaHPn0Jm"
      },
      "source": [
        "## 4. RULER evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXFDO-B-n0Jn"
      },
      "execution_count": 17,
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > /content/ruler_adapter.py <<'PY'\n",
        "import os\n",
        "from dual_substrate_adapter import DualSubstrateGenerator\n",
        "\n",
        "_model = None\n",
        "\n",
        "def load_model():\n",
        "    global _model\n",
        "    if _model is None:\n",
        "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "        _model = DualSubstrateGenerator(name, hf_token=os.environ.get(\"HF_TOKEN\"))\n",
        "    return _model\n",
        "\n",
        "def generate(prompt: str) -> str:\n",
        "    model = load_model()\n",
        "    return model.generate(prompt, max_new_tokens=256, temperature=0.2)\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56t_anUYn0Jn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "633e1f57-b649-492b-e01c-033c246dd52f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: /usr/bin/python3 -m ruler.evaluate --model custom --custom_module ruler_adapter --tasks kv_retrieval --context_lengths 4k,8k --num_samples 50\n",
            "\n",
            "/usr/bin/python3: Error while finding module specification for 'ruler.evaluate' (ModuleNotFoundError: No module named 'ruler')\n",
            "\n",
            "Saved: /content/ruler_dual_substrate.txt\n"
          ]
        }
      ],
      "source": [
        "import os, subprocess, sys\n",
        "\n",
        "os.environ[\"PYTHONPATH\"] = f\"/content:{os.environ.get('PYTHONPATH', '')}\"\n",
        "os.environ[\"RULER_MODEL\"] = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    '-m',\n",
        "    'ruler.evaluate',\n",
        "    '--model',\n",
        "    'custom',\n",
        "    '--custom_module',\n",
        "    'ruler_adapter',\n",
        "    '--tasks',\n",
        "    'kv_retrieval',\n",
        "    '--context_lengths',\n",
        "    '4k,8k',\n",
        "    '--num_samples',\n",
        "    '50',\n",
        "]\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print(completed.stdout)\n",
        "print(completed.stderr)\n",
        "\n",
        "with open('/content/ruler_dual_substrate.txt', 'w') as f:\n",
        "    f.write(completed.stdout)\n",
        "\n",
        "print('Saved:', '/content/ruler_dual_substrate.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK0uV5yYn0Jn"
      },
      "execution_count": 19,
      "outputs": [],
      "source": [
        "# Optional vanilla RULER baseline using transformers only\n",
        "%%bash\n",
        "cat > /content/ruler_vanilla_adapter.py <<'PY'\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "_model = None\n",
        "_tok = None\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    global _model, _tok\n",
        "    if _model is None or _tok is None:\n",
        "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "        qconf = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "        _tok = AutoTokenizer.from_pretrained(name, token=os.environ.get(\"HF_TOKEN\"))\n",
        "        _model = AutoModelForCausalLM.from_pretrained(\n",
        "            name,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            quantization_config=qconf,\n",
        "        )\n",
        "    return _tok, _model\n",
        "\n",
        "\n",
        "def generate(prompt: str) -> str:\n",
        "    tok, model = load_model()\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            pad_token_id=tok.eos_token_id,\n",
        "        )\n",
        "    return tok.decode(output[0], skip_special_tokens=True)\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCHxB-Ron0Jn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ddc1ed0-5811-4c0b-d47d-26b6ac31d0e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: /usr/bin/python3 -m ruler.evaluate --model custom --custom_module ruler_vanilla_adapter --tasks kv_retrieval --context_lengths 4k,8k --num_samples 50\n",
            "\n",
            "/usr/bin/python3: Error while finding module specification for 'ruler.evaluate' (ModuleNotFoundError: No module named 'ruler')\n",
            "\n",
            "Saved: /content/ruler_baseline.txt\n"
          ]
        }
      ],
      "source": [
        "import subprocess, sys\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    '-m',\n",
        "    'ruler.evaluate',\n",
        "    '--model',\n",
        "    'custom',\n",
        "    '--custom_module',\n",
        "    'ruler_vanilla_adapter',\n",
        "    '--tasks',\n",
        "    'kv_retrieval',\n",
        "    '--context_lengths',\n",
        "    '4k,8k',\n",
        "    '--num_samples',\n",
        "    '50',\n",
        "]\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print(completed.stdout)\n",
        "print(completed.stderr)\n",
        "\n",
        "with open('/content/ruler_baseline.txt', 'w') as f:\n",
        "    f.write(completed.stdout)\n",
        "\n",
        "print('Saved:', '/content/ruler_baseline.txt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7-WvjIPn0Jn"
      },
      "source": [
        "## 5. Export and persist results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0Dqr0agn0Jo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c8a00e-4937-402e-f5c1-e2384223d33a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root  609 Oct 14 13:57 /content/longbench_baseline.json\n",
            "-rw-r--r-- 1 root root 1.6K Oct 14 13:57 /content/longbench_dual_substrate.json\n",
            "-rw-r--r-- 1 root root  458 Oct 14 13:58 /content/ruler_adapter.py\n",
            "-rw-r--r-- 1 root root    0 Oct 14 13:58 /content/ruler_baseline.txt\n",
            "-rw-r--r-- 1 root root    0 Oct 14 13:58 /content/ruler_dual_substrate.txt\n",
            "-rw-r--r-- 1 root root 1.2K Oct 14 13:58 /content/ruler_vanilla_adapter.py\n"
          ]
        }
      ],
      "source": [
        "!ls -lh /content/*longbench*.json /content/*ruler* 2>/dev/null || true\n",
        "!cp /content/longbench_*.json /content/ruler_* /content/drive/MyDrive/ 2>/dev/null || true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kor6GcPnn0Jo"
      },
      "source": [
        "## 6. Scaling plan\n",
        "\n",
        "1. Swap `MODEL_NAME` to **mistralai/Mistral-7B-Instruct-v0.2** with 4-bit quantisation.\n",
        "2. Increase LongBench `sample_size` (e.g., 25 → 100) and add tasks such as `LongBookSummEng` and additional QA tracks.\n",
        "3. Extend RULER coverage to multi-hop and longer contexts once the pipeline is reliable.\n",
        "4. Introduce vLLM for batching after verifying correctness with Transformers.\n",
        "5. Maintain A/B JSON outputs (`baseline` vs `dual_substrate`) and track latency, VRAM, and accuracy deltas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qxrWXzqn0Jo"
      },
      "source": [
        "## 7. Troubleshooting tips\n",
        "\n",
        "* **CUDA out-of-memory**: lower `max_new_tokens`, revert to the TinyLlama checkpoint, or ensure 4-bit loading is active.\n",
        "* **Tokenizer errors**: set `pad_token_id` to `tok.eos_token_id`.\n",
        "* **Authentication failures**: provide a Hugging Face token and request model access if required.\n",
        "* **Dataset download issues**: run the dataset setup cells once with a stable internet connection.\n",
        "* **Custom module not found**: confirm that `/content` is on `PYTHONPATH` before invoking RULER.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JH4C4fXn0Jo"
      },
      "source": [
        "## 8. Publishing checklist\n",
        "\n",
        "* Commit `dual_substrate_adapter.py`, `ruler_adapter.py`, and this notebook to a dedicated branch (e.g., `colab-benchmark/`).\n",
        "* Archive JSON artefacts (`longbench_*.json`, `ruler_*.txt`) for baseline comparisons.\n",
        "* Summarise the metrics in a short report covering recall, drift, latency, and energy usage.\n"
      ]
    }
  ]
}