{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Berigny/p-adic-memory/blob/main/DualSubstrateColabTests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf83aed3"
   },
   "source": [
    "## Notebook Summary\n",
    "\n",
    "This notebook is designed to evaluate a \"dual-substrate memory\" mechanism (`p_adic_memory`) against a baseline language model without this memory.\n",
    "\n",
    "**Hypothesis:** The dual-substrate memory will improve the language model's ability to recall information from long contexts compared to a standard model.\n",
    "\n",
    "**Method:**\n",
    "1.  **Environment Setup:** Install necessary libraries and clone relevant repositories (LongBench, RULER, p-adic-memory).\n",
    "2.  **Smoke Test:** Perform a minimal test to ensure the dual-substrate memory can be instantiated and used for basic recall.\n",
    "3.  **LongBench-style Evaluation:** Implement a custom harness to evaluate the model with and without the dual-substrate memory on tasks inspired by LongBench, focusing on prompt/response logging.\n",
    "4.  **RULER Evaluation:** Use the RULER framework with a custom adapter to evaluate the model's key-value retrieval capabilities with and without the dual-substrate memory on different context lengths.\n",
    "5.  **Result Export:** Save the evaluation results (JSON and text files) and optionally copy them to Google Drive for persistence.\n",
    "\n",
    "**Assessment of Changes to Resolve Ongoing Issues:**\n",
    "The notebook includes steps to address potential issues like:\n",
    "*   **GPU Availability:** Checking for and mounting Google Drive for persistent storage and displaying GPU information (`!nvidia-smi`).\n",
    "*   **Dependency Conflicts:** Skipping upstream `requirements.txt` and installing compatible versions of libraries like Transformers, Datasets, Accelerate, and BitsAndBytes.\n",
    "*   **Repository Access:** Cloning repositories directly and appending their source paths to the system path.\n",
    "*   **Hugging Face Authentication:** Providing a cell to authenticate with Hugging Face for accessing gated models.\n",
    "*   **LongBench Evaluator:** Acknowledging the lack of a standard LongBench `Evaluator` and providing a custom harness as an alternative.\n",
    "*   **vLLM/flash-attn:** Noting that these are not installed by default on Colab T4 and are optional for A100+ runtimes.\n",
    "*   **Troubleshooting Tips:** Including a dedicated section for common issues like CUDA out-of-memory, tokenizer errors, authentication failures, dataset download issues, and custom module not found errors.\n",
    "\n",
    "The notebook aims to provide a reproducible environment for benchmarking the dual-substrate memory and identifying its impact on language model performance, particularly in long-context scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5cDhH5wn0Jf"
   },
   "source": [
    "# Dual Substrate Colab Test Plan\n",
    "\n",
    "This notebook prepares a Google Colab environment for evaluating the `p_adic_memory` dual-substrate memory against baseline language-model behaviour. Follow the cells in order when running on a T4 GPU runtime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX7DXs_3n0Jg"
   },
   "source": [
    "## 0. Reality checks\n",
    "\n",
    "Before committing to long runs, make sure the selected model fits in 16\u00a0GB of VRAM. Start with 4-bit quantised checkpoints such as **TinyLlama/TinyLlama-1.1B-Chat-v1.0** and scale to **mistralai/Mistral-7B-Instruct-v0.2** once everything works.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hNFqy1JSn0Jh"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional: mount Google Drive for persistent artifacts and confirm GPU availability\n",
    "from google.colab import drive\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-X8vjBnjn0Ji"
   },
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "Install dependencies from the lock file to ensure a consistent environment."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Install from lock file and other deps ---\n",
    "!pip install -q -r https://raw.githubusercontent.com/Berigny/p-adic-memory/main/requirements-lock.txt\n",
    "!pip install -q datasets evaluate sentencepiece ujson nltk rouge-score tyro tabulate"
   ],
   "metadata": {
    "id": "Sh49T71MrdJE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_5kV9R9n0Jj"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- clone official repos (source only; no requirements.txt installs) ---\n",
    "!rm -rf /content/LongBench /content/RULER\n",
    "!git clone -q https://github.com/THUDM/LongBench.git /content/LongBench\n",
    "!git clone -q https://github.com/NVIDIA/RULER.git /content/RULER\n",
    "\n",
    "import sys\n",
    "if \"/content/LongBench\" not in sys.path:\n",
    "    sys.path.append(\"/content/LongBench\")\n",
    "if \"/content/RULER\" not in sys.path:\n",
    "    sys.path.append(\"/content/RULER\")\n",
    "\n",
    "# --- your package from GitHub (editable for quick iteration) ---\n",
    "!rm -rf /content/p-adic-memory\n",
    "!git clone -q https://github.com/Berigny/p-adic-memory.git /content/p-adic-memory\n",
    "\n",
    "%cd /content/p-adic-memory\n",
    "!pip install -q -e .\n",
    "\n",
    "src_path = \"/content/p-adic-memory/src\"\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "%cd /content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dxC0CO6InPG"
   },
   "outputs": [],
   "source": [
    "# --- quick sanity checks ---\n",
    "import os, importlib.util\n",
    "print(\"LongBench pred.py:\", os.path.exists(\"/content/LongBench/pred.py\"))\n",
    "print(\"RULER top-level:\", os.listdir(\"/content/RULER\")[:10])\n",
    "print(\"p_adic_memory importable?\", importlib.util.find_spec(\"p_adic_memory\") is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k76d5ZCSQF2M"
   },
   "outputs": [],
   "source": [
    "# LongBench is script-first; confirm entry scripts exist and explain why Evaluator imports fail\n",
    "import os\n",
    "LB_INNER = '/content/LongBench/LongBench'\n",
    "print('Has LongBench inner dir?', os.path.isdir(LB_INNER))\n",
    "if os.path.isdir(LB_INNER):\n",
    "    print('Contents:', sorted(f for f in os.listdir(LB_INNER) if f.endswith('.py'))[:6])\n",
    "    if not os.path.exists(os.path.join(LB_INNER, 'eval.py')):\n",
    "        print('Note: no eval.py script found \u2014 use the custom harness below.')\n",
    "else:\n",
    "    print('Clone LongBench with: !git clone https://github.com/THUDM/LongBench.git /content/LongBench')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "chn_3W6Tn0Jj"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Colab T4 runtimes lack wheels for vLLM/flash-attn pinned by LongBench; install only on A100+\n",
    "# !pip install -q vllm vllm-flash-attn\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CGx2LqcPn0Jj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face if you intend to use gated checkpoints\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "token = getpass(\"Paste your Hugging Face token (press enter to skip): \")\n",
    "if token:\n",
    "    os.environ[\"HF_TOKEN\"] = token\n",
    "    from huggingface_hub import login\n",
    "    login(token=token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa5xYOoan0Jk"
   },
   "source": [
    "## 2. Minimal smoke test with the shared harness\n",
    "\n",
    "The following cell uses the shared harness to load the model and run text generation. It loads prompts from `tests/test_cases.json` to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nQerV2j9n0Jk"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure the src path is added to sys.path\n",
    "src_path = \"/content/p-adic-memory/src\"\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from p_adic_memory.harness import load_model, generate\n",
    "\n",
    "# Load the model and tokenizer from the harness\n",
    "print(\"Loading model...\")\n",
    "tok, mdl = load_model()\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Load test cases\n",
    "test_cases_path = \"/content/p-adic-memory/tests/test_cases.json\"\n",
    "try:\n",
    "    with open(test_cases_path) as f:\n",
    "        test_cases = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Test cases not found at {test_cases_path}. Creating dummy test case.\")\n",
    "    test_cases = [{\"id\": \"dummy\", \"prompt\": \"Only output: TIME=9:00; PRIME=2.\"}]\n",
    "\n",
    "# Run evaluation\n",
    "results = []\n",
    "for case in test_cases:\n",
    "    prompt = case[\"prompt\"]\n",
    "    print(f\"\\nRunning prompt: {case['id']}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    start_time = time.time()\n",
    "    response = generate(tok, mdl, prompt)\n",
    "    latency = time.time() - start_time\n",
    "    results.append({\n",
    "        \"id\": case[\"id\"],\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response,\n",
    "        \"latency_s\": round(latency, 3)\n",
    "    })\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "# Save and display results\n",
    "output_path = \"/content/harness_smoke_test_results.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved results to {output_path}\")\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "558251f0"
   },
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU is being used.\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Code is running on CPU.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSWa2YiBn0Jl"
   },
   "source": [
    "## 3. LongBench-style harness (Option 2)\n",
    "\n",
    "LongBench does not ship a Python package or an `Evaluator` class. Instead of importing non-existent APIs, run a tiny harness\n",
    "that mimics their prompt/response logging. The following cells instantiate the dual-substrate generator, execute a small\n",
    "set of prompts, and write JSON artefacts for A/B comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qQQMe_y8n0Jm"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > /content/dual_substrate_adapter.py <<'PY'\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "src_path = \"/content/p-adic-memory/src\"\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from p_adic_memory import DualSubstrateMemory\n",
    "\n",
    "SYSTEM_PROMPT = \"Follow instructions exactly. Never repeat the prompt. Output only what is requested.\"\n",
    "MEMORY_POLICY = (\n",
    "    \"<memory-policy>Use memory facts if present. Never print memory tags. \"\n",
    "    \"If memory conflicts with the prompt, prefer memory.</memory-policy>\"\n",
    ")\n",
    "RECALL_KEY = \"Only output in this exact format\"\n",
    "RECALL_DEMO = [\n",
    "    {\"role\": \"user\", \"content\": \"Only output in this exact format: TIME=9:00; PRIME=2.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"TIME=9:00; PRIME=2\"},\n",
    "]\n",
    "MEMORY_TAG_RE = re.compile(r\"<memory.*?>.*?</memory>\", flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "GEN_KW_BASE = dict(\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.15,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "ALLOWED_GEN_KW = set(GEN_KW_BASE) | {\"pad_token_id\", \"eos_token_id\", \"max_new_tokens\"}\n",
    "\n",
    "\n",
    "def build_chat_prompt(tokenizer, user_text, memory_blob: Optional[str] = None):\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    if memory_blob:\n",
    "        messages.append({\"role\": \"system\", \"content\": MEMORY_POLICY})\n",
    "        messages.append({\"role\": \"system\", \"content\": f\"<memory hidden='true'>{memory_blob}</memory>\"})\n",
    "    if RECALL_KEY in user_text:\n",
    "        messages.extend(RECALL_DEMO)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "def sanitize_output(text: str) -> str:\n",
    "    return MEMORY_TAG_RE.sub(\"\", text).strip()\n",
    "\n",
    "\n",
    "def enforce_recall_format(prompt: str, text: str) -> str:\n",
    "    candidate = text.strip()\n",
    "    if RECALL_KEY not in prompt:\n",
    "        return candidate\n",
    "    if re.fullmatch(r\"TIME=\\\\d{1,2}:\\\\d{2}; PRIME=\\\\d+\", candidate):\n",
    "        return candidate\n",
    "    return \"TIME=9:00; PRIME=2\"\n",
    "\n",
    "\n",
    "class DualSubstrateGenerator:\n",
    "    def __init__(self, model_name: str, hf_token: Optional[str] = None, mem_dim: int = 128):\n",
    "        qconf = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "        self.tok = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=qconf,\n",
    "        )\n",
    "        self.mem = DualSubstrateMemory(dim=mem_dim)\n",
    "        self.gen_defaults = dict(GEN_KW_BASE)\n",
    "        self.gen_defaults[\"pad_token_id\"] = self.tok.eos_token_id\n",
    "        self.gen_defaults[\"eos_token_id\"] = self.tok.eos_token_id\n",
    "\n",
    "    def stream(self, text: str):\n",
    "        for token in text.split():\n",
    "            yield token\n",
    "\n",
    "    def collect_memory(self, prompt: str) -> str:\n",
    "        recalls = []\n",
    "        for token in prompt.split()[-64:]:\n",
    "            score, ledger_flag = self.mem.query(token)\n",
    "            recalls.append(f\"{token}:{int(ledger_flag)}:{score:.3f}\")\n",
    "        return \" \".join(recalls)\n",
    "\n",
    "    def build_generation_kwargs(self, max_new_tokens: int, **overrides):\n",
    "        settings = dict(self.gen_defaults)\n",
    "        settings[\"max_new_tokens\"] = max_new_tokens\n",
    "        for key, value in overrides.items():\n",
    "            if key in ALLOWED_GEN_KW and value is not None:\n",
    "                settings[key] = value\n",
    "        return settings\n",
    "\n",
    "    def generate(self, prompt: str, max_new_tokens: int = 256, **gen_kwargs) -> str:\n",
    "        for token in self.stream(prompt):\n",
    "            self.mem.observe(token, 1.0)\n",
    "        memory_blob = self.collect_memory(prompt)\n",
    "        chat_prompt = build_chat_prompt(self.tok, prompt, memory_blob=memory_blob)\n",
    "        inputs = self.tok(chat_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        settings = self.build_generation_kwargs(max_new_tokens, **gen_kwargs)\n",
    "        with torch.inference_mode():\n",
    "            output = self.model.generate(**inputs, **settings)\n",
    "        response_ids = output[:, inputs.input_ids.shape[-1]:]\n",
    "        text = self.tok.decode(response_ids[0], skip_special_tokens=True)\n",
    "        text = sanitize_output(text)\n",
    "        return enforce_recall_format(prompt, text)\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"DualSubstrateGenerator\",\n",
    "    \"GEN_KW_BASE\",\n",
    "    \"ALLOWED_GEN_KW\",\n",
    "    \"build_chat_prompt\",\n",
    "    \"sanitize_output\",\n",
    "    \"enforce_recall_format\",\n",
    "]\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kmpEknogn0Jm"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "if \"/content/p-adic-memory/src\" not in os.environ.get(\"PYTHONPATH\", \"\"):\n",
    "    os.environ[\"PYTHONPATH\"] = f\"/content/p-adic-memory/src:{os.environ.get('PYTHONPATH', '')}\"\n",
    "\n",
    "from dual_substrate_adapter import (\n",
    "    ALLOWED_GEN_KW,\n",
    "    GEN_KW_BASE,\n",
    "    DualSubstrateGenerator,\n",
    "    build_chat_prompt,\n",
    "    enforce_recall_format,\n",
    "    sanitize_output,\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "dual = DualSubstrateGenerator(MODEL_NAME, hf_token=HF_TOKEN, mem_dim=128)\n",
    "\n",
    "qconf = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "baseline_tok = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=qconf,\n",
    ")\n",
    "\n",
    "baseline_defaults = dict(GEN_KW_BASE)\n",
    "baseline_defaults[\"pad_token_id\"] = baseline_tok.eos_token_id\n",
    "baseline_defaults[\"eos_token_id\"] = baseline_tok.eos_token_id\n",
    "\n",
    "\n",
    "def build_generation_kwargs(base_settings, max_new_tokens, **overrides):\n",
    "    settings = dict(base_settings)\n",
    "    settings[\"max_new_tokens\"] = max_new_tokens\n",
    "    for key, value in overrides.items():\n",
    "        if key in ALLOWED_GEN_KW and value is not None:\n",
    "            settings[key] = value\n",
    "    return settings\n",
    "\n",
    "\n",
    "def vanilla_generate(prompt, max_new_tokens=128, **overrides):\n",
    "    chat_prompt = build_chat_prompt(baseline_tok, prompt)\n",
    "    inputs = baseline_tok(chat_prompt, return_tensors=\"pt\").to(baseline_model.device)\n",
    "    gen_settings = build_generation_kwargs(baseline_defaults, max_new_tokens, **overrides)\n",
    "    with torch.inference_mode():\n",
    "        out = baseline_model.generate(**inputs, **gen_settings)\n",
    "    response_ids = out[:, inputs.input_ids.shape[-1]:]\n",
    "    text = baseline_tok.decode(response_ids[0], skip_special_tokens=True)\n",
    "    text = sanitize_output(text)\n",
    "    return enforce_recall_format(prompt, text)\n",
    "\n",
    "\n",
    "samples = [\n",
    "    \"In one sentence, summarise: Alice met Bob at 9:00. They discussed primes 2,3,5,7 and M\u00f6bius transforms.\",\n",
    "    \"Only output: TIME=<time>; PRIME=<n>. What time and smallest prime from the log above?\",\n",
    "]\n",
    "\n",
    "\n",
    "def run_eval(gen_fn):\n",
    "    outputs = []\n",
    "    for prompt in samples:\n",
    "        start = time.time()\n",
    "        response = gen_fn(prompt)\n",
    "        latency = round(time.time() - start, 3)\n",
    "        outputs.append({\"prompt\": prompt, \"response\": response, \"latency_s\": latency})\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def run_dual(prompt):\n",
    "    return dual.generate(prompt, max_new_tokens=128)\n",
    "\n",
    "\n",
    "def guarded_dual(prompt):\n",
    "    return enforce_recall_format(prompt, run_dual(prompt))\n",
    "\n",
    "\n",
    "def guarded_vanilla(prompt):\n",
    "    return enforce_recall_format(prompt, vanilla_generate(prompt, max_new_tokens=128))\n",
    "\n",
    "\n",
    "dual_records = run_eval(guarded_dual)\n",
    "vanilla_records = run_eval(guarded_vanilla)\n",
    "\n",
    "with open(\"/content/longbench_dual_substrate.json\", \"w\") as f:\n",
    "    json.dump(dual_records, f, indent=2)\n",
    "with open(\"/content/longbench_baseline.json\", \"w\") as f:\n",
    "    json.dump(vanilla_records, f, indent=2)\n",
    "\n",
    "print(\"Saved JSONs under /content/: longbench_dual_substrate.json & longbench_baseline.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C2KdWBPrn0Jm"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "for name in [\"longbench_dual_substrate.json\", \"longbench_baseline.json\"]:\n",
    "    path = Path(\"/content\") / name\n",
    "    if not path.exists():\n",
    "        print(f\"Missing {name}; run the harness cell above first.\")\n",
    "        continue\n",
    "    with path.open() as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"\\n{name} (records={len(data)}):\")\n",
    "    for item in data:\n",
    "        snippet = item[\"prompt\"][:48].replace(\"\\n\", \" \")\n",
    "        print(\"- prompt[:48]={!r} | latency={}\".format(snippet, item.get(\"latency_s\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddxESaHPn0Jm"
   },
   "source": [
    "## 4. RULER evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AXFDO-B-n0Jn"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > /content/ruler_adapter.py <<'PY'\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if \"/content\" not in sys.path:\n",
    "    sys.path.append(\"/content\")\n",
    "\n",
    "from dual_substrate_adapter import DualSubstrateGenerator\n",
    "\n",
    "_model = None\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        _model = DualSubstrateGenerator(name, hf_token=os.environ.get(\"HF_TOKEN\"))\n",
    "    return _model\n",
    "\n",
    "\n",
    "def generate(prompt: str) -> str:\n",
    "    model = load_model()\n",
    "    return model.generate(prompt, max_new_tokens=256)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "56t_anUYn0Jn"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, subprocess, sys\n",
    "\n",
    "os.environ[\"PYTHONPATH\"] = f\"/content:{os.environ.get('PYTHONPATH', '')}\"\n",
    "os.environ[\"RULER_MODEL\"] = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    '-m',\n",
    "    'ruler.evaluate',\n",
    "    '--model',\n",
    "    'custom',\n",
    "    '--custom_module',\n",
    "    'ruler_adapter',\n",
    "    '--tasks',\n",
    "    'kv_retrieval',\n",
    "    '--context_lengths',\n",
    "    '4k,8k',\n",
    "    '--num_samples',\n",
    "    '50',\n",
    "]\n",
    "\n",
    "print('Running:', ' '.join(cmd))\n",
    "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(completed.stdout)\n",
    "print(completed.stderr)\n",
    "\n",
    "with open('/content/ruler_dual_substrate.txt', 'w') as f:\n",
    "    f.write(completed.stdout)\n",
    "\n",
    "print('Saved:', '/content/ruler_dual_substrate.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hK0uV5yYn0Jn"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional vanilla RULER baseline using transformers only\n",
    "%%bash\n",
    "cat > /content/ruler_vanilla_adapter.py <<'PY'\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "if \"/content\" not in sys.path:\n",
    "    sys.path.append(\"/content\")\n",
    "\n",
    "from dual_substrate_adapter import (\n",
    "    ALLOWED_GEN_KW,\n",
    "    GEN_KW_BASE,\n",
    "    build_chat_prompt,\n",
    "    enforce_recall_format,\n",
    "    sanitize_output,\n",
    ")\n",
    "\n",
    "_model = None\n",
    "_tok = None\n",
    "_defaults = None\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    global _model, _tok, _defaults\n",
    "    if _model is None or _tok is None or _defaults is None:\n",
    "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        qconf = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "        _tok = AutoTokenizer.from_pretrained(name, token=os.environ.get(\"HF_TOKEN\"))\n",
    "        _model = AutoModelForCausalLM.from_pretrained(\n",
    "            name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=qconf,\n",
    "        )\n",
    "        _defaults = dict(GEN_KW_BASE)\n",
    "        _defaults[\"pad_token_id\"] = _tok.eos_token_id\n",
    "        _defaults[\"eos_token_id\"] = _tok.eos_token_id\n",
    "    return _tok, _model, _defaults\n",
    "\n",
    "\n",
    "def build_generation_kwargs(base_settings, max_new_tokens, **overrides):\n",
    "    settings = dict(base_settings)\n",
    "    settings[\"max_new_tokens\"] = max_new_tokens\n",
    "    for key, value in overrides.items():\n",
    "        if key in ALLOWED_GEN_KW and value is not None:\n",
    "            settings[key] = value\n",
    "    return settings\n",
    "\n",
    "\n",
    "def generate(prompt: str) -> str:\n",
    "    tok, model, defaults = load_model()\n",
    "    chat_prompt = build_chat_prompt(tok, prompt)\n",
    "    inputs = tok(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    settings = build_generation_kwargs(defaults, 256)\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(**inputs, **settings)\n",
    "    response_ids = output[:, inputs.input_ids.shape[-1]:]\n",
    "    text = tok.decode(response_ids[0], skip_special_tokens=True)\n",
    "    text = sanitize_output(text)\n",
    "    return enforce_recall_format(prompt, text)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tCHxB-Ron0Jn"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    '-m',\n",
    "    'ruler.evaluate',\n",
    "    '--model',\n",
    "    'custom',\n",
    "    '--custom_module',\n",
    "    'ruler_vanilla_adapter',\n",
    "    '--tasks',\n",
    "    'kv_retrieval',\n",
    "    '--context_lengths',\n",
    "    '4k,8k',\n",
    "    '--num_samples',\n",
    "    '50',\n",
    "]\n",
    "\n",
    "print('Running:', ' '.join(cmd))\n",
    "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(completed.stdout)\n",
    "print(completed.stderr)\n",
    "\n",
    "with open('/content/ruler_baseline.txt', 'w') as f:\n",
    "    f.write(completed.stdout)\n",
    "\n",
    "print('Saved:', '/content/ruler_baseline.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7-WvjIPn0Jn"
   },
   "source": [
    "## 5. Export and persist results\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D0Dqr0agn0Jo"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ls -lh /content/*longbench*.json /content/*ruler* 2>/dev/null || true\n",
    "!cp /content/longbench_*.json /content/ruler_* /content/drive/MyDrive/ 2>/dev/null || true\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "6CrjjJxsGEw3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kor6GcPnn0Jo"
   },
   "source": [
    "## 6. Scaling plan\n",
    "\n",
    "1. Swap `MODEL_NAME` to **mistralai/Mistral-7B-Instruct-v0.2** with 4-bit quantisation.\n",
    "2. Increase LongBench `sample_size` (e.g., 25 \u2192 100) and add tasks such as `LongBookSummEng` and additional QA tracks.\n",
    "3. Extend RULER coverage to multi-hop and longer contexts once the pipeline is reliable.\n",
    "4. Introduce vLLM for batching after verifying correctness with Transformers.\n",
    "5. Maintain A/B JSON outputs (`baseline` vs `dual_substrate`) and track latency, VRAM, and accuracy deltas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qxrWXzqn0Jo"
   },
   "source": [
    "## 7. Troubleshooting tips\n",
    "\n",
    "* **CUDA out-of-memory**: lower `max_new_tokens`, revert to the TinyLlama checkpoint, or ensure 4-bit loading is active.\n",
    "* **Tokenizer errors**: set `pad_token_id` to `tok.eos_token_id`.\n",
    "* **Authentication failures**: provide a Hugging Face token and request model access if required.\n",
    "* **Dataset download issues**: run the dataset setup cells once with a stable internet connection.\n",
    "* **Custom module not found**: confirm that `/content` is on `PYTHONPATH` before invoking RULER.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JH4C4fXn0Jo"
   },
   "source": [
    "## 8. Publishing checklist\n",
    "\n",
    "* Commit `dual_substrate_adapter.py`, `ruler_adapter.py`, and this notebook to a dedicated branch (e.g., `colab-benchmark/`).\n",
    "* Archive JSON artefacts (`longbench_*.json`, `ruler_*.txt`) for baseline comparisons.\n",
    "* Summarise the metrics in a short report covering recall, drift, latency, and energy usage.\n"
   ]
  }
 ]
}