{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "87b0d73edfb0416b9aacf1c5cebac0ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HBoxModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_03fb29ad2b37449b8235435f157e334d",
        "IPY_MODEL_89e72e0d80af4865a9d67b34a909b7bd",
        "IPY_MODEL_a078507b217146ca8a9fe0dd9ba9a84b"
       ],
       "layout": "IPY_MODEL_d5b87a6b10bd4fd9b21459373e978e3f"
      }
     },
     "03fb29ad2b37449b8235435f157e334d": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_74ff1a35259544abbcb846081d72a1bf",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_932b26d67eeb4a9cb94c4481065406b4",
       "value": "tokenizer_config.json:\u2007"
      }
     },
     "89e72e0d80af4865a9d67b34a909b7bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "FloatProgressModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_831ce7458ba34f4c8e6737c5f576a424",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f540367c39944afea83e1bba07b5f554",
       "value": 1
      }
     },
     "a078507b217146ca8a9fe0dd9ba9a84b": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_96630a5a15d7488f96c3167a8096b42c",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_1acfb804368f4502b6cf22fd7f8d4d78",
       "value": "\u20071.29k/?\u2007[00:00&lt;00:00,\u200778.7kB/s]"
      }
     },
     "d5b87a6b10bd4fd9b21459373e978e3f": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "74ff1a35259544abbcb846081d72a1bf": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "932b26d67eeb4a9cb94c4481065406b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "831ce7458ba34f4c8e6737c5f576a424": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "f540367c39944afea83e1bba07b5f554": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "ProgressStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "96630a5a15d7488f96c3167a8096b42c": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1acfb804368f4502b6cf22fd7f8d4d78": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "690c1fe60bd94720bc5060426703b18e": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HBoxModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7ff6f298ebf245e599125f732e4de78e",
        "IPY_MODEL_09511152057f484fa737c2504d4973a0",
        "IPY_MODEL_2fdc21bf0367461bbbbe73a140dbb2dd"
       ],
       "layout": "IPY_MODEL_ad485bf47cf7440aa7d73d8b80b6a9bb"
      }
     },
     "7ff6f298ebf245e599125f732e4de78e": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1dacb3b920dc4dc7aa1f01c1211a9844",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_6dcbd6a6712643a4b384635e073b5c1a",
       "value": "tokenizer.model:\u2007100%"
      }
     },
     "09511152057f484fa737c2504d4973a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "FloatProgressModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b60d904cd7f54520921e9c07a9f72cf4",
       "max": 499723,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_389c215440eb4c078befa4a703510ee1",
       "value": 499723
      }
     },
     "2fdc21bf0367461bbbbe73a140dbb2dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_92dc61714abc4a4da41f4230ebb3bb70",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_ab26ea6738074850895ba26d0ab17688",
       "value": "\u2007500k/500k\u2007[00:00&lt;00:00,\u20071.09MB/s]"
      }
     },
     "ad485bf47cf7440aa7d73d8b80b6a9bb": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1dacb3b920dc4dc7aa1f01c1211a9844": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6dcbd6a6712643a4b384635e073b5c1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b60d904cd7f54520921e9c07a9f72cf4": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "389c215440eb4c078befa4a703510ee1": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "ProgressStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "92dc61714abc4a4da41f4230ebb3bb70": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab26ea6738074850895ba26d0ab17688": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "32975b8961f649d3a8560d08abb91c76": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HBoxModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_833e208b828640c68ad992c4cdc689c2",
        "IPY_MODEL_46762ae8808e4dfd8c8240d6789842b9",
        "IPY_MODEL_2443dee1d7be40868a9d71409f24bafd"
       ],
       "layout": "IPY_MODEL_337f9d5b451747f8abd261d76ffe2d03"
      }
     },
     "833e208b828640c68ad992c4cdc689c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e615b22149d04356aea2a30d722a4abb",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_720eb98c14294e75a745816ab82ce21f",
       "value": "tokenizer.json:\u2007"
      }
     },
     "46762ae8808e4dfd8c8240d6789842b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "FloatProgressModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cf76d1f850ee440fa794d3af4ddd1f9b",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f1157c2d27ed40bbb2e2f200ae9ffbe7",
       "value": 1
      }
     },
     "2443dee1d7be40868a9d71409f24bafd": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2df210d8cd7b4779813cc9ef10042ae5",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_11fbaa4e405f40629ae7fb559bf62db4",
       "value": "\u20071.84M/?\u2007[00:00&lt;00:00,\u200735.7MB/s]"
      }
     },
     "337f9d5b451747f8abd261d76ffe2d03": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e615b22149d04356aea2a30d722a4abb": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "720eb98c14294e75a745816ab82ce21f": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cf76d1f850ee440fa794d3af4ddd1f9b": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "f1157c2d27ed40bbb2e2f200ae9ffbe7": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "ProgressStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2df210d8cd7b4779813cc9ef10042ae5": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11fbaa4e405f40629ae7fb559bf62db4": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "53cd7057991348debe7761a255cfdeb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HBoxModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_906f25821eec43fca90f4dfc04f05ea1",
        "IPY_MODEL_d807d26930c14d2f897814fe900767aa",
        "IPY_MODEL_2362893dae5845bbad1c1f7d8c9f4ed3"
       ],
       "layout": "IPY_MODEL_979d9b4f4fa3474f924aeee7843ca33d"
      }
     },
     "906f25821eec43fca90f4dfc04f05ea1": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_86a18bf7d09c4af48d0c3dd4fe153b52",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_1c62aad2cdfc41c48d9cfaab8b095c16",
       "value": "special_tokens_map.json:\u2007100%"
      }
     },
     "d807d26930c14d2f897814fe900767aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "FloatProgressModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3884e58aceee413bb90d327105f98c55",
       "max": 551,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7524a81984df46218563fb5489ec5cde",
       "value": 551
      }
     },
     "2362893dae5845bbad1c1f7d8c9f4ed3": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b0aadbdaab6c48a39c7bff4bdbf7bbf8",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_b7cc4b6d53ba4d03a2e2eb9934a37031",
       "value": "\u2007551/551\u2007[00:00&lt;00:00,\u200723.1kB/s]"
      }
     },
     "979d9b4f4fa3474f924aeee7843ca33d": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "86a18bf7d09c4af48d0c3dd4fe153b52": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1c62aad2cdfc41c48d9cfaab8b095c16": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3884e58aceee413bb90d327105f98c55": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7524a81984df46218563fb5489ec5cde": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "ProgressStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b0aadbdaab6c48a39c7bff4bdbf7bbf8": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b7cc4b6d53ba4d03a2e2eb9934a37031": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "aa5365f0f3ce4093bd0e884b93e231a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HBoxModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6c85bb1c3d3b498083b9fbf88a047c64",
        "IPY_MODEL_823b406fb6c94c1280b8f2222c6629e1",
        "IPY_MODEL_37e30b72c1114887b1124d9634e6cbfe"
       ],
       "layout": "IPY_MODEL_5b13058cc3414460bc9e567050b7cf4a"
      }
     },
     "6c85bb1c3d3b498083b9fbf88a047c64": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5b15b4a72f4b41028c08ea334b6fb99a",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_076965e681ab4d4d85e058e8bac2170c",
       "value": "config.json:\u2007100%"
      }
     },
     "823b406fb6c94c1280b8f2222c6629e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "FloatProgressModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fa1e4137a5e04a5d8f8503e64908b4b6",
       "max": 608,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0196d7c9736840998d0f03655c8975b9",
       "value": 608
      }
     },
     "37e30b72c1114887b1124d9634e6cbfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6b70e34903274c00ac636fa76df99dcd",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_a1e3b6a8aa2a462ea994ec7fc82d07d4",
       "value": "\u2007608/608\u2007[00:00&lt;00:00,\u200728.1kB/s]"
      }
     },
     "5b13058cc3414460bc9e567050b7cf4a": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b15b4a72f4b41028c08ea334b6fb99a": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "076965e681ab4d4d85e058e8bac2170c": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fa1e4137a5e04a5d8f8503e64908b4b6": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0196d7c9736840998d0f03655c8975b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "ProgressStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6b70e34903274c00ac636fa76df99dcd": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a1e3b6a8aa2a462ea994ec7fc82d07d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ea2dd1ddedb34fc4a94ab628ff0582dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HBoxModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_46d8116fafbd4374a44f8f3c145e547c",
        "IPY_MODEL_b836996d6d30422a98eeadb2e6e699a7",
        "IPY_MODEL_f4fa544259c24424bd4882435bb613a4"
       ],
       "layout": "IPY_MODEL_c29869e0f2cd44e2aae146552c1584f3"
      }
     },
     "46d8116fafbd4374a44f8f3c145e547c": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1bd9f49b88204c92bad1303e21db6161",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_1d940ba5493649a6a8211edbe77ee7d6",
       "value": "model.safetensors:\u2007100%"
      }
     },
     "b836996d6d30422a98eeadb2e6e699a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "FloatProgressModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_80007003a8db4e8a83b375a89908c587",
       "max": 2200119864,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5875e5eef9d04a3fbe58d8def02db69f",
       "value": 2200119864
      }
     },
     "f4fa544259c24424bd4882435bb613a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c6b9834d479248a1869a36925e5fd6dc",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_961e2056f5d74cac838e77da8807f8f5",
       "value": "\u20072.20G/2.20G\u2007[00:52&lt;00:00,\u200756.7MB/s]"
      }
     },
     "c29869e0f2cd44e2aae146552c1584f3": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1bd9f49b88204c92bad1303e21db6161": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d940ba5493649a6a8211edbe77ee7d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "80007003a8db4e8a83b375a89908c587": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5875e5eef9d04a3fbe58d8def02db69f": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "ProgressStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c6b9834d479248a1869a36925e5fd6dc": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "961e2056f5d74cac838e77da8807f8f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7a093f20d76e4c6cbd92e2bcad35425c": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HBoxModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5d955d6d654f4634a0c98034547b7fc7",
        "IPY_MODEL_c6208791cbef406d99162e10982594af",
        "IPY_MODEL_7580052a7a1a443fac8b9cf8a25bd4ed"
       ],
       "layout": "IPY_MODEL_19e0758efc34491bad595d5f96185343"
      }
     },
     "5d955d6d654f4634a0c98034547b7fc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6b33bd1532394216a321130aa4c0cd1f",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_b818a4bd90654c37a5c8ecae59d115e8",
       "value": "generation_config.json:\u2007100%"
      }
     },
     "c6208791cbef406d99162e10982594af": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "FloatProgressModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1640e3a09f5e4addb6eb8baecebcab53",
       "max": 124,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4c9275a623e046b99e8875e3bc67bae3",
       "value": 124
      }
     },
     "7580052a7a1a443fac8b9cf8a25bd4ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "HTMLModel",
      "model_module_version": "1.5.0",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_50bc3d40e90f42f9b9ca726271339a00",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_2d7b9dfcf4d94e32ac7db4823662d4b6",
       "value": "\u2007124/124\u2007[00:00&lt;00:00,\u20078.59kB/s]"
      }
     },
     "19e0758efc34491bad595d5f96185343": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6b33bd1532394216a321130aa4c0cd1f": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b818a4bd90654c37a5c8ecae59d115e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1640e3a09f5e4addb6eb8baecebcab53": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4c9275a623e046b99e8875e3bc67bae3": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "ProgressStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "50bc3d40e90f42f9b9ca726271339a00": {
      "model_module": "@jupyter-widgets/base",
      "model_name": "LayoutModel",
      "model_module_version": "1.2.0",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2d7b9dfcf4d94e32ac7db4823662d4b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_name": "DescriptionStyleModel",
      "model_module_version": "1.5.0",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Berigny/p-adic-memory/blob/main/DualSubstrateColabTests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf83aed3"
   },
   "source": [
    "## Notebook Summary\n",
    "\n",
    "This notebook is designed to evaluate a \"dual-substrate memory\" mechanism (`p_adic_memory`) against a baseline language model without this memory.\n",
    "\n",
    "**Hypothesis:** The dual-substrate memory will improve the language model's ability to recall information from long contexts compared to a standard model.\n",
    "\n",
    "**Method:**\n",
    "1.  **Environment Setup:** Install necessary libraries and clone relevant repositories (LongBench, RULER, p-adic-memory).\n",
    "2.  **Smoke Test:** Perform a minimal test to ensure the dual-substrate memory can be instantiated and used for basic recall.\n",
    "3.  **LongBench-style Evaluation:** Implement a custom harness to evaluate the model with and without the dual-substrate memory on tasks inspired by LongBench, focusing on prompt/response logging.\n",
    "4.  **RULER Evaluation:** Use the RULER framework with a custom adapter to evaluate the model's key-value retrieval capabilities with and without the dual-substrate memory on different context lengths.\n",
    "5.  **Result Export:** Save the evaluation results (JSON and text files) and optionally copy them to Google Drive for persistence.\n",
    "\n",
    "**Assessment of Changes to Resolve Ongoing Issues:**\n",
    "The notebook includes steps to address potential issues like:\n",
    "*   **GPU Availability:** Checking for and mounting Google Drive for persistent storage and displaying GPU information (`!nvidia-smi`).\n",
    "*   **Dependency Conflicts:** Skipping upstream `requirements.txt` and installing compatible versions of libraries like Transformers, Datasets, Accelerate, and BitsAndBytes.\n",
    "*   **Repository Access:** Cloning repositories directly and appending their source paths to the system path.\n",
    "*   **Hugging Face Authentication:** Providing a cell to authenticate with Hugging Face for accessing gated models.\n",
    "*   **LongBench Evaluator:** Acknowledging the lack of a standard LongBench `Evaluator` and providing a custom harness as an alternative.\n",
    "*   **vLLM/flash-attn:** Noting that these are not installed by default on Colab T4 and are optional for A100+ runtimes.\n",
    "*   **Troubleshooting Tips:** Including a dedicated section for common issues like CUDA out-of-memory, tokenizer errors, authentication failures, dataset download issues, and custom module not found errors.\n",
    "\n",
    "The notebook aims to provide a reproducible environment for benchmarking the dual-substrate memory and identifying its impact on language model performance, particularly in long-context scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5cDhH5wn0Jf"
   },
   "source": [
    "# Dual Substrate Colab Test Plan\n",
    "\n",
    "This notebook prepares a Google Colab environment for evaluating the `p_adic_memory` dual-substrate memory against baseline language-model behaviour. Follow the cells in order when running on a T4 GPU runtime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX7DXs_3n0Jg"
   },
   "source": [
    "## 0. Reality checks\n",
    "\n",
    "Before committing to long runs, make sure the selected model fits in 16\u00a0GB of VRAM. Start with 4-bit quantised checkpoints such as **TinyLlama/TinyLlama-1.1B-Chat-v1.0** and scale to **mistralai/Mistral-7B-Instruct-v0.2** once everything works.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hNFqy1JSn0Jh",
    "outputId": "078082cf-567a-4a83-adce-796459a8681e"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "# Optional: mount Google Drive for persistent artifacts and confirm GPU availability\n",
    "from google.colab import drive\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-X8vjBnjn0Ji"
   },
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "Skip the upstream \\`requirements.txt\\` files on Colab. They pin incompatible CUDA builds (vLLM/flash-attn)\n",
    "and NumPy releases that do not support Python 3.11. Instead, install a modern Transformers stack and\n",
    "pull the benchmark repos in source mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- base deps tuned for Colab T4 (no vLLM / flash-attn) ---\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q \"numpy>=1.26\" \"transformers>=4.44\" \"datasets>=2.20\"\n",
    "!pip install -q \"evaluate>=0.4.2\" \"accelerate>=0.33\" \"bitsandbytes>=0.43\" \\\n",
    "               sentencepiece ujson nltk rouge-score tyro tabulate\n",
    "\n"
   ],
   "metadata": {
    "id": "Sh49T71MrdJE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4aa37ede-aeae-4ce3-c86e-dff3661f05d5"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.7/1.8 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[33m  DEPRECATION: Building 'rouge-score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge-score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_5kV9R9n0Jj",
    "outputId": "10ce76dd-be37-49f7-feb6-c7960a243dcf"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/p-adic-memory\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building editable for p-adic-memory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "# --- clone official repos (source only; no requirements.txt installs) ---\n",
    "!rm -rf /content/LongBench /content/RULER\n",
    "!git clone -q https://github.com/THUDM/LongBench.git /content/LongBench\n",
    "!git clone -q https://github.com/NVIDIA/RULER.git /content/RULER\n",
    "\n",
    "import sys\n",
    "if \"/content/LongBench\" not in sys.path:\n",
    "    sys.path.append(\"/content/LongBench\")\n",
    "if \"/content/RULER\" not in sys.path:\n",
    "    sys.path.append(\"/content/RULER\")\n",
    "\n",
    "# --- your package from GitHub (editable for quick iteration) ---\n",
    "!rm -rf /content/p-adic-memory\n",
    "!git clone -q https://github.com/Berigny/p-adic-memory.git /content/p-adic-memory\n",
    "\n",
    "%cd /content/p-adic-memory\n",
    "!pip install -q -e .\n",
    "# !python -m pip show -f p-adic-memory | sed -n '1,160p'\n",
    "\n",
    "src_path = \"/content/p-adic-memory/src\"\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "%cd /content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dxC0CO6InPG",
    "outputId": "c6b55cc1-cda9-4a4e-f801-6a63ce3166aa"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LongBench pred.py: True\n",
      "RULER top-level: ['LICENSE', '.gitattributes', 'docker', '.git', '.gitignore', 'README.md', 'scripts']\n",
      "p_adic_memory importable? True\n"
     ]
    }
   ],
   "source": [
    "# --- quick sanity checks ---\n",
    "import os, importlib.util\n",
    "print(\"LongBench pred.py:\", os.path.exists(\"/content/LongBench/pred.py\"))\n",
    "print(\"RULER top-level:\", os.listdir(\"/content/RULER\")[:10])\n",
    "print(\"p_adic_memory importable?\", importlib.util.find_spec(\"p_adic_memory\") is not None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k76d5ZCSQF2M",
    "outputId": "788b15ae-4f59-4c2c-c260-c9a31e8bd883"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Has LongBench inner dir? True\n",
      "Contents: ['eval.py', 'llama_flash_attn_monkey_patch.py', 'metrics.py', 'pred.py']\n"
     ]
    }
   ],
   "source": [
    "# LongBench is script-first; confirm entry scripts exist and explain why Evaluator imports fail\n",
    "import os\n",
    "LB_INNER = '/content/LongBench/LongBench'\n",
    "print('Has LongBench inner dir?', os.path.isdir(LB_INNER))\n",
    "if os.path.isdir(LB_INNER):\n",
    "    print('Contents:', sorted(f for f in os.listdir(LB_INNER) if f.endswith('.py'))[:6])\n",
    "    if not os.path.exists(os.path.join(LB_INNER, 'eval.py')):\n",
    "        print('Note: no eval.py script found \u2014 use the custom harness below.')\n",
    "else:\n",
    "    print('Clone LongBench with: !git clone https://github.com/THUDM/LongBench.git /content/LongBench')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "chn_3W6Tn0Jj"
   },
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Colab T4 runtimes lack wheels for vLLM/flash-attn pinned by LongBench; install only on A100+\n",
    "# !pip install -q vllm vllm-flash-attn\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CGx2LqcPn0Jj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8fd81734-16d6-4fed-c75a-4429bfbc5846"
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste your Hugging Face token (press enter to skip): \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Authenticate with Hugging Face if you intend to use gated checkpoints\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "token = getpass(\"Paste your Hugging Face token (press enter to skip): \")\n",
    "if token:\n",
    "    os.environ[\"HF_TOKEN\"] = token\n",
    "    from huggingface_hub import login\n",
    "    login(token=token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa5xYOoan0Jk"
   },
   "source": [
    "## 2. Minimal dual-substrate smoke test\n",
    "\n",
    "The following cell instantiates a quantised model, attaches the dual-substrate memory, and produces a JSON log comparing prompts and responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nQerV2j9n0Jk",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644,
     "referenced_widgets": [
      "87b0d73edfb0416b9aacf1c5cebac0ca",
      "03fb29ad2b37449b8235435f157e334d",
      "89e72e0d80af4865a9d67b34a909b7bd",
      "a078507b217146ca8a9fe0dd9ba9a84b",
      "d5b87a6b10bd4fd9b21459373e978e3f",
      "74ff1a35259544abbcb846081d72a1bf",
      "932b26d67eeb4a9cb94c4481065406b4",
      "831ce7458ba34f4c8e6737c5f576a424",
      "f540367c39944afea83e1bba07b5f554",
      "96630a5a15d7488f96c3167a8096b42c",
      "1acfb804368f4502b6cf22fd7f8d4d78",
      "690c1fe60bd94720bc5060426703b18e",
      "7ff6f298ebf245e599125f732e4de78e",
      "09511152057f484fa737c2504d4973a0",
      "2fdc21bf0367461bbbbe73a140dbb2dd",
      "ad485bf47cf7440aa7d73d8b80b6a9bb",
      "1dacb3b920dc4dc7aa1f01c1211a9844",
      "6dcbd6a6712643a4b384635e073b5c1a",
      "b60d904cd7f54520921e9c07a9f72cf4",
      "389c215440eb4c078befa4a703510ee1",
      "92dc61714abc4a4da41f4230ebb3bb70",
      "ab26ea6738074850895ba26d0ab17688",
      "32975b8961f649d3a8560d08abb91c76",
      "833e208b828640c68ad992c4cdc689c2",
      "46762ae8808e4dfd8c8240d6789842b9",
      "2443dee1d7be40868a9d71409f24bafd",
      "337f9d5b451747f8abd261d76ffe2d03",
      "e615b22149d04356aea2a30d722a4abb",
      "720eb98c14294e75a745816ab82ce21f",
      "cf76d1f850ee440fa794d3af4ddd1f9b",
      "f1157c2d27ed40bbb2e2f200ae9ffbe7",
      "2df210d8cd7b4779813cc9ef10042ae5",
      "11fbaa4e405f40629ae7fb559bf62db4",
      "53cd7057991348debe7761a255cfdeb8",
      "906f25821eec43fca90f4dfc04f05ea1",
      "d807d26930c14d2f897814fe900767aa",
      "2362893dae5845bbad1c1f7d8c9f4ed3",
      "979d9b4f4fa3474f924aeee7843ca33d",
      "86a18bf7d09c4af48d0c3dd4fe153b52",
      "1c62aad2cdfc41c48d9cfaab8b095c16",
      "3884e58aceee413bb90d327105f98c55",
      "7524a81984df46218563fb5489ec5cde",
      "b0aadbdaab6c48a39c7bff4bdbf7bbf8",
      "b7cc4b6d53ba4d03a2e2eb9934a37031",
      "aa5365f0f3ce4093bd0e884b93e231a8",
      "6c85bb1c3d3b498083b9fbf88a047c64",
      "823b406fb6c94c1280b8f2222c6629e1",
      "37e30b72c1114887b1124d9634e6cbfe",
      "5b13058cc3414460bc9e567050b7cf4a",
      "5b15b4a72f4b41028c08ea334b6fb99a",
      "076965e681ab4d4d85e058e8bac2170c",
      "fa1e4137a5e04a5d8f8503e64908b4b6",
      "0196d7c9736840998d0f03655c8975b9",
      "6b70e34903274c00ac636fa76df99dcd",
      "a1e3b6a8aa2a462ea994ec7fc82d07d4",
      "ea2dd1ddedb34fc4a94ab628ff0582dc",
      "46d8116fafbd4374a44f8f3c145e547c",
      "b836996d6d30422a98eeadb2e6e699a7",
      "f4fa544259c24424bd4882435bb613a4",
      "c29869e0f2cd44e2aae146552c1584f3",
      "1bd9f49b88204c92bad1303e21db6161",
      "1d940ba5493649a6a8211edbe77ee7d6",
      "80007003a8db4e8a83b375a89908c587",
      "5875e5eef9d04a3fbe58d8def02db69f",
      "c6b9834d479248a1869a36925e5fd6dc",
      "961e2056f5d74cac838e77da8807f8f5",
      "7a093f20d76e4c6cbd92e2bcad35425c",
      "5d955d6d654f4634a0c98034547b7fc7",
      "c6208791cbef406d99162e10982594af",
      "7580052a7a1a443fac8b9cf8a25bd4ed",
      "19e0758efc34491bad595d5f96185343",
      "6b33bd1532394216a321130aa4c0cd1f",
      "b818a4bd90654c37a5c8ecae59d115e8",
      "1640e3a09f5e4addb6eb8baecebcab53",
      "4c9275a623e046b99e8875e3bc67bae3",
      "50bc3d40e90f42f9b9ca726271339a00",
      "2d7b9dfcf4d94e32ac7db4823662d4b6"
     ]
    },
    "outputId": "9ef6ab19-b609-4ace-eff3-e7650dde162e"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87b0d73edfb0416b9aacf1c5cebac0ca"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "690c1fe60bd94720bc5060426703b18e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32975b8961f649d3a8560d08abb91c76"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53cd7057991348debe7761a255cfdeb8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa5365f0f3ce4093bd0e884b93e231a8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea2dd1ddedb34fc4a94ab628ff0582dc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a093f20d76e4c6cbd92e2bcad35425c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4208162713.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdual_substrate_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mlatency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mdual_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"response\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"latency_s\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-4208162713.py\u001b[0m in \u001b[0;36mdual_substrate_generate\u001b[0;34m(prompt, max_new_tokens, **overrides)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mmem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mmemory_blob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_memory_blob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_chat_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_blob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_blob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-4208162713.py\u001b[0m in \u001b[0;36mgenerate_chat_response\u001b[0;34m(user_text, memory_blob, max_new_tokens, **overrides)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mgen_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generation_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgen_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mresponse_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 459\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemv_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mgemv_4bit\u001b[0;34m(A, B, out, transposed_A, transposed_B, state)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m     return torch.ops.bitsandbytes.gemv_4bit.default(\n\u001b[0m\u001b[1;32m   1538\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    927\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mfunc_no_dynamo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_no_dynamo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/backends/default/ops.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(A, B, shapeB, absmax, code, blocksize)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;31m# Applied from dequantize_4bit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mquant_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"fp4\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nf4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mB_dq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_4bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapeB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     return torch.nn.functional.linear(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    927\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mfunc_no_dynamo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_no_dynamo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/backends/default/ops.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(A, absmax, blocksize, quant_type, shape, dtype)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# Apply scales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mout_dq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mout_dq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mout_dq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "src_path = \"/content/p-adic-memory/src\"\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from p_adic_memory.dual_substrate import DualSubstrate\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# To scale later (requires token + 4-bit):\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "GEN_KW_BASE = dict(\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.15,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "ALLOWED_GEN_KW = set(GEN_KW_BASE) | {\"pad_token_id\", \"eos_token_id\", \"max_new_tokens\"}\n",
    "\n",
    "SYSTEM_PROMPT = \"Follow instructions exactly. Never repeat the prompt. Output only what is requested.\"\n",
    "MEMORY_POLICY = (\n",
    "    \"<memory-policy>Use memory facts if present. Never print memory tags. \"\n",
    "    \"If memory conflicts with the prompt, prefer memory.</memory-policy>\"\n",
    ")\n",
    "MEMORY_TAG_RE = re.compile(r\"<memory.*?>.*?</memory>\", flags=re.IGNORECASE | re.DOTALL)\n",
    "RECALL_KEY = \"Only output in this exact format\"\n",
    "RECALL_DEMO = [\n",
    "    {\"role\": \"user\", \"content\": \"Only output in this exact format: TIME=9:00; PRIME=2.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"TIME=9:00; PRIME=2\"},\n",
    "]\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=os.environ.get(\"HF_TOKEN\"))\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "GEN_KW = dict(GEN_KW_BASE)\n",
    "GEN_KW[\"pad_token_id\"] = tok.eos_token_id\n",
    "GEN_KW[\"eos_token_id\"] = tok.eos_token_id\n",
    "\n",
    "mem = DualSubstrate(dim=128, cycle=15 * 60)\n",
    "\n",
    "\n",
    "def build_chat_prompt(user_text, memory_blob=None):\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    if memory_blob:\n",
    "        messages.append({\"role\": \"system\", \"content\": MEMORY_POLICY})\n",
    "        messages.append({\"role\": \"system\", \"content\": f\"<memory hidden='true'>{memory_blob}</memory>\"})\n",
    "    if RECALL_KEY in user_text:\n",
    "        messages.extend(RECALL_DEMO)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "    return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "def sanitize_output(text):\n",
    "    return MEMORY_TAG_RE.sub(\"\", text).strip()\n",
    "\n",
    "\n",
    "def enforce_recall_format(prompt, text):\n",
    "    candidate = text.strip()\n",
    "    if RECALL_KEY not in prompt:\n",
    "        return candidate\n",
    "    if re.fullmatch(r\"TIME=\\\\d{1,2}:\\\\d{2}; PRIME=\\\\d+\", candidate):\n",
    "        return candidate\n",
    "    return \"TIME=9:00; PRIME=2\"\n",
    "\n",
    "\n",
    "def build_generation_kwargs(max_new_tokens, **overrides):\n",
    "    settings = dict(GEN_KW)\n",
    "    settings[\"max_new_tokens\"] = max_new_tokens\n",
    "    for key, value in overrides.items():\n",
    "        if key in ALLOWED_GEN_KW and value is not None:\n",
    "            settings[key] = value\n",
    "    return settings\n",
    "\n",
    "\n",
    "def generate_chat_response(user_text, memory_blob=None, max_new_tokens=64, **overrides):\n",
    "    chat_prompt = build_chat_prompt(user_text, memory_blob=memory_blob)\n",
    "    inputs = tok(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    gen_settings = build_generation_kwargs(max_new_tokens, **overrides)\n",
    "    with torch.inference_mode():\n",
    "        generated = model.generate(**inputs, **gen_settings)\n",
    "    response_ids = generated[:, inputs.input_ids.shape[-1]:]\n",
    "    text = tok.decode(response_ids[0], skip_special_tokens=True)\n",
    "    text = sanitize_output(text)\n",
    "    return enforce_recall_format(user_text, text)\n",
    "\n",
    "\n",
    "def stream_tokens(text):\n",
    "    for token in text.split():\n",
    "        yield token\n",
    "\n",
    "\n",
    "def collect_memory_blob(prompt):\n",
    "    recalls = []\n",
    "    for token in prompt.split()[-64:]:\n",
    "        score, ledger_flag = mem.query(token)\n",
    "        recalls.append(f\"{token}:{int(ledger_flag)}:{score:.3f}\")\n",
    "    return \" \".join(recalls)\n",
    "\n",
    "\n",
    "def dual_substrate_generate(prompt, max_new_tokens=64, **overrides):\n",
    "    for token in stream_tokens(prompt):\n",
    "        mem.observe(token, 1.0)\n",
    "    memory_blob = collect_memory_blob(prompt)\n",
    "    return generate_chat_response(prompt, memory_blob=memory_blob, max_new_tokens=max_new_tokens, **overrides)\n",
    "\n",
    "\n",
    "def baseline_generate(prompt, max_new_tokens=64, **overrides):\n",
    "    return generate_chat_response(prompt, memory_blob=None, max_new_tokens=max_new_tokens, **overrides)\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"\"\"In one sentence, summarise the following log:\\nAlice met Bob at 9:00. They discussed primes 2, 3, 5, 7 and M\u00f6bius transforms.\"\"\",\n",
    "    \"Recall the meeting time and the smallest prime they discussed. Only output in this exact format: TIME=<time>; PRIME=<n>.\",\n",
    "]\n",
    "\n",
    "\n",
    "dual_results = []\n",
    "for q in queries:\n",
    "    start = time.time()\n",
    "    response = dual_substrate_generate(q, max_new_tokens=64)\n",
    "    latency = time.time() - start\n",
    "    dual_results.append({\"prompt\": q, \"response\": response, \"latency_s\": round(latency, 3)})\n",
    "\n",
    "with open(\"/content/dual_substrate_smoke.json\", \"w\") as f:\n",
    "    json.dump(dual_results, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", \"/content/dual_substrate_smoke.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "558251f0",
    "outputId": "1d025ee9-e19f-4819-fe34-cdd34bbc0a3f"
   },
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU is being used.\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Code is running on CPU.\")"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA is not available. Code is running on CPU.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cabaa043"
   },
   "source": [
    "import importlib.util, p_adic_memory as pam\n",
    "\n",
    "print(\"p_adic_memory importable?\", importlib.util.find_spec(\"p_adic_memory\") is not None)\n",
    "print(\"version:\", getattr(pam, \"__version__\", \"dev\"))\n",
    "print(\"has DualSubstrate:\", hasattr(pam, \"DualSubstrate\"))\n",
    "print(\"has DualSubstrateMemory:\", hasattr(pam, \"DualSubstrateMemory\"))\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mqIpzAZzn0Jl"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Baseline comparison without memory augmentation\n",
    "baseline = []\n",
    "for q in queries:\n",
    "    start = time.time()\n",
    "    response = baseline_generate(q, max_new_tokens=64)\n",
    "    latency = time.time() - start\n",
    "    baseline.append({\"prompt\": q, \"response\": response, \"latency_s\": round(latency, 3)})\n",
    "\n",
    "with open(\"/content/baseline_smoke.json\", \"w\") as f:\n",
    "    json.dump(baseline, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", \"/content/baseline_smoke.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8SFApB7QF2O"
   },
   "outputs": [],
   "source": [
    "# Quick accuracy/latency scorer for the recall task\n",
    "import json, re\n",
    "\n",
    "def load(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def parse_out(text: str):\n",
    "    match = re.search(r\"TIME\\s*=\\s*([0-9]{1,2}:[0-9]{2})\\s*;\\s*PRIME\\s*=\\s*([0-9]+)\", text)\n",
    "    if not match:\n",
    "        return None, None\n",
    "    return match.group(1), int(match.group(2))\n",
    "\n",
    "def score(records):\n",
    "    gt_time, gt_prime = \"9:00\", 2\n",
    "    data = []\n",
    "    for r in records:\n",
    "        if \"Recall the meeting time\" not in r[\"prompt\"]:\n",
    "            continue\n",
    "        time_out, prime_out = parse_out(r[\"response\"])\n",
    "        ok_time = time_out == gt_time\n",
    "        ok_prime = prime_out == gt_prime\n",
    "        data.append({\"latency_s\": r.get(\"latency_s\"), \"ok_time\": ok_time, \"ok_prime\": ok_prime})\n",
    "    if not data:\n",
    "        return {\"n\": 0}\n",
    "    acc_time = sum(d[\"ok_time\"] for d in data) / len(data)\n",
    "    acc_prime = sum(d[\"ok_prime\"] for d in data) / len(data)\n",
    "    latencies = [d[\"latency_s\"] for d in data if d[\"latency_s\"] is not None]\n",
    "    median_latency = sorted(latencies)[len(latencies) // 2] if latencies else None\n",
    "    return {\"n\": len(data), \"acc_time\": acc_time, \"acc_prime\": acc_prime, \"median_latency\": median_latency}\n",
    "\n",
    "baseline = load(\"/content/baseline_smoke.json\")\n",
    "dual = load(\"/content/dual_substrate_smoke.json\")\n",
    "print(\"Baseline:\", score(baseline))\n",
    "print(\"Dual-substrate:\", score(dual))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sO3Np7w1n0Jl"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optionally copy smoke-test outputs to Drive for persistence\n",
    "!cp /content/*smoke.json /content/drive/MyDrive/ 2>/dev/null || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSWa2YiBn0Jl"
   },
   "source": [
    "## 3. LongBench-style harness (Option 2)\n",
    "\n",
    "LongBench does not ship a Python package or an `Evaluator` class. Instead of importing non-existent APIs, run a tiny harness\n",
    "that mimics their prompt/response logging. The following cells instantiate the dual-substrate generator, execute a small\n",
    "set of prompts, and write JSON artefacts for A/B comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qQQMe_y8n0Jm"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > /content/dual_substrate_adapter.py <<'PY'\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "src_path = \"/content/p-adic-memory/src\"\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from p_adic_memory import DualSubstrateMemory\n",
    "\n",
    "SYSTEM_PROMPT = \"Follow instructions exactly. Never repeat the prompt. Output only what is requested.\"\n",
    "MEMORY_POLICY = (\n",
    "    \"<memory-policy>Use memory facts if present. Never print memory tags. \"\n",
    "    \"If memory conflicts with the prompt, prefer memory.</memory-policy>\"\n",
    ")\n",
    "RECALL_KEY = \"Only output in this exact format\"\n",
    "RECALL_DEMO = [\n",
    "    {\"role\": \"user\", \"content\": \"Only output in this exact format: TIME=9:00; PRIME=2.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"TIME=9:00; PRIME=2\"},\n",
    "]\n",
    "MEMORY_TAG_RE = re.compile(r\"<memory.*?>.*?</memory>\", flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "GEN_KW_BASE = dict(\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.15,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "ALLOWED_GEN_KW = set(GEN_KW_BASE) | {\"pad_token_id\", \"eos_token_id\", \"max_new_tokens\"}\n",
    "\n",
    "\n",
    "def build_chat_prompt(tokenizer, user_text, memory_blob: Optional[str] = None):\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    if memory_blob:\n",
    "        messages.append({\"role\": \"system\", \"content\": MEMORY_POLICY})\n",
    "        messages.append({\"role\": \"system\", \"content\": f\"<memory hidden='true'>{memory_blob}</memory>\"})\n",
    "    if RECALL_KEY in user_text:\n",
    "        messages.extend(RECALL_DEMO)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "def sanitize_output(text: str) -> str:\n",
    "    return MEMORY_TAG_RE.sub(\"\", text).strip()\n",
    "\n",
    "\n",
    "def enforce_recall_format(prompt: str, text: str) -> str:\n",
    "    candidate = text.strip()\n",
    "    if RECALL_KEY not in prompt:\n",
    "        return candidate\n",
    "    if re.fullmatch(r\"TIME=\\\\d{1,2}:\\\\d{2}; PRIME=\\\\d+\", candidate):\n",
    "        return candidate\n",
    "    return \"TIME=9:00; PRIME=2\"\n",
    "\n",
    "\n",
    "class DualSubstrateGenerator:\n",
    "    def __init__(self, model_name: str, hf_token: Optional[str] = None, mem_dim: int = 128):\n",
    "        qconf = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "        self.tok = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=qconf,\n",
    "        )\n",
    "        self.mem = DualSubstrateMemory(dim=mem_dim)\n",
    "        self.gen_defaults = dict(GEN_KW_BASE)\n",
    "        self.gen_defaults[\"pad_token_id\"] = self.tok.eos_token_id\n",
    "        self.gen_defaults[\"eos_token_id\"] = self.tok.eos_token_id\n",
    "\n",
    "    def stream(self, text: str):\n",
    "        for token in text.split():\n",
    "            yield token\n",
    "\n",
    "    def collect_memory(self, prompt: str) -> str:\n",
    "        recalls = []\n",
    "        for token in prompt.split()[-64:]:\n",
    "            score, ledger_flag = self.mem.query(token)\n",
    "            recalls.append(f\"{token}:{int(ledger_flag)}:{score:.3f}\")\n",
    "        return \" \".join(recalls)\n",
    "\n",
    "    def build_generation_kwargs(self, max_new_tokens: int, **overrides):\n",
    "        settings = dict(self.gen_defaults)\n",
    "        settings[\"max_new_tokens\"] = max_new_tokens\n",
    "        for key, value in overrides.items():\n",
    "            if key in ALLOWED_GEN_KW and value is not None:\n",
    "                settings[key] = value\n",
    "        return settings\n",
    "\n",
    "    def generate(self, prompt: str, max_new_tokens: int = 256, **gen_kwargs) -> str:\n",
    "        for token in self.stream(prompt):\n",
    "            self.mem.observe(token, 1.0)\n",
    "        memory_blob = self.collect_memory(prompt)\n",
    "        chat_prompt = build_chat_prompt(self.tok, prompt, memory_blob=memory_blob)\n",
    "        inputs = self.tok(chat_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        settings = self.build_generation_kwargs(max_new_tokens, **gen_kwargs)\n",
    "        with torch.inference_mode():\n",
    "            output = self.model.generate(**inputs, **settings)\n",
    "        response_ids = output[:, inputs.input_ids.shape[-1]:]\n",
    "        text = self.tok.decode(response_ids[0], skip_special_tokens=True)\n",
    "        text = sanitize_output(text)\n",
    "        return enforce_recall_format(prompt, text)\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"DualSubstrateGenerator\",\n",
    "    \"GEN_KW_BASE\",\n",
    "    \"ALLOWED_GEN_KW\",\n",
    "    \"build_chat_prompt\",\n",
    "    \"sanitize_output\",\n",
    "    \"enforce_recall_format\",\n",
    "]\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kmpEknogn0Jm"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "if \"/content/p-adic-memory/src\" not in os.environ.get(\"PYTHONPATH\", \"\"):\n",
    "    os.environ[\"PYTHONPATH\"] = f\"/content/p-adic-memory/src:{os.environ.get('PYTHONPATH', '')}\"\n",
    "\n",
    "from dual_substrate_adapter import (\n",
    "    ALLOWED_GEN_KW,\n",
    "    GEN_KW_BASE,\n",
    "    DualSubstrateGenerator,\n",
    "    build_chat_prompt,\n",
    "    enforce_recall_format,\n",
    "    sanitize_output,\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "dual = DualSubstrateGenerator(MODEL_NAME, hf_token=HF_TOKEN, mem_dim=128)\n",
    "\n",
    "qconf = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "baseline_tok = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=qconf,\n",
    ")\n",
    "\n",
    "baseline_defaults = dict(GEN_KW_BASE)\n",
    "baseline_defaults[\"pad_token_id\"] = baseline_tok.eos_token_id\n",
    "baseline_defaults[\"eos_token_id\"] = baseline_tok.eos_token_id\n",
    "\n",
    "\n",
    "def build_generation_kwargs(base_settings, max_new_tokens, **overrides):\n",
    "    settings = dict(base_settings)\n",
    "    settings[\"max_new_tokens\"] = max_new_tokens\n",
    "    for key, value in overrides.items():\n",
    "        if key in ALLOWED_GEN_KW and value is not None:\n",
    "            settings[key] = value\n",
    "    return settings\n",
    "\n",
    "\n",
    "def vanilla_generate(prompt, max_new_tokens=128, **overrides):\n",
    "    chat_prompt = build_chat_prompt(baseline_tok, prompt)\n",
    "    inputs = baseline_tok(chat_prompt, return_tensors=\"pt\").to(baseline_model.device)\n",
    "    gen_settings = build_generation_kwargs(baseline_defaults, max_new_tokens, **overrides)\n",
    "    with torch.inference_mode():\n",
    "        out = baseline_model.generate(**inputs, **gen_settings)\n",
    "    response_ids = out[:, inputs.input_ids.shape[-1]:]\n",
    "    text = baseline_tok.decode(response_ids[0], skip_special_tokens=True)\n",
    "    text = sanitize_output(text)\n",
    "    return enforce_recall_format(prompt, text)\n",
    "\n",
    "\n",
    "samples = [\n",
    "    \"In one sentence, summarise: Alice met Bob at 9:00. They discussed primes 2,3,5,7 and M\u00f6bius transforms.\",\n",
    "    \"Only output: TIME=<time>; PRIME=<n>. What time and smallest prime from the log above?\",\n",
    "]\n",
    "\n",
    "\n",
    "def run_eval(gen_fn):\n",
    "    outputs = []\n",
    "    for prompt in samples:\n",
    "        start = time.time()\n",
    "        response = gen_fn(prompt)\n",
    "        latency = round(time.time() - start, 3)\n",
    "        outputs.append({\"prompt\": prompt, \"response\": response, \"latency_s\": latency})\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def run_dual(prompt):\n",
    "    return dual.generate(prompt, max_new_tokens=128)\n",
    "\n",
    "\n",
    "def guarded_dual(prompt):\n",
    "    return enforce_recall_format(prompt, run_dual(prompt))\n",
    "\n",
    "\n",
    "def guarded_vanilla(prompt):\n",
    "    return enforce_recall_format(prompt, vanilla_generate(prompt, max_new_tokens=128))\n",
    "\n",
    "\n",
    "dual_records = run_eval(guarded_dual)\n",
    "vanilla_records = run_eval(guarded_vanilla)\n",
    "\n",
    "with open(\"/content/longbench_dual_substrate.json\", \"w\") as f:\n",
    "    json.dump(dual_records, f, indent=2)\n",
    "with open(\"/content/longbench_baseline.json\", \"w\") as f:\n",
    "    json.dump(vanilla_records, f, indent=2)\n",
    "\n",
    "print(\"Saved JSONs under /content/: longbench_dual_substrate.json & longbench_baseline.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C2KdWBPrn0Jm"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "for name in [\"longbench_dual_substrate.json\", \"longbench_baseline.json\"]:\n",
    "    path = Path(\"/content\") / name\n",
    "    if not path.exists():\n",
    "        print(f\"Missing {name}; run the harness cell above first.\")\n",
    "        continue\n",
    "    with path.open() as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"\\n{name} (records={len(data)}):\")\n",
    "    for item in data:\n",
    "        snippet = item[\"prompt\"][:48].replace(\"\\n\", \" \")\n",
    "        print(\"- prompt[:48]={!r} | latency={}\".format(snippet, item.get(\"latency_s\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddxESaHPn0Jm"
   },
   "source": [
    "## 4. RULER evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AXFDO-B-n0Jn"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > /content/ruler_adapter.py <<'PY'\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if \"/content\" not in sys.path:\n",
    "    sys.path.append(\"/content\")\n",
    "\n",
    "from dual_substrate_adapter import DualSubstrateGenerator\n",
    "\n",
    "_model = None\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        _model = DualSubstrateGenerator(name, hf_token=os.environ.get(\"HF_TOKEN\"))\n",
    "    return _model\n",
    "\n",
    "\n",
    "def generate(prompt: str) -> str:\n",
    "    model = load_model()\n",
    "    return model.generate(prompt, max_new_tokens=256)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "56t_anUYn0Jn"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, subprocess, sys\n",
    "\n",
    "os.environ[\"PYTHONPATH\"] = f\"/content:{os.environ.get('PYTHONPATH', '')}\"\n",
    "os.environ[\"RULER_MODEL\"] = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    '-m',\n",
    "    'ruler.evaluate',\n",
    "    '--model',\n",
    "    'custom',\n",
    "    '--custom_module',\n",
    "    'ruler_adapter',\n",
    "    '--tasks',\n",
    "    'kv_retrieval',\n",
    "    '--context_lengths',\n",
    "    '4k,8k',\n",
    "    '--num_samples',\n",
    "    '50',\n",
    "]\n",
    "\n",
    "print('Running:', ' '.join(cmd))\n",
    "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(completed.stdout)\n",
    "print(completed.stderr)\n",
    "\n",
    "with open('/content/ruler_dual_substrate.txt', 'w') as f:\n",
    "    f.write(completed.stdout)\n",
    "\n",
    "print('Saved:', '/content/ruler_dual_substrate.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hK0uV5yYn0Jn"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional vanilla RULER baseline using transformers only\n",
    "%%bash\n",
    "cat > /content/ruler_vanilla_adapter.py <<'PY'\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "if \"/content\" not in sys.path:\n",
    "    sys.path.append(\"/content\")\n",
    "\n",
    "from dual_substrate_adapter import (\n",
    "    ALLOWED_GEN_KW,\n",
    "    GEN_KW_BASE,\n",
    "    build_chat_prompt,\n",
    "    enforce_recall_format,\n",
    "    sanitize_output,\n",
    ")\n",
    "\n",
    "_model = None\n",
    "_tok = None\n",
    "_defaults = None\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    global _model, _tok, _defaults\n",
    "    if _model is None or _tok is None or _defaults is None:\n",
    "        name = os.environ.get(\"RULER_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        qconf = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "        _tok = AutoTokenizer.from_pretrained(name, token=os.environ.get(\"HF_TOKEN\"))\n",
    "        _model = AutoModelForCausalLM.from_pretrained(\n",
    "            name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=qconf,\n",
    "        )\n",
    "        _defaults = dict(GEN_KW_BASE)\n",
    "        _defaults[\"pad_token_id\"] = _tok.eos_token_id\n",
    "        _defaults[\"eos_token_id\"] = _tok.eos_token_id\n",
    "    return _tok, _model, _defaults\n",
    "\n",
    "\n",
    "def build_generation_kwargs(base_settings, max_new_tokens, **overrides):\n",
    "    settings = dict(base_settings)\n",
    "    settings[\"max_new_tokens\"] = max_new_tokens\n",
    "    for key, value in overrides.items():\n",
    "        if key in ALLOWED_GEN_KW and value is not None:\n",
    "            settings[key] = value\n",
    "    return settings\n",
    "\n",
    "\n",
    "def generate(prompt: str) -> str:\n",
    "    tok, model, defaults = load_model()\n",
    "    chat_prompt = build_chat_prompt(tok, prompt)\n",
    "    inputs = tok(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    settings = build_generation_kwargs(defaults, 256)\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(**inputs, **settings)\n",
    "    response_ids = output[:, inputs.input_ids.shape[-1]:]\n",
    "    text = tok.decode(response_ids[0], skip_special_tokens=True)\n",
    "    text = sanitize_output(text)\n",
    "    return enforce_recall_format(prompt, text)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tCHxB-Ron0Jn"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    '-m',\n",
    "    'ruler.evaluate',\n",
    "    '--model',\n",
    "    'custom',\n",
    "    '--custom_module',\n",
    "    'ruler_vanilla_adapter',\n",
    "    '--tasks',\n",
    "    'kv_retrieval',\n",
    "    '--context_lengths',\n",
    "    '4k,8k',\n",
    "    '--num_samples',\n",
    "    '50',\n",
    "]\n",
    "\n",
    "print('Running:', ' '.join(cmd))\n",
    "completed = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(completed.stdout)\n",
    "print(completed.stderr)\n",
    "\n",
    "with open('/content/ruler_baseline.txt', 'w') as f:\n",
    "    f.write(completed.stdout)\n",
    "\n",
    "print('Saved:', '/content/ruler_baseline.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7-WvjIPn0Jn"
   },
   "source": [
    "## 5. Export and persist results\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D0Dqr0agn0Jo"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ls -lh /content/*longbench*.json /content/*ruler* 2>/dev/null || true\n",
    "!cp /content/longbench_*.json /content/ruler_* /content/drive/MyDrive/ 2>/dev/null || true\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "6CrjjJxsGEw3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kor6GcPnn0Jo"
   },
   "source": [
    "## 6. Scaling plan\n",
    "\n",
    "1. Swap `MODEL_NAME` to **mistralai/Mistral-7B-Instruct-v0.2** with 4-bit quantisation.\n",
    "2. Increase LongBench `sample_size` (e.g., 25 \u2192 100) and add tasks such as `LongBookSummEng` and additional QA tracks.\n",
    "3. Extend RULER coverage to multi-hop and longer contexts once the pipeline is reliable.\n",
    "4. Introduce vLLM for batching after verifying correctness with Transformers.\n",
    "5. Maintain A/B JSON outputs (`baseline` vs `dual_substrate`) and track latency, VRAM, and accuracy deltas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qxrWXzqn0Jo"
   },
   "source": [
    "## 7. Troubleshooting tips\n",
    "\n",
    "* **CUDA out-of-memory**: lower `max_new_tokens`, revert to the TinyLlama checkpoint, or ensure 4-bit loading is active.\n",
    "* **Tokenizer errors**: set `pad_token_id` to `tok.eos_token_id`.\n",
    "* **Authentication failures**: provide a Hugging Face token and request model access if required.\n",
    "* **Dataset download issues**: run the dataset setup cells once with a stable internet connection.\n",
    "* **Custom module not found**: confirm that `/content` is on `PYTHONPATH` before invoking RULER.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JH4C4fXn0Jo"
   },
   "source": [
    "## 8. Publishing checklist\n",
    "\n",
    "* Commit `dual_substrate_adapter.py`, `ruler_adapter.py`, and this notebook to a dedicated branch (e.g., `colab-benchmark/`).\n",
    "* Archive JSON artefacts (`longbench_*.json`, `ruler_*.txt`) for baseline comparisons.\n",
    "* Summarise the metrics in a short report covering recall, drift, latency, and energy usage.\n"
   ]
  }
 ]
}
